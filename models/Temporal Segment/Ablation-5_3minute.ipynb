{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 22:23:35.437147: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_model(input_a_shape,input_b_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = BatchNormalization()(input1)    \n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "\n",
    "    x2 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    \n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    \n",
    "\n",
    "    attention1 = ResidualAttentionBlock(256, 256)\n",
    "    attention2 = ResidualAttentionBlock(64, 64)\n",
    "    x1 = attention1(x1)\n",
    "    x2 = attention1(x2)\n",
    "    \n",
    "   \n",
    "    concat = keras.layers.concatenate([x1, x2], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1,input2], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  # Convert to two categories\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 28s 61ms/step - loss: 4.0175 - accuracy: 0.6648 - val_loss: 4.4174 - val_accuracy: 0.3883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 2.8820 - accuracy: 0.8060 - val_loss: 3.1011 - val_accuracy: 0.4282 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.0810 - accuracy: 0.8440 - val_loss: 1.8379 - val_accuracy: 0.8149 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 1.5314 - accuracy: 0.8525 - val_loss: 1.4072 - val_accuracy: 0.8057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.1510 - accuracy: 0.8698 - val_loss: 1.0083 - val_accuracy: 0.8744 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.9116 - accuracy: 0.8806 - val_loss: 0.9540 - val_accuracy: 0.8375 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.7503 - accuracy: 0.8868 - val_loss: 0.7694 - val_accuracy: 0.8592 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.6606 - accuracy: 0.8878 - val_loss: 0.6476 - val_accuracy: 0.8753 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.5831 - accuracy: 0.8920 - val_loss: 0.6049 - val_accuracy: 0.8718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.5284 - accuracy: 0.8992 - val_loss: 0.5343 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.5038 - accuracy: 0.8997 - val_loss: 0.4766 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4695 - accuracy: 0.9052 - val_loss: 0.4522 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4510 - accuracy: 0.9042 - val_loss: 0.4349 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4483 - accuracy: 0.9045 - val_loss: 0.4448 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4359 - accuracy: 0.9029 - val_loss: 0.4099 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4325 - accuracy: 0.9050 - val_loss: 0.4083 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4238 - accuracy: 0.9051 - val_loss: 0.5090 - val_accuracy: 0.8712 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4215 - accuracy: 0.9069 - val_loss: 0.4501 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.4146 - accuracy: 0.9072 - val_loss: 0.4032 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4020 - accuracy: 0.9073 - val_loss: 0.4081 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4113 - accuracy: 0.9038 - val_loss: 0.4066 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4025 - accuracy: 0.9074 - val_loss: 0.4268 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4005 - accuracy: 0.9100 - val_loss: 0.4477 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4067 - accuracy: 0.9070 - val_loss: 0.4132 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4014 - accuracy: 0.9085 - val_loss: 0.4057 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3994 - accuracy: 0.9083 - val_loss: 0.7770 - val_accuracy: 0.7627 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3928 - accuracy: 0.9092 - val_loss: 0.4419 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3994 - accuracy: 0.9085 - val_loss: 0.4145 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3964 - accuracy: 0.9100 - val_loss: 0.6714 - val_accuracy: 0.7736 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3987 - accuracy: 0.9092 - val_loss: 0.3801 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3936 - accuracy: 0.9094 - val_loss: 0.3983 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3906 - accuracy: 0.9124 - val_loss: 0.5562 - val_accuracy: 0.8510 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3927 - accuracy: 0.9099 - val_loss: 0.4239 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4042 - accuracy: 0.9072 - val_loss: 0.3766 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3953 - accuracy: 0.9125 - val_loss: 0.4024 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3929 - accuracy: 0.9112 - val_loss: 0.4339 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3881 - accuracy: 0.9113 - val_loss: 0.3712 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4007 - accuracy: 0.9052 - val_loss: 0.4015 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3871 - accuracy: 0.9112 - val_loss: 0.4062 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3903 - accuracy: 0.9114 - val_loss: 0.3803 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3893 - accuracy: 0.9079 - val_loss: 0.4876 - val_accuracy: 0.8654 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3816 - accuracy: 0.9132 - val_loss: 0.4849 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3876 - accuracy: 0.9111 - val_loss: 0.4031 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3860 - accuracy: 0.9126 - val_loss: 0.3801 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3940 - accuracy: 0.9081 - val_loss: 0.5318 - val_accuracy: 0.8608 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3907 - accuracy: 0.9108 - val_loss: 0.3919 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3969 - accuracy: 0.9103 - val_loss: 0.3804 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3871 - accuracy: 0.9128 - val_loss: 0.3677 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3937 - accuracy: 0.9082 - val_loss: 0.3780 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3923 - accuracy: 0.9083 - val_loss: 0.3765 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3788 - accuracy: 0.9098 - val_loss: 0.3681 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3875 - accuracy: 0.9085 - val_loss: 0.3624 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3807 - accuracy: 0.9107 - val_loss: 0.4239 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3813 - accuracy: 0.9108 - val_loss: 0.3981 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3970 - accuracy: 0.9093 - val_loss: 0.3936 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3763 - accuracy: 0.9145 - val_loss: 0.3750 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3812 - accuracy: 0.9120 - val_loss: 0.3860 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3769 - accuracy: 0.9134 - val_loss: 0.4274 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3881 - accuracy: 0.9101 - val_loss: 0.3741 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3800 - accuracy: 0.9136 - val_loss: 0.3701 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3793 - accuracy: 0.9119 - val_loss: 0.3703 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3814 - accuracy: 0.9124 - val_loss: 0.3676 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3730 - accuracy: 0.9114 - val_loss: 0.5035 - val_accuracy: 0.8694 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3723 - accuracy: 0.9132 - val_loss: 0.4223 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3732 - accuracy: 0.9173 - val_loss: 0.3585 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3725 - accuracy: 0.9138 - val_loss: 0.3675 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3775 - accuracy: 0.9136 - val_loss: 0.5347 - val_accuracy: 0.8418 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3783 - accuracy: 0.9116 - val_loss: 0.3629 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3891 - accuracy: 0.9085 - val_loss: 0.3617 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3776 - accuracy: 0.9096 - val_loss: 0.3751 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3653 - accuracy: 0.9165 - val_loss: 0.3914 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3574 - accuracy: 0.9216 - val_loss: 0.3655 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3305 - accuracy: 0.9279 - val_loss: 0.3484 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3211 - accuracy: 0.9272 - val_loss: 0.3328 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3115 - accuracy: 0.9297 - val_loss: 0.3117 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3115 - accuracy: 0.9237 - val_loss: 0.3007 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2970 - accuracy: 0.9274 - val_loss: 0.3037 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2941 - accuracy: 0.9275 - val_loss: 0.3123 - val_accuracy: 0.9188 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2837 - accuracy: 0.9292 - val_loss: 0.2942 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2779 - accuracy: 0.9308 - val_loss: 0.2866 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2782 - accuracy: 0.9274 - val_loss: 0.2804 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2677 - accuracy: 0.9332 - val_loss: 0.2737 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2637 - accuracy: 0.9342 - val_loss: 0.2723 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2588 - accuracy: 0.9357 - val_loss: 0.2723 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2642 - accuracy: 0.9345 - val_loss: 0.2718 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2633 - accuracy: 0.9318 - val_loss: 0.2703 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2577 - accuracy: 0.9337 - val_loss: 0.2706 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2603 - accuracy: 0.9333 - val_loss: 0.2706 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2595 - accuracy: 0.9341 - val_loss: 0.2692 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2574 - accuracy: 0.9362 - val_loss: 0.2686 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2598 - accuracy: 0.9323 - val_loss: 0.2687 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2577 - accuracy: 0.9371 - val_loss: 0.2685 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2597 - accuracy: 0.9326 - val_loss: 0.2686 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2540 - accuracy: 0.9331 - val_loss: 0.2685 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2544 - accuracy: 0.9364 - val_loss: 0.2682 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2542 - accuracy: 0.9350 - val_loss: 0.2681 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2594 - accuracy: 0.9343 - val_loss: 0.2680 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2558 - accuracy: 0.9344 - val_loss: 0.2679 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2565 - accuracy: 0.9342 - val_loss: 0.2680 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2565 - accuracy: 0.9339 - val_loss: 0.2680 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 15ms/step - loss: 0.2880 - accuracy: 0.9250\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5816, TN:9859, FP:596, FN:675, loss0.28804177045822144, acc0.9249970494511979, sn0.8960098598058851, sp0.9429937828790053, f10.9014957761760832, auc0.974114045706961\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 27s 67ms/step - loss: 4.0008 - accuracy: 0.6489 - val_loss: 4.5913 - val_accuracy: 0.3965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 2.7612 - accuracy: 0.8243 - val_loss: 2.6029 - val_accuracy: 0.6298 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 1.9487 - accuracy: 0.8536 - val_loss: 1.6695 - val_accuracy: 0.8432 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 1.4273 - accuracy: 0.8571 - val_loss: 1.2892 - val_accuracy: 0.8504 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.0852 - accuracy: 0.8690 - val_loss: 1.0368 - val_accuracy: 0.8426 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.8607 - accuracy: 0.8801 - val_loss: 0.8216 - val_accuracy: 0.8686 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.7277 - accuracy: 0.8817 - val_loss: 0.8821 - val_accuracy: 0.8263 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.6381 - accuracy: 0.8901 - val_loss: 0.7553 - val_accuracy: 0.8410 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.5757 - accuracy: 0.8933 - val_loss: 0.7554 - val_accuracy: 0.8387 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.5359 - accuracy: 0.8991 - val_loss: 0.5163 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.5151 - accuracy: 0.8949 - val_loss: 0.5024 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.4928 - accuracy: 0.8977 - val_loss: 0.4560 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4648 - accuracy: 0.9029 - val_loss: 0.4367 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4602 - accuracy: 0.9010 - val_loss: 0.4681 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4483 - accuracy: 0.9008 - val_loss: 0.4865 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4322 - accuracy: 0.9043 - val_loss: 0.6059 - val_accuracy: 0.8207 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4332 - accuracy: 0.9019 - val_loss: 0.4879 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4208 - accuracy: 0.9066 - val_loss: 0.4228 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4259 - accuracy: 0.9035 - val_loss: 0.4390 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4157 - accuracy: 0.9069 - val_loss: 0.3917 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.4069 - accuracy: 0.9067 - val_loss: 0.3960 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.4140 - accuracy: 0.9061 - val_loss: 0.4052 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3979 - accuracy: 0.9072 - val_loss: 0.3874 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4065 - accuracy: 0.9067 - val_loss: 0.3675 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3936 - accuracy: 0.9104 - val_loss: 0.3878 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4079 - accuracy: 0.9048 - val_loss: 0.3829 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3933 - accuracy: 0.9130 - val_loss: 0.4390 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.4075 - accuracy: 0.9071 - val_loss: 0.3814 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3954 - accuracy: 0.9121 - val_loss: 0.4524 - val_accuracy: 0.8819 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4021 - accuracy: 0.9062 - val_loss: 0.3681 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3954 - accuracy: 0.9078 - val_loss: 0.3903 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3851 - accuracy: 0.9099 - val_loss: 0.4041 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3898 - accuracy: 0.9101 - val_loss: 0.4611 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3871 - accuracy: 0.9097 - val_loss: 0.4181 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3859 - accuracy: 0.9097 - val_loss: 0.3823 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3889 - accuracy: 0.9106 - val_loss: 0.3693 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3904 - accuracy: 0.9121 - val_loss: 0.4371 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3900 - accuracy: 0.9095 - val_loss: 0.3949 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3858 - accuracy: 0.9085 - val_loss: 0.3997 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3871 - accuracy: 0.9102 - val_loss: 0.4603 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3863 - accuracy: 0.9120 - val_loss: 0.3840 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3909 - accuracy: 0.9122 - val_loss: 0.4006 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3961 - accuracy: 0.9116 - val_loss: 0.3858 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3903 - accuracy: 0.9138 - val_loss: 0.3828 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3851 - accuracy: 0.9120 - val_loss: 0.3712 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3931 - accuracy: 0.9068 - val_loss: 0.3629 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3773 - accuracy: 0.9102 - val_loss: 0.3577 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3763 - accuracy: 0.9118 - val_loss: 0.3834 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3731 - accuracy: 0.9126 - val_loss: 0.4150 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3825 - accuracy: 0.9102 - val_loss: 0.3638 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3782 - accuracy: 0.9111 - val_loss: 0.3678 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3834 - accuracy: 0.9095 - val_loss: 0.3479 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3742 - accuracy: 0.9112 - val_loss: 0.3583 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3755 - accuracy: 0.9150 - val_loss: 0.3734 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3707 - accuracy: 0.9134 - val_loss: 0.3728 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3790 - accuracy: 0.9097 - val_loss: 0.5663 - val_accuracy: 0.8476 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3866 - accuracy: 0.9085 - val_loss: 0.5429 - val_accuracy: 0.8440 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3841 - accuracy: 0.9111 - val_loss: 0.3647 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3800 - accuracy: 0.9120 - val_loss: 0.3627 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3774 - accuracy: 0.9114 - val_loss: 0.3793 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3820 - accuracy: 0.9123 - val_loss: 0.3987 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3732 - accuracy: 0.9153 - val_loss: 0.4152 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3789 - accuracy: 0.9145 - val_loss: 0.4036 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3790 - accuracy: 0.9105 - val_loss: 0.3663 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3751 - accuracy: 0.9132 - val_loss: 0.3668 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3828 - accuracy: 0.9125 - val_loss: 0.4004 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3781 - accuracy: 0.9115 - val_loss: 0.3813 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3725 - accuracy: 0.9137 - val_loss: 0.3747 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3656 - accuracy: 0.9161 - val_loss: 0.3592 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3747 - accuracy: 0.9146 - val_loss: 0.3858 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3738 - accuracy: 0.9120 - val_loss: 0.3785 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3484 - accuracy: 0.9221 - val_loss: 0.3451 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3313 - accuracy: 0.9234 - val_loss: 0.3334 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3205 - accuracy: 0.9260 - val_loss: 0.3225 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3070 - accuracy: 0.9268 - val_loss: 0.3126 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2990 - accuracy: 0.9279 - val_loss: 0.3079 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2970 - accuracy: 0.9236 - val_loss: 0.2903 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.2855 - accuracy: 0.9278 - val_loss: 0.2916 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.2825 - accuracy: 0.9260 - val_loss: 0.3184 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2760 - accuracy: 0.9293 - val_loss: 0.2839 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2692 - accuracy: 0.9291 - val_loss: 0.2713 - val_accuracy: 0.9316 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2642 - accuracy: 0.9319 - val_loss: 0.2691 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2636 - accuracy: 0.9319 - val_loss: 0.2684 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2596 - accuracy: 0.9322 - val_loss: 0.2676 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2595 - accuracy: 0.9318 - val_loss: 0.2672 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2588 - accuracy: 0.9329 - val_loss: 0.2652 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2564 - accuracy: 0.9321 - val_loss: 0.2649 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.2593 - accuracy: 0.9313 - val_loss: 0.2651 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2543 - accuracy: 0.9344 - val_loss: 0.2652 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.2572 - accuracy: 0.9336 - val_loss: 0.2640 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2590 - accuracy: 0.9331 - val_loss: 0.2665 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2562 - accuracy: 0.9317 - val_loss: 0.2644 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2545 - accuracy: 0.9340 - val_loss: 0.2637 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2527 - accuracy: 0.9320 - val_loss: 0.2635 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2525 - accuracy: 0.9359 - val_loss: 0.2637 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2553 - accuracy: 0.9353 - val_loss: 0.2638 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.2540 - accuracy: 0.9343 - val_loss: 0.2635 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2537 - accuracy: 0.9342 - val_loss: 0.2633 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2492 - accuracy: 0.9359 - val_loss: 0.2631 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.2535 - accuracy: 0.9357 - val_loss: 0.2634 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 15ms/step - loss: 0.2833 - accuracy: 0.9261\n",
      "17/17 [==============================] - 1s 21ms/step\n",
      "TP:5841, TN:9853, FP:602, FN:650, loss0.28332796692848206, acc0.9261182579959872, sn0.8998613464797411, sp0.9424198947871831, f10.9032008659347458, auc0.9740119803891361\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 27s 68ms/step - loss: 4.0960 - accuracy: 0.6377 - val_loss: 4.4303 - val_accuracy: 0.3899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.9256 - accuracy: 0.7996 - val_loss: 3.4399 - val_accuracy: 0.3945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 2.1130 - accuracy: 0.8440 - val_loss: 2.0425 - val_accuracy: 0.6897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 1.5523 - accuracy: 0.8593 - val_loss: 1.3715 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 1.1825 - accuracy: 0.8685 - val_loss: 1.1063 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.9322 - accuracy: 0.8802 - val_loss: 0.9275 - val_accuracy: 0.8518 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.7765 - accuracy: 0.8867 - val_loss: 0.7369 - val_accuracy: 0.8730 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.6651 - accuracy: 0.8886 - val_loss: 0.6300 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.5892 - accuracy: 0.8938 - val_loss: 0.5573 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.5428 - accuracy: 0.8968 - val_loss: 0.5418 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.5047 - accuracy: 0.9025 - val_loss: 0.4868 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4745 - accuracy: 0.9038 - val_loss: 0.5568 - val_accuracy: 0.8744 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4698 - accuracy: 0.9008 - val_loss: 0.4325 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4429 - accuracy: 0.9052 - val_loss: 0.4153 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4405 - accuracy: 0.9029 - val_loss: 0.4240 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4315 - accuracy: 0.9040 - val_loss: 0.3953 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4255 - accuracy: 0.9036 - val_loss: 0.4002 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4157 - accuracy: 0.9026 - val_loss: 0.3984 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4089 - accuracy: 0.9060 - val_loss: 0.4279 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4015 - accuracy: 0.9063 - val_loss: 0.4276 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4046 - accuracy: 0.9072 - val_loss: 0.3826 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3961 - accuracy: 0.9076 - val_loss: 0.3841 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3986 - accuracy: 0.9080 - val_loss: 0.4218 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3948 - accuracy: 0.9091 - val_loss: 0.3732 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3921 - accuracy: 0.9107 - val_loss: 0.4062 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3956 - accuracy: 0.9073 - val_loss: 0.4895 - val_accuracy: 0.8718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3847 - accuracy: 0.9127 - val_loss: 0.3712 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3905 - accuracy: 0.9066 - val_loss: 0.4340 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3880 - accuracy: 0.9096 - val_loss: 0.3779 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3928 - accuracy: 0.9083 - val_loss: 0.3870 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3923 - accuracy: 0.9082 - val_loss: 0.4187 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3805 - accuracy: 0.9096 - val_loss: 0.3690 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3792 - accuracy: 0.9112 - val_loss: 0.4453 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3946 - accuracy: 0.9074 - val_loss: 0.3975 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3868 - accuracy: 0.9084 - val_loss: 0.4342 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3848 - accuracy: 0.9104 - val_loss: 0.4011 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3898 - accuracy: 0.9104 - val_loss: 0.3764 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3863 - accuracy: 0.9092 - val_loss: 0.3811 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3923 - accuracy: 0.9069 - val_loss: 0.4053 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3806 - accuracy: 0.9114 - val_loss: 0.6435 - val_accuracy: 0.8195 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3839 - accuracy: 0.9119 - val_loss: 0.4064 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3830 - accuracy: 0.9067 - val_loss: 0.5937 - val_accuracy: 0.8291 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3991 - accuracy: 0.9062 - val_loss: 0.3680 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3889 - accuracy: 0.9100 - val_loss: 0.4167 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3894 - accuracy: 0.9095 - val_loss: 0.3880 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3819 - accuracy: 0.9139 - val_loss: 0.3910 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3854 - accuracy: 0.9117 - val_loss: 0.3882 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3787 - accuracy: 0.9121 - val_loss: 0.3713 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3829 - accuracy: 0.9102 - val_loss: 0.3962 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3918 - accuracy: 0.9102 - val_loss: 0.3706 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3816 - accuracy: 0.9122 - val_loss: 0.3875 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3793 - accuracy: 0.9112 - val_loss: 0.3936 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3860 - accuracy: 0.9132 - val_loss: 0.3964 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3837 - accuracy: 0.9092 - val_loss: 0.3647 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3776 - accuracy: 0.9102 - val_loss: 0.3531 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3754 - accuracy: 0.9140 - val_loss: 0.3575 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3750 - accuracy: 0.9134 - val_loss: 0.4092 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3771 - accuracy: 0.9135 - val_loss: 0.3848 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3809 - accuracy: 0.9132 - val_loss: 0.4274 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3781 - accuracy: 0.9109 - val_loss: 0.4038 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3723 - accuracy: 0.9144 - val_loss: 0.3772 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3775 - accuracy: 0.9104 - val_loss: 0.4372 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3691 - accuracy: 0.9142 - val_loss: 0.3679 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3830 - accuracy: 0.9114 - val_loss: 0.3679 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3747 - accuracy: 0.9132 - val_loss: 0.3584 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3769 - accuracy: 0.9108 - val_loss: 0.4315 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3643 - accuracy: 0.9147 - val_loss: 0.5649 - val_accuracy: 0.8634 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3683 - accuracy: 0.9144 - val_loss: 0.4054 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3748 - accuracy: 0.9122 - val_loss: 0.3823 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3770 - accuracy: 0.9114 - val_loss: 0.3921 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3642 - accuracy: 0.9139 - val_loss: 0.3480 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3440 - accuracy: 0.9241 - val_loss: 0.3589 - val_accuracy: 0.9105 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3272 - accuracy: 0.9222 - val_loss: 0.3647 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3138 - accuracy: 0.9252 - val_loss: 0.3357 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3070 - accuracy: 0.9248 - val_loss: 0.3290 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2962 - accuracy: 0.9276 - val_loss: 0.3035 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2955 - accuracy: 0.9260 - val_loss: 0.3130 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2819 - accuracy: 0.9276 - val_loss: 0.2868 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2770 - accuracy: 0.9301 - val_loss: 0.2996 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2776 - accuracy: 0.9279 - val_loss: 0.2789 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2664 - accuracy: 0.9293 - val_loss: 0.2771 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2626 - accuracy: 0.9302 - val_loss: 0.2727 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2643 - accuracy: 0.9297 - val_loss: 0.2701 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2587 - accuracy: 0.9338 - val_loss: 0.2699 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2589 - accuracy: 0.9322 - val_loss: 0.2688 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2547 - accuracy: 0.9337 - val_loss: 0.2696 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2565 - accuracy: 0.9341 - val_loss: 0.2667 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2568 - accuracy: 0.9313 - val_loss: 0.2674 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2552 - accuracy: 0.9329 - val_loss: 0.2654 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2585 - accuracy: 0.9291 - val_loss: 0.2661 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2607 - accuracy: 0.9303 - val_loss: 0.2653 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2558 - accuracy: 0.9303 - val_loss: 0.2652 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2564 - accuracy: 0.9322 - val_loss: 0.2648 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2542 - accuracy: 0.9332 - val_loss: 0.2648 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2573 - accuracy: 0.9302 - val_loss: 0.2649 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2513 - accuracy: 0.9341 - val_loss: 0.2650 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2529 - accuracy: 0.9326 - val_loss: 0.2646 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2524 - accuracy: 0.9341 - val_loss: 0.2645 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2518 - accuracy: 0.9330 - val_loss: 0.2643 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2549 - accuracy: 0.9338 - val_loss: 0.2641 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2822 - accuracy: 0.9256\n",
      "17/17 [==============================] - 1s 21ms/step\n",
      "TP:5802, TN:9884, FP:571, FN:689, loss0.28221672773361206, acc0.9256461701876549, sn0.8938530272685257, sp0.9453849832615974, f10.9020522388059701, auc0.97392259495379\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 28s 63ms/step - loss: 4.0515 - accuracy: 0.6488 - val_loss: 4.4886 - val_accuracy: 0.3901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 2.9002 - accuracy: 0.8107 - val_loss: 2.6159 - val_accuracy: 0.7114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.0916 - accuracy: 0.8426 - val_loss: 1.8176 - val_accuracy: 0.8375 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 1.5345 - accuracy: 0.8587 - val_loss: 1.3248 - val_accuracy: 0.8678 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 1.1596 - accuracy: 0.8688 - val_loss: 1.0815 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.9132 - accuracy: 0.8807 - val_loss: 0.8988 - val_accuracy: 0.8506 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.7610 - accuracy: 0.8814 - val_loss: 0.7597 - val_accuracy: 0.8718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.6446 - accuracy: 0.8922 - val_loss: 0.6956 - val_accuracy: 0.8566 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.5771 - accuracy: 0.8926 - val_loss: 0.6193 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5454 - accuracy: 0.8916 - val_loss: 0.5181 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4894 - accuracy: 0.9009 - val_loss: 0.5801 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4812 - accuracy: 0.8951 - val_loss: 0.4887 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4468 - accuracy: 0.9043 - val_loss: 0.7066 - val_accuracy: 0.8343 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4480 - accuracy: 0.9001 - val_loss: 0.4652 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4321 - accuracy: 0.9027 - val_loss: 0.4085 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4251 - accuracy: 0.9055 - val_loss: 0.4639 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4148 - accuracy: 0.9050 - val_loss: 0.4198 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4084 - accuracy: 0.9082 - val_loss: 0.3913 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4169 - accuracy: 0.9055 - val_loss: 0.3973 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3986 - accuracy: 0.9094 - val_loss: 0.4221 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4005 - accuracy: 0.9091 - val_loss: 0.4138 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4102 - accuracy: 0.9061 - val_loss: 0.4095 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3995 - accuracy: 0.9083 - val_loss: 0.3805 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4094 - accuracy: 0.9076 - val_loss: 0.4261 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4041 - accuracy: 0.9062 - val_loss: 0.4413 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4006 - accuracy: 0.9078 - val_loss: 0.4126 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4028 - accuracy: 0.9098 - val_loss: 0.3966 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3969 - accuracy: 0.9062 - val_loss: 0.4199 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3935 - accuracy: 0.9080 - val_loss: 0.3847 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3962 - accuracy: 0.9087 - val_loss: 0.4421 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3996 - accuracy: 0.9073 - val_loss: 0.4377 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4006 - accuracy: 0.9102 - val_loss: 0.5825 - val_accuracy: 0.8510 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4062 - accuracy: 0.9063 - val_loss: 0.3983 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4022 - accuracy: 0.9070 - val_loss: 0.4173 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3882 - accuracy: 0.9083 - val_loss: 0.3835 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3995 - accuracy: 0.9071 - val_loss: 0.3888 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3885 - accuracy: 0.9118 - val_loss: 0.3982 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3877 - accuracy: 0.9121 - val_loss: 0.4606 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3965 - accuracy: 0.9124 - val_loss: 0.4071 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3922 - accuracy: 0.9080 - val_loss: 0.4408 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3940 - accuracy: 0.9068 - val_loss: 0.3870 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3775 - accuracy: 0.9138 - val_loss: 0.3726 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3857 - accuracy: 0.9120 - val_loss: 0.3813 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3841 - accuracy: 0.9098 - val_loss: 0.4057 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3826 - accuracy: 0.9109 - val_loss: 0.3663 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3785 - accuracy: 0.9114 - val_loss: 0.3767 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3797 - accuracy: 0.9139 - val_loss: 0.3639 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3960 - accuracy: 0.9067 - val_loss: 0.3886 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3910 - accuracy: 0.9094 - val_loss: 0.3656 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3810 - accuracy: 0.9114 - val_loss: 0.3779 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3811 - accuracy: 0.9132 - val_loss: 0.3607 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3763 - accuracy: 0.9137 - val_loss: 0.4027 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3841 - accuracy: 0.9116 - val_loss: 0.4409 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3780 - accuracy: 0.9126 - val_loss: 0.4243 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3712 - accuracy: 0.9131 - val_loss: 0.4074 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3857 - accuracy: 0.9079 - val_loss: 0.3886 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3810 - accuracy: 0.9146 - val_loss: 0.3751 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3899 - accuracy: 0.9101 - val_loss: 0.3782 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3824 - accuracy: 0.9116 - val_loss: 0.3838 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3739 - accuracy: 0.9119 - val_loss: 0.3743 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3756 - accuracy: 0.9147 - val_loss: 0.3802 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3777 - accuracy: 0.9113 - val_loss: 0.3742 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3708 - accuracy: 0.9131 - val_loss: 0.3569 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3792 - accuracy: 0.9121 - val_loss: 0.3994 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3695 - accuracy: 0.9131 - val_loss: 0.4169 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3825 - accuracy: 0.9108 - val_loss: 0.4026 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3799 - accuracy: 0.9122 - val_loss: 0.3830 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3697 - accuracy: 0.9148 - val_loss: 0.3531 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3806 - accuracy: 0.9108 - val_loss: 0.4901 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3670 - accuracy: 0.9159 - val_loss: 0.3573 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3697 - accuracy: 0.9150 - val_loss: 0.3513 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3479 - accuracy: 0.9213 - val_loss: 0.3395 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3296 - accuracy: 0.9271 - val_loss: 0.3530 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3206 - accuracy: 0.9236 - val_loss: 0.3328 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3078 - accuracy: 0.9268 - val_loss: 0.3147 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2954 - accuracy: 0.9300 - val_loss: 0.3136 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2899 - accuracy: 0.9282 - val_loss: 0.3018 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2861 - accuracy: 0.9290 - val_loss: 0.2878 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2833 - accuracy: 0.9269 - val_loss: 0.3043 - val_accuracy: 0.9194 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2751 - accuracy: 0.9284 - val_loss: 0.2972 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2780 - accuracy: 0.9279 - val_loss: 0.2832 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2661 - accuracy: 0.9290 - val_loss: 0.2707 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2644 - accuracy: 0.9316 - val_loss: 0.2695 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2659 - accuracy: 0.9330 - val_loss: 0.2693 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2616 - accuracy: 0.9323 - val_loss: 0.2690 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2590 - accuracy: 0.9319 - val_loss: 0.2674 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2565 - accuracy: 0.9335 - val_loss: 0.2662 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2572 - accuracy: 0.9332 - val_loss: 0.2655 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2577 - accuracy: 0.9330 - val_loss: 0.2652 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2532 - accuracy: 0.9332 - val_loss: 0.2659 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2560 - accuracy: 0.9353 - val_loss: 0.2660 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2588 - accuracy: 0.9310 - val_loss: 0.2656 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2585 - accuracy: 0.9301 - val_loss: 0.2656 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2524 - accuracy: 0.9344 - val_loss: 0.2655 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2542 - accuracy: 0.9336 - val_loss: 0.2655 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2572 - accuracy: 0.9332 - val_loss: 0.2655 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2496 - accuracy: 0.9352 - val_loss: 0.2656 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2516 - accuracy: 0.9352 - val_loss: 0.2656 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2516 - accuracy: 0.9367 - val_loss: 0.2654 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2524 - accuracy: 0.9338 - val_loss: 0.2653 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 15ms/step - loss: 0.2860 - accuracy: 0.9265\n",
      "17/17 [==============================] - 2s 21ms/step\n",
      "TP:5869, TN:9832, FP:623, FN:622, loss0.2860299050807953, acc0.9265313348282781, sn0.90417501155446, sp0.9404112864658059, f10.9041053685588847, auc0.9745070041799405\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 27s 67ms/step - loss: 4.0552 - accuracy: 0.6515 - val_loss: 4.0764 - val_accuracy: 0.3949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 2.9086 - accuracy: 0.8096 - val_loss: 2.4669 - val_accuracy: 0.8305 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.0847 - accuracy: 0.8476 - val_loss: 1.8030 - val_accuracy: 0.8365 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 1.5318 - accuracy: 0.8593 - val_loss: 1.3826 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 1.1628 - accuracy: 0.8693 - val_loss: 1.1598 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.9218 - accuracy: 0.8773 - val_loss: 0.9102 - val_accuracy: 0.8596 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.7567 - accuracy: 0.8849 - val_loss: 0.7741 - val_accuracy: 0.8714 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.6638 - accuracy: 0.8897 - val_loss: 0.7639 - val_accuracy: 0.8502 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.5964 - accuracy: 0.8931 - val_loss: 0.8085 - val_accuracy: 0.8201 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.5455 - accuracy: 0.8955 - val_loss: 0.5169 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.5061 - accuracy: 0.9020 - val_loss: 0.6241 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4934 - accuracy: 0.8991 - val_loss: 0.4864 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4613 - accuracy: 0.9028 - val_loss: 0.5197 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4542 - accuracy: 0.9005 - val_loss: 0.4166 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4384 - accuracy: 0.9061 - val_loss: 0.4133 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.4243 - accuracy: 0.9056 - val_loss: 0.4032 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4280 - accuracy: 0.9048 - val_loss: 0.4204 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4141 - accuracy: 0.9097 - val_loss: 0.4327 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 0.4153 - accuracy: 0.9066 - val_loss: 0.3921 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4085 - accuracy: 0.9069 - val_loss: 0.3895 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4084 - accuracy: 0.9085 - val_loss: 0.4532 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4187 - accuracy: 0.9074 - val_loss: 0.3858 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4069 - accuracy: 0.9097 - val_loss: 0.4139 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3979 - accuracy: 0.9096 - val_loss: 0.3900 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3978 - accuracy: 0.9097 - val_loss: 0.3845 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3984 - accuracy: 0.9129 - val_loss: 0.3739 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4015 - accuracy: 0.9069 - val_loss: 0.4353 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3974 - accuracy: 0.9097 - val_loss: 0.4010 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3958 - accuracy: 0.9116 - val_loss: 0.3890 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4030 - accuracy: 0.9080 - val_loss: 0.4423 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3965 - accuracy: 0.9084 - val_loss: 0.4123 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3971 - accuracy: 0.9108 - val_loss: 0.3801 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3909 - accuracy: 0.9075 - val_loss: 0.4673 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3936 - accuracy: 0.9079 - val_loss: 0.3651 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3968 - accuracy: 0.9076 - val_loss: 0.4002 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3912 - accuracy: 0.9068 - val_loss: 0.3801 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3883 - accuracy: 0.9119 - val_loss: 0.4876 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3953 - accuracy: 0.9071 - val_loss: 0.3767 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3912 - accuracy: 0.9120 - val_loss: 0.3997 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3938 - accuracy: 0.9072 - val_loss: 0.4100 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3847 - accuracy: 0.9126 - val_loss: 0.3637 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3915 - accuracy: 0.9111 - val_loss: 0.4098 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4036 - accuracy: 0.9072 - val_loss: 0.3763 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3897 - accuracy: 0.9107 - val_loss: 0.4155 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3813 - accuracy: 0.9146 - val_loss: 0.4010 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3920 - accuracy: 0.9138 - val_loss: 0.3786 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3977 - accuracy: 0.9102 - val_loss: 0.4027 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3857 - accuracy: 0.9121 - val_loss: 0.3844 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3958 - accuracy: 0.9120 - val_loss: 0.4184 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3912 - accuracy: 0.9103 - val_loss: 0.4379 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3918 - accuracy: 0.9107 - val_loss: 0.3708 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3734 - accuracy: 0.9122 - val_loss: 0.3642 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3800 - accuracy: 0.9117 - val_loss: 0.3587 - val_accuracy: 0.9252 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3914 - accuracy: 0.9097 - val_loss: 0.4556 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3740 - accuracy: 0.9148 - val_loss: 0.3684 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3738 - accuracy: 0.9161 - val_loss: 0.3788 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3824 - accuracy: 0.9111 - val_loss: 0.3573 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3776 - accuracy: 0.9140 - val_loss: 0.3735 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3887 - accuracy: 0.9111 - val_loss: 0.3679 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3728 - accuracy: 0.9154 - val_loss: 0.4261 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3817 - accuracy: 0.9140 - val_loss: 0.3863 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3722 - accuracy: 0.9167 - val_loss: 0.3563 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3780 - accuracy: 0.9116 - val_loss: 0.4217 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3804 - accuracy: 0.9132 - val_loss: 0.4200 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3832 - accuracy: 0.9109 - val_loss: 0.3701 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3748 - accuracy: 0.9128 - val_loss: 0.3680 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3699 - accuracy: 0.9120 - val_loss: 0.3605 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3768 - accuracy: 0.9143 - val_loss: 0.3633 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3724 - accuracy: 0.9118 - val_loss: 0.3571 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3694 - accuracy: 0.9138 - val_loss: 0.3807 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3733 - accuracy: 0.9089 - val_loss: 0.3643 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3433 - accuracy: 0.9217 - val_loss: 0.3827 - val_accuracy: 0.9049 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3252 - accuracy: 0.9246 - val_loss: 0.3410 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3113 - accuracy: 0.9279 - val_loss: 0.3428 - val_accuracy: 0.9128 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3053 - accuracy: 0.9267 - val_loss: 0.3470 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2972 - accuracy: 0.9288 - val_loss: 0.3118 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.2912 - accuracy: 0.9285 - val_loss: 0.3058 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2797 - accuracy: 0.9297 - val_loss: 0.3013 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2780 - accuracy: 0.9292 - val_loss: 0.2816 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2745 - accuracy: 0.9303 - val_loss: 0.3006 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.2664 - accuracy: 0.9308 - val_loss: 0.2858 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.2591 - accuracy: 0.9334 - val_loss: 0.2696 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.2606 - accuracy: 0.9324 - val_loss: 0.2667 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2558 - accuracy: 0.9362 - val_loss: 0.2655 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2627 - accuracy: 0.9286 - val_loss: 0.2643 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2555 - accuracy: 0.9339 - val_loss: 0.2635 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2574 - accuracy: 0.9314 - val_loss: 0.2634 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2545 - accuracy: 0.9347 - val_loss: 0.2631 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2549 - accuracy: 0.9343 - val_loss: 0.2614 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2566 - accuracy: 0.9340 - val_loss: 0.2630 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2512 - accuracy: 0.9338 - val_loss: 0.2607 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2499 - accuracy: 0.9339 - val_loss: 0.2609 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2567 - accuracy: 0.9343 - val_loss: 0.2610 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2511 - accuracy: 0.9324 - val_loss: 0.2610 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2500 - accuracy: 0.9348 - val_loss: 0.2609 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2541 - accuracy: 0.9347 - val_loss: 0.2610 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2556 - accuracy: 0.9320 - val_loss: 0.2611 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2512 - accuracy: 0.9343 - val_loss: 0.2610 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2551 - accuracy: 0.9339 - val_loss: 0.2609 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2437 - accuracy: 0.9373 - val_loss: 0.2609 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2810 - accuracy: 0.9257\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5834, TN:9853, FP:602, FN:657, loss0.2809785306453705, acc0.9257051811636965, sn0.8987829302110615, sp0.9424198947871831, f10.902606946700704, auc0.9748085437210231\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 19s 38ms/step - loss: 4.0223 - accuracy: 0.6548 - val_loss: 3.6347 - val_accuracy: 0.4537 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 2.8882 - accuracy: 0.7884 - val_loss: 2.7559 - val_accuracy: 0.5676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 2.0681 - accuracy: 0.8371 - val_loss: 2.0302 - val_accuracy: 0.6338 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.4949 - accuracy: 0.8542 - val_loss: 1.2891 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 1.1285 - accuracy: 0.8678 - val_loss: 0.9936 - val_accuracy: 0.8702 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.8957 - accuracy: 0.8742 - val_loss: 0.8213 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.7461 - accuracy: 0.8808 - val_loss: 0.6849 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.6478 - accuracy: 0.8868 - val_loss: 0.6211 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.5852 - accuracy: 0.8891 - val_loss: 0.6809 - val_accuracy: 0.8508 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.5259 - accuracy: 0.8989 - val_loss: 0.5380 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.5066 - accuracy: 0.8972 - val_loss: 0.4922 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4687 - accuracy: 0.9047 - val_loss: 0.4542 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4575 - accuracy: 0.9020 - val_loss: 0.6003 - val_accuracy: 0.8544 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4425 - accuracy: 0.9038 - val_loss: 0.4584 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4367 - accuracy: 0.9025 - val_loss: 0.4383 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4358 - accuracy: 0.9026 - val_loss: 0.4218 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4230 - accuracy: 0.9053 - val_loss: 0.4621 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4152 - accuracy: 0.9079 - val_loss: 0.4046 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4060 - accuracy: 0.9043 - val_loss: 0.3894 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4079 - accuracy: 0.9066 - val_loss: 0.3695 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4009 - accuracy: 0.9049 - val_loss: 0.4408 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4013 - accuracy: 0.9057 - val_loss: 0.3839 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3980 - accuracy: 0.9089 - val_loss: 0.4332 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4048 - accuracy: 0.9066 - val_loss: 0.3965 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3925 - accuracy: 0.9096 - val_loss: 0.3810 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4025 - accuracy: 0.9078 - val_loss: 0.5761 - val_accuracy: 0.8367 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3975 - accuracy: 0.9065 - val_loss: 0.3895 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3924 - accuracy: 0.9079 - val_loss: 0.3663 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3826 - accuracy: 0.9085 - val_loss: 0.3835 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3908 - accuracy: 0.9110 - val_loss: 0.4514 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3991 - accuracy: 0.9072 - val_loss: 0.3964 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3892 - accuracy: 0.9123 - val_loss: 0.4100 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3953 - accuracy: 0.9111 - val_loss: 0.3801 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3890 - accuracy: 0.9098 - val_loss: 0.4662 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3836 - accuracy: 0.9135 - val_loss: 0.3668 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3943 - accuracy: 0.9108 - val_loss: 0.3924 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3927 - accuracy: 0.9120 - val_loss: 0.4251 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3953 - accuracy: 0.9073 - val_loss: 0.4061 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3833 - accuracy: 0.9103 - val_loss: 0.3679 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3945 - accuracy: 0.9089 - val_loss: 0.3821 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3820 - accuracy: 0.9138 - val_loss: 0.3848 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3876 - accuracy: 0.9079 - val_loss: 0.4464 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3856 - accuracy: 0.9126 - val_loss: 0.3898 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3861 - accuracy: 0.9125 - val_loss: 0.3768 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3860 - accuracy: 0.9092 - val_loss: 0.4544 - val_accuracy: 0.8742 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3833 - accuracy: 0.9132 - val_loss: 0.3728 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3938 - accuracy: 0.9085 - val_loss: 0.3635 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3811 - accuracy: 0.9104 - val_loss: 0.3978 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3839 - accuracy: 0.9106 - val_loss: 0.3816 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3893 - accuracy: 0.9097 - val_loss: 0.3658 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3891 - accuracy: 0.9103 - val_loss: 0.3914 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3765 - accuracy: 0.9129 - val_loss: 0.3755 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3827 - accuracy: 0.9126 - val_loss: 0.4644 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3853 - accuracy: 0.9112 - val_loss: 0.4924 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3804 - accuracy: 0.9145 - val_loss: 0.3850 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3818 - accuracy: 0.9097 - val_loss: 0.3764 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3719 - accuracy: 0.9126 - val_loss: 0.3710 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3773 - accuracy: 0.9137 - val_loss: 0.3913 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3841 - accuracy: 0.9091 - val_loss: 0.3860 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3769 - accuracy: 0.9144 - val_loss: 0.3655 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3768 - accuracy: 0.9114 - val_loss: 0.5030 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3779 - accuracy: 0.9119 - val_loss: 0.3551 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3750 - accuracy: 0.9127 - val_loss: 0.3661 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3746 - accuracy: 0.9118 - val_loss: 0.3623 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3763 - accuracy: 0.9087 - val_loss: 0.3597 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3719 - accuracy: 0.9125 - val_loss: 0.3587 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3673 - accuracy: 0.9142 - val_loss: 0.3932 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3788 - accuracy: 0.9105 - val_loss: 0.4565 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3705 - accuracy: 0.9091 - val_loss: 0.3913 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3728 - accuracy: 0.9116 - val_loss: 0.4167 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3604 - accuracy: 0.9151 - val_loss: 0.3771 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3412 - accuracy: 0.9220 - val_loss: 0.3491 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3260 - accuracy: 0.9254 - val_loss: 0.3288 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3151 - accuracy: 0.9249 - val_loss: 0.3145 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3032 - accuracy: 0.9273 - val_loss: 0.3139 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2964 - accuracy: 0.9255 - val_loss: 0.2970 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2893 - accuracy: 0.9279 - val_loss: 0.2986 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2834 - accuracy: 0.9283 - val_loss: 0.3050 - val_accuracy: 0.9158 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2736 - accuracy: 0.9306 - val_loss: 0.3012 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2722 - accuracy: 0.9303 - val_loss: 0.2852 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2759 - accuracy: 0.9295 - val_loss: 0.2734 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2641 - accuracy: 0.9308 - val_loss: 0.2705 - val_accuracy: 0.9276 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2646 - accuracy: 0.9306 - val_loss: 0.2672 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2580 - accuracy: 0.9305 - val_loss: 0.2663 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2585 - accuracy: 0.9304 - val_loss: 0.2659 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2614 - accuracy: 0.9326 - val_loss: 0.2673 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2572 - accuracy: 0.9348 - val_loss: 0.2659 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2588 - accuracy: 0.9319 - val_loss: 0.2652 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2532 - accuracy: 0.9346 - val_loss: 0.2640 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2543 - accuracy: 0.9351 - val_loss: 0.2638 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2539 - accuracy: 0.9344 - val_loss: 0.2650 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2566 - accuracy: 0.9342 - val_loss: 0.2639 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2548 - accuracy: 0.9336 - val_loss: 0.2637 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2526 - accuracy: 0.9326 - val_loss: 0.2637 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2517 - accuracy: 0.9320 - val_loss: 0.2638 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2520 - accuracy: 0.9321 - val_loss: 0.2636 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2538 - accuracy: 0.9340 - val_loss: 0.2637 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2546 - accuracy: 0.9350 - val_loss: 0.2637 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2543 - accuracy: 0.9337 - val_loss: 0.2636 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2512 - accuracy: 0.9321 - val_loss: 0.2637 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2840 - accuracy: 0.9258\n",
      "17/17 [==============================] - 1s 14ms/step\n",
      "TP:5858, TN:9831, FP:624, FN:633, loss0.2840217351913452, acc0.9258232031157795, sn0.9024803574179633, sp0.9403156384505021, f10.9031064518615586, auc0.9741112385975916\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 38ms/step - loss: 4.1136 - accuracy: 0.6327 - val_loss: 4.1284 - val_accuracy: 0.3945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 2.9668 - accuracy: 0.8077 - val_loss: 2.9003 - val_accuracy: 0.5726 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 2.1873 - accuracy: 0.8408 - val_loss: 1.9073 - val_accuracy: 0.8291 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 1.6059 - accuracy: 0.8616 - val_loss: 1.4806 - val_accuracy: 0.8127 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 1.2327 - accuracy: 0.8658 - val_loss: 1.0660 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.9754 - accuracy: 0.8736 - val_loss: 0.8803 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.7992 - accuracy: 0.8827 - val_loss: 0.8003 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.6849 - accuracy: 0.8847 - val_loss: 0.7381 - val_accuracy: 0.8632 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.6116 - accuracy: 0.8918 - val_loss: 0.6593 - val_accuracy: 0.8692 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.5466 - accuracy: 0.8987 - val_loss: 0.6004 - val_accuracy: 0.8698 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.5111 - accuracy: 0.8996 - val_loss: 0.5338 - val_accuracy: 0.8805 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4858 - accuracy: 0.9027 - val_loss: 0.4735 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4668 - accuracy: 0.9009 - val_loss: 0.4309 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4504 - accuracy: 0.9014 - val_loss: 0.4237 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4368 - accuracy: 0.9032 - val_loss: 0.4186 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4284 - accuracy: 0.9066 - val_loss: 0.4902 - val_accuracy: 0.8819 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4276 - accuracy: 0.9020 - val_loss: 0.4031 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4102 - accuracy: 0.9067 - val_loss: 0.4109 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4142 - accuracy: 0.9043 - val_loss: 0.4271 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4098 - accuracy: 0.9067 - val_loss: 0.3752 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3957 - accuracy: 0.9098 - val_loss: 0.3995 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3991 - accuracy: 0.9069 - val_loss: 0.4982 - val_accuracy: 0.8550 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4012 - accuracy: 0.9086 - val_loss: 0.3866 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4055 - accuracy: 0.9053 - val_loss: 0.3712 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3818 - accuracy: 0.9114 - val_loss: 0.3743 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3951 - accuracy: 0.9081 - val_loss: 0.6004 - val_accuracy: 0.8241 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3960 - accuracy: 0.9079 - val_loss: 0.3778 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3972 - accuracy: 0.9114 - val_loss: 0.4198 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3969 - accuracy: 0.9104 - val_loss: 0.3823 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3919 - accuracy: 0.9077 - val_loss: 0.4493 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3918 - accuracy: 0.9102 - val_loss: 0.3712 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3996 - accuracy: 0.9079 - val_loss: 0.3786 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3901 - accuracy: 0.9094 - val_loss: 0.3721 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3915 - accuracy: 0.9114 - val_loss: 0.4046 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3806 - accuracy: 0.9124 - val_loss: 0.3909 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3947 - accuracy: 0.9079 - val_loss: 0.4940 - val_accuracy: 0.8576 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3803 - accuracy: 0.9114 - val_loss: 0.3829 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3854 - accuracy: 0.9106 - val_loss: 0.3850 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3902 - accuracy: 0.9110 - val_loss: 0.3735 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3938 - accuracy: 0.9099 - val_loss: 0.3951 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3816 - accuracy: 0.9113 - val_loss: 0.3573 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3778 - accuracy: 0.9111 - val_loss: 0.3645 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3844 - accuracy: 0.9102 - val_loss: 0.3848 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3910 - accuracy: 0.9093 - val_loss: 0.3670 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3791 - accuracy: 0.9092 - val_loss: 0.3810 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3870 - accuracy: 0.9112 - val_loss: 0.3997 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3792 - accuracy: 0.9130 - val_loss: 0.3712 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3802 - accuracy: 0.9100 - val_loss: 0.4225 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3844 - accuracy: 0.9068 - val_loss: 0.3824 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3794 - accuracy: 0.9130 - val_loss: 0.4567 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3872 - accuracy: 0.9105 - val_loss: 0.4088 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3864 - accuracy: 0.9134 - val_loss: 0.3936 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3760 - accuracy: 0.9153 - val_loss: 0.3846 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3827 - accuracy: 0.9144 - val_loss: 0.5223 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3785 - accuracy: 0.9127 - val_loss: 0.3984 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3821 - accuracy: 0.9118 - val_loss: 0.3726 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3730 - accuracy: 0.9165 - val_loss: 0.3681 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3740 - accuracy: 0.9136 - val_loss: 0.3563 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3721 - accuracy: 0.9122 - val_loss: 0.4079 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3784 - accuracy: 0.9132 - val_loss: 0.4445 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3791 - accuracy: 0.9143 - val_loss: 0.4073 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3791 - accuracy: 0.9123 - val_loss: 0.3582 - val_accuracy: 0.9254 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3811 - accuracy: 0.9138 - val_loss: 0.3542 - val_accuracy: 0.9230 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3936 - accuracy: 0.9136 - val_loss: 0.3724 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3683 - accuracy: 0.9145 - val_loss: 0.3785 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3648 - accuracy: 0.9154 - val_loss: 0.3736 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3740 - accuracy: 0.9119 - val_loss: 0.3579 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3712 - accuracy: 0.9144 - val_loss: 0.4143 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3735 - accuracy: 0.9140 - val_loss: 0.3475 - val_accuracy: 0.9246 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3783 - accuracy: 0.9124 - val_loss: 0.3615 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3703 - accuracy: 0.9136 - val_loss: 0.3642 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3422 - accuracy: 0.9228 - val_loss: 0.3465 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3205 - accuracy: 0.9295 - val_loss: 0.3594 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3142 - accuracy: 0.9279 - val_loss: 0.3131 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3032 - accuracy: 0.9260 - val_loss: 0.3013 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2968 - accuracy: 0.9298 - val_loss: 0.3109 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2894 - accuracy: 0.9307 - val_loss: 0.2890 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2831 - accuracy: 0.9307 - val_loss: 0.3054 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2826 - accuracy: 0.9268 - val_loss: 0.2784 - val_accuracy: 0.9314 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2750 - accuracy: 0.9290 - val_loss: 0.2772 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2705 - accuracy: 0.9288 - val_loss: 0.2692 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2610 - accuracy: 0.9306 - val_loss: 0.2664 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2619 - accuracy: 0.9319 - val_loss: 0.2653 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2603 - accuracy: 0.9316 - val_loss: 0.2651 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2628 - accuracy: 0.9337 - val_loss: 0.2637 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2563 - accuracy: 0.9336 - val_loss: 0.2640 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2581 - accuracy: 0.9338 - val_loss: 0.2629 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2560 - accuracy: 0.9332 - val_loss: 0.2631 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2569 - accuracy: 0.9336 - val_loss: 0.2653 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2559 - accuracy: 0.9341 - val_loss: 0.2634 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2534 - accuracy: 0.9348 - val_loss: 0.2617 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2532 - accuracy: 0.9348 - val_loss: 0.2624 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2555 - accuracy: 0.9340 - val_loss: 0.2623 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2493 - accuracy: 0.9349 - val_loss: 0.2623 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2519 - accuracy: 0.9344 - val_loss: 0.2624 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2561 - accuracy: 0.9335 - val_loss: 0.2624 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2565 - accuracy: 0.9332 - val_loss: 0.2629 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2521 - accuracy: 0.9332 - val_loss: 0.2626 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2537 - accuracy: 0.9344 - val_loss: 0.2627 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2525 - accuracy: 0.9335 - val_loss: 0.2627 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2882 - accuracy: 0.9245\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5888, TN:9779, FP:676, FN:603, loss0.28823116421699524, acc0.9245249616428656, sn0.9071021414265906, sp0.9353419416547106, f10.9020298736116431, auc0.9740588907968883\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 19s 40ms/step - loss: 4.1265 - accuracy: 0.6487 - val_loss: 4.8962 - val_accuracy: 0.4063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 3.0073 - accuracy: 0.8209 - val_loss: 2.6022 - val_accuracy: 0.8223 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 2.2295 - accuracy: 0.8455 - val_loss: 1.9209 - val_accuracy: 0.8498 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 1.6645 - accuracy: 0.8587 - val_loss: 1.4661 - val_accuracy: 0.8568 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 1.2718 - accuracy: 0.8709 - val_loss: 1.1558 - val_accuracy: 0.8642 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 1.0081 - accuracy: 0.8761 - val_loss: 1.0908 - val_accuracy: 0.8377 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.8324 - accuracy: 0.8828 - val_loss: 0.7893 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.7122 - accuracy: 0.8917 - val_loss: 0.7212 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.6316 - accuracy: 0.8902 - val_loss: 0.6701 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.5781 - accuracy: 0.8939 - val_loss: 0.5808 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.5280 - accuracy: 0.9028 - val_loss: 0.5291 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.5103 - accuracy: 0.8996 - val_loss: 0.4808 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4882 - accuracy: 0.9017 - val_loss: 0.4717 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4629 - accuracy: 0.9030 - val_loss: 0.4661 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4523 - accuracy: 0.9049 - val_loss: 0.4247 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4470 - accuracy: 0.9012 - val_loss: 0.5880 - val_accuracy: 0.8518 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4323 - accuracy: 0.9038 - val_loss: 0.5834 - val_accuracy: 0.8558 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4238 - accuracy: 0.9019 - val_loss: 0.3964 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4190 - accuracy: 0.9061 - val_loss: 0.3995 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4147 - accuracy: 0.9079 - val_loss: 0.3868 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4102 - accuracy: 0.9088 - val_loss: 0.3938 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4059 - accuracy: 0.9063 - val_loss: 0.4775 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3996 - accuracy: 0.9087 - val_loss: 0.3860 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4056 - accuracy: 0.9086 - val_loss: 0.3938 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4011 - accuracy: 0.9073 - val_loss: 0.4007 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3976 - accuracy: 0.9091 - val_loss: 0.3927 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4023 - accuracy: 0.9092 - val_loss: 0.4688 - val_accuracy: 0.8783 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3971 - accuracy: 0.9100 - val_loss: 0.4132 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4064 - accuracy: 0.9088 - val_loss: 0.4481 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3967 - accuracy: 0.9120 - val_loss: 0.4006 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3958 - accuracy: 0.9080 - val_loss: 0.3886 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3938 - accuracy: 0.9104 - val_loss: 0.3905 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3992 - accuracy: 0.9064 - val_loss: 0.3763 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3920 - accuracy: 0.9110 - val_loss: 0.3712 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4012 - accuracy: 0.9068 - val_loss: 0.5417 - val_accuracy: 0.8420 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3920 - accuracy: 0.9081 - val_loss: 0.3632 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3844 - accuracy: 0.9122 - val_loss: 0.3988 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3887 - accuracy: 0.9080 - val_loss: 0.5909 - val_accuracy: 0.8063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3861 - accuracy: 0.9102 - val_loss: 0.3734 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3863 - accuracy: 0.9105 - val_loss: 0.3836 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3901 - accuracy: 0.9099 - val_loss: 0.4293 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3891 - accuracy: 0.9138 - val_loss: 0.3848 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3931 - accuracy: 0.9126 - val_loss: 0.3954 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3852 - accuracy: 0.9109 - val_loss: 0.3891 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3796 - accuracy: 0.9131 - val_loss: 0.3719 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3785 - accuracy: 0.9125 - val_loss: 0.3910 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3846 - accuracy: 0.9138 - val_loss: 0.3632 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3835 - accuracy: 0.9074 - val_loss: 0.5695 - val_accuracy: 0.8259 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3875 - accuracy: 0.9101 - val_loss: 0.3866 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3745 - accuracy: 0.9140 - val_loss: 0.3660 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3862 - accuracy: 0.9120 - val_loss: 0.3664 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3939 - accuracy: 0.9070 - val_loss: 0.4272 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3778 - accuracy: 0.9145 - val_loss: 0.3728 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3834 - accuracy: 0.9104 - val_loss: 0.3759 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3806 - accuracy: 0.9120 - val_loss: 0.3625 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3868 - accuracy: 0.9094 - val_loss: 0.4047 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3781 - accuracy: 0.9125 - val_loss: 0.4529 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3820 - accuracy: 0.9112 - val_loss: 0.4285 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3824 - accuracy: 0.9138 - val_loss: 0.4137 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3698 - accuracy: 0.9160 - val_loss: 0.3535 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3755 - accuracy: 0.9113 - val_loss: 0.3555 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3802 - accuracy: 0.9133 - val_loss: 0.3767 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3752 - accuracy: 0.9116 - val_loss: 0.3803 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3832 - accuracy: 0.9094 - val_loss: 0.3869 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3745 - accuracy: 0.9113 - val_loss: 0.3803 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3837 - accuracy: 0.9095 - val_loss: 0.4533 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3760 - accuracy: 0.9118 - val_loss: 0.3639 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3776 - accuracy: 0.9132 - val_loss: 0.3731 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3771 - accuracy: 0.9140 - val_loss: 0.3605 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3722 - accuracy: 0.9145 - val_loss: 0.3686 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3726 - accuracy: 0.9099 - val_loss: 0.3725 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3433 - accuracy: 0.9240 - val_loss: 0.3429 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3274 - accuracy: 0.9232 - val_loss: 0.3353 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3151 - accuracy: 0.9250 - val_loss: 0.3381 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3042 - accuracy: 0.9281 - val_loss: 0.3213 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2966 - accuracy: 0.9275 - val_loss: 0.3019 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2925 - accuracy: 0.9276 - val_loss: 0.3139 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2856 - accuracy: 0.9285 - val_loss: 0.2958 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2776 - accuracy: 0.9285 - val_loss: 0.2991 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2759 - accuracy: 0.9282 - val_loss: 0.2891 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2701 - accuracy: 0.9302 - val_loss: 0.2708 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2642 - accuracy: 0.9310 - val_loss: 0.2706 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2615 - accuracy: 0.9318 - val_loss: 0.2715 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2675 - accuracy: 0.9281 - val_loss: 0.2678 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2601 - accuracy: 0.9320 - val_loss: 0.2672 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2548 - accuracy: 0.9343 - val_loss: 0.2674 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2572 - accuracy: 0.9316 - val_loss: 0.2662 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2543 - accuracy: 0.9333 - val_loss: 0.2658 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2581 - accuracy: 0.9350 - val_loss: 0.2654 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2550 - accuracy: 0.9362 - val_loss: 0.2655 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2563 - accuracy: 0.9309 - val_loss: 0.2643 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2517 - accuracy: 0.9363 - val_loss: 0.2644 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2497 - accuracy: 0.9347 - val_loss: 0.2647 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2544 - accuracy: 0.9343 - val_loss: 0.2650 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2505 - accuracy: 0.9367 - val_loss: 0.2647 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2546 - accuracy: 0.9347 - val_loss: 0.2648 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2506 - accuracy: 0.9352 - val_loss: 0.2648 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2540 - accuracy: 0.9338 - val_loss: 0.2648 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2547 - accuracy: 0.9335 - val_loss: 0.2647 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2557 - accuracy: 0.9325 - val_loss: 0.2647 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2846 - accuracy: 0.9261\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5853, TN:9841, FP:614, FN:638, loss0.28457099199295044, acc0.9261182579959872, sn0.9017100600831921, sp0.941272118603539, f10.9033801512579102, auc0.9742274426106381\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 36ms/step - loss: 3.9880 - accuracy: 0.6630 - val_loss: 3.5154 - val_accuracy: 0.5856 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 2.8570 - accuracy: 0.7833 - val_loss: 2.9651 - val_accuracy: 0.4489 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 2.0255 - accuracy: 0.8371 - val_loss: 1.8689 - val_accuracy: 0.7329 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.4745 - accuracy: 0.8555 - val_loss: 1.4464 - val_accuracy: 0.7561 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 1.1223 - accuracy: 0.8649 - val_loss: 0.9717 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.8911 - accuracy: 0.8749 - val_loss: 0.8489 - val_accuracy: 0.8642 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.7390 - accuracy: 0.8861 - val_loss: 0.6992 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.6452 - accuracy: 0.8880 - val_loss: 0.6778 - val_accuracy: 0.8698 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.5835 - accuracy: 0.8935 - val_loss: 0.5472 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.5267 - accuracy: 0.8997 - val_loss: 0.6067 - val_accuracy: 0.8608 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4960 - accuracy: 0.8993 - val_loss: 0.6156 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4802 - accuracy: 0.9012 - val_loss: 0.4433 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4553 - accuracy: 0.9032 - val_loss: 0.5377 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4495 - accuracy: 0.9029 - val_loss: 0.5470 - val_accuracy: 0.8662 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4390 - accuracy: 0.9042 - val_loss: 0.5328 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4303 - accuracy: 0.9056 - val_loss: 0.4354 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4249 - accuracy: 0.9076 - val_loss: 0.5304 - val_accuracy: 0.8616 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4129 - accuracy: 0.9045 - val_loss: 0.4040 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4078 - accuracy: 0.9062 - val_loss: 0.4032 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4110 - accuracy: 0.9062 - val_loss: 0.4258 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4147 - accuracy: 0.9058 - val_loss: 0.5723 - val_accuracy: 0.8387 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4064 - accuracy: 0.9071 - val_loss: 0.3807 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3987 - accuracy: 0.9091 - val_loss: 0.3953 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3970 - accuracy: 0.9080 - val_loss: 0.3975 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3963 - accuracy: 0.9096 - val_loss: 0.4071 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3970 - accuracy: 0.9069 - val_loss: 0.4065 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3993 - accuracy: 0.9037 - val_loss: 0.3863 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3965 - accuracy: 0.9083 - val_loss: 0.3757 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3859 - accuracy: 0.9126 - val_loss: 0.4727 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3937 - accuracy: 0.9074 - val_loss: 0.4071 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3807 - accuracy: 0.9132 - val_loss: 0.3596 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3836 - accuracy: 0.9100 - val_loss: 0.3808 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3932 - accuracy: 0.9090 - val_loss: 0.3781 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3789 - accuracy: 0.9108 - val_loss: 0.5009 - val_accuracy: 0.8486 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4035 - accuracy: 0.9055 - val_loss: 0.3626 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3897 - accuracy: 0.9107 - val_loss: 0.4210 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3861 - accuracy: 0.9091 - val_loss: 0.4488 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3849 - accuracy: 0.9132 - val_loss: 0.4190 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3769 - accuracy: 0.9126 - val_loss: 0.4096 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3894 - accuracy: 0.9104 - val_loss: 0.3739 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3825 - accuracy: 0.9128 - val_loss: 0.4448 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3854 - accuracy: 0.9120 - val_loss: 0.4085 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3760 - accuracy: 0.9119 - val_loss: 0.3788 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3891 - accuracy: 0.9110 - val_loss: 0.4897 - val_accuracy: 0.8704 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3823 - accuracy: 0.9107 - val_loss: 0.3647 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3761 - accuracy: 0.9106 - val_loss: 0.3687 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3751 - accuracy: 0.9106 - val_loss: 0.4300 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3736 - accuracy: 0.9154 - val_loss: 0.3983 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3773 - accuracy: 0.9126 - val_loss: 0.4292 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3804 - accuracy: 0.9112 - val_loss: 0.4400 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3813 - accuracy: 0.9125 - val_loss: 0.3747 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3763 - accuracy: 0.9109 - val_loss: 0.3743 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3689 - accuracy: 0.9132 - val_loss: 0.3436 - val_accuracy: 0.9232 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3662 - accuracy: 0.9149 - val_loss: 0.4201 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3725 - accuracy: 0.9119 - val_loss: 0.3682 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3726 - accuracy: 0.9109 - val_loss: 0.3686 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3655 - accuracy: 0.9148 - val_loss: 0.3593 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3769 - accuracy: 0.9139 - val_loss: 0.3948 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3725 - accuracy: 0.9133 - val_loss: 0.3997 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3720 - accuracy: 0.9104 - val_loss: 0.3727 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3776 - accuracy: 0.9099 - val_loss: 0.3778 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3772 - accuracy: 0.9113 - val_loss: 0.3485 - val_accuracy: 0.9234 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3649 - accuracy: 0.9147 - val_loss: 0.3519 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3653 - accuracy: 0.9145 - val_loss: 0.4071 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3676 - accuracy: 0.9153 - val_loss: 0.3935 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3845 - accuracy: 0.9126 - val_loss: 0.3926 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3705 - accuracy: 0.9120 - val_loss: 0.3479 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3648 - accuracy: 0.9110 - val_loss: 0.4080 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3796 - accuracy: 0.9105 - val_loss: 0.3755 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3666 - accuracy: 0.9144 - val_loss: 0.3822 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3663 - accuracy: 0.9172 - val_loss: 0.3579 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3467 - accuracy: 0.9220 - val_loss: 0.3517 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3282 - accuracy: 0.9267 - val_loss: 0.3400 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3209 - accuracy: 0.9260 - val_loss: 0.3197 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3102 - accuracy: 0.9276 - val_loss: 0.3128 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3050 - accuracy: 0.9253 - val_loss: 0.3035 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2933 - accuracy: 0.9273 - val_loss: 0.3090 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2824 - accuracy: 0.9302 - val_loss: 0.2943 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2813 - accuracy: 0.9297 - val_loss: 0.2786 - val_accuracy: 0.9308 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2770 - accuracy: 0.9294 - val_loss: 0.2860 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2727 - accuracy: 0.9300 - val_loss: 0.2700 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2658 - accuracy: 0.9307 - val_loss: 0.2690 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2656 - accuracy: 0.9328 - val_loss: 0.2679 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2600 - accuracy: 0.9320 - val_loss: 0.2656 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2570 - accuracy: 0.9347 - val_loss: 0.2650 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2614 - accuracy: 0.9314 - val_loss: 0.2644 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2622 - accuracy: 0.9326 - val_loss: 0.2637 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2558 - accuracy: 0.9350 - val_loss: 0.2641 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2593 - accuracy: 0.9338 - val_loss: 0.2627 - val_accuracy: 0.9336 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2587 - accuracy: 0.9332 - val_loss: 0.2625 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2543 - accuracy: 0.9353 - val_loss: 0.2623 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2543 - accuracy: 0.9323 - val_loss: 0.2623 - val_accuracy: 0.9342 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2566 - accuracy: 0.9339 - val_loss: 0.2623 - val_accuracy: 0.9342 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2563 - accuracy: 0.9320 - val_loss: 0.2623 - val_accuracy: 0.9350 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2539 - accuracy: 0.9366 - val_loss: 0.2623 - val_accuracy: 0.9348 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2557 - accuracy: 0.9367 - val_loss: 0.2623 - val_accuracy: 0.9350 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2524 - accuracy: 0.9344 - val_loss: 0.2622 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2525 - accuracy: 0.9343 - val_loss: 0.2620 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2571 - accuracy: 0.9360 - val_loss: 0.2620 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2552 - accuracy: 0.9364 - val_loss: 0.2621 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2842 - accuracy: 0.9263\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5849, TN:9848, FP:607, FN:642, loss0.2841721177101135, acc0.9262952909241119, sn0.9010938222153752, sp0.9419416547106647, f10.9035297752375069, auc0.9744761996542908\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 40ms/step - loss: 4.0593 - accuracy: 0.6818 - val_loss: 4.9791 - val_accuracy: 0.4089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 2.9722 - accuracy: 0.8216 - val_loss: 2.9091 - val_accuracy: 0.5834 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 2.1782 - accuracy: 0.8472 - val_loss: 1.9158 - val_accuracy: 0.8329 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 1.6079 - accuracy: 0.8612 - val_loss: 1.4207 - val_accuracy: 0.8534 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.2189 - accuracy: 0.8701 - val_loss: 1.1069 - val_accuracy: 0.8574 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.9545 - accuracy: 0.8783 - val_loss: 0.9929 - val_accuracy: 0.8422 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.7958 - accuracy: 0.8843 - val_loss: 0.7891 - val_accuracy: 0.8652 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.6714 - accuracy: 0.8898 - val_loss: 0.7318 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.5908 - accuracy: 0.8952 - val_loss: 0.5830 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.5376 - accuracy: 0.8999 - val_loss: 0.5670 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.5076 - accuracy: 0.8973 - val_loss: 0.4756 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4835 - accuracy: 0.9002 - val_loss: 0.4766 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4627 - accuracy: 0.9009 - val_loss: 0.4419 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4451 - accuracy: 0.9049 - val_loss: 0.4252 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4375 - accuracy: 0.9060 - val_loss: 0.4442 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4260 - accuracy: 0.9048 - val_loss: 0.4208 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4175 - accuracy: 0.9042 - val_loss: 0.4324 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4171 - accuracy: 0.9048 - val_loss: 0.4976 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4105 - accuracy: 0.9042 - val_loss: 0.4110 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4027 - accuracy: 0.9073 - val_loss: 0.5521 - val_accuracy: 0.8598 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4084 - accuracy: 0.9046 - val_loss: 0.3864 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4003 - accuracy: 0.9085 - val_loss: 0.5164 - val_accuracy: 0.8672 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3973 - accuracy: 0.9095 - val_loss: 0.4186 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3915 - accuracy: 0.9110 - val_loss: 0.3933 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3935 - accuracy: 0.9098 - val_loss: 0.3989 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3919 - accuracy: 0.9078 - val_loss: 0.4141 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3916 - accuracy: 0.9079 - val_loss: 0.4911 - val_accuracy: 0.8538 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3865 - accuracy: 0.9084 - val_loss: 0.3981 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3880 - accuracy: 0.9104 - val_loss: 0.3727 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3877 - accuracy: 0.9089 - val_loss: 0.4182 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3821 - accuracy: 0.9136 - val_loss: 0.4089 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3829 - accuracy: 0.9113 - val_loss: 0.3726 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3938 - accuracy: 0.9083 - val_loss: 0.3806 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3858 - accuracy: 0.9095 - val_loss: 0.3863 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3894 - accuracy: 0.9081 - val_loss: 0.3895 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3844 - accuracy: 0.9109 - val_loss: 0.3704 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3840 - accuracy: 0.9095 - val_loss: 0.3781 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3887 - accuracy: 0.9104 - val_loss: 0.5037 - val_accuracy: 0.8634 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3900 - accuracy: 0.9094 - val_loss: 0.3764 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3781 - accuracy: 0.9126 - val_loss: 0.3711 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3841 - accuracy: 0.9079 - val_loss: 0.4123 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3904 - accuracy: 0.9085 - val_loss: 0.3852 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3829 - accuracy: 0.9109 - val_loss: 0.3977 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3936 - accuracy: 0.9091 - val_loss: 0.5551 - val_accuracy: 0.8486 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3907 - accuracy: 0.9086 - val_loss: 0.4560 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3751 - accuracy: 0.9149 - val_loss: 0.3612 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3868 - accuracy: 0.9105 - val_loss: 0.3719 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3842 - accuracy: 0.9109 - val_loss: 0.3966 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3887 - accuracy: 0.9099 - val_loss: 0.3741 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3818 - accuracy: 0.9101 - val_loss: 0.4069 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3764 - accuracy: 0.9120 - val_loss: 0.3795 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3821 - accuracy: 0.9098 - val_loss: 0.3565 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3761 - accuracy: 0.9136 - val_loss: 0.3710 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3820 - accuracy: 0.9091 - val_loss: 0.3636 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3821 - accuracy: 0.9088 - val_loss: 0.3814 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3727 - accuracy: 0.9145 - val_loss: 0.3708 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3820 - accuracy: 0.9108 - val_loss: 0.4428 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3831 - accuracy: 0.9120 - val_loss: 0.3636 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3818 - accuracy: 0.9108 - val_loss: 0.4095 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3782 - accuracy: 0.9143 - val_loss: 0.4172 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3899 - accuracy: 0.9112 - val_loss: 0.4671 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3846 - accuracy: 0.9158 - val_loss: 0.5087 - val_accuracy: 0.8640 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3841 - accuracy: 0.9131 - val_loss: 0.3666 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3813 - accuracy: 0.9160 - val_loss: 0.4361 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3695 - accuracy: 0.9149 - val_loss: 0.3589 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3770 - accuracy: 0.9112 - val_loss: 0.3570 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3725 - accuracy: 0.9159 - val_loss: 0.3702 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3767 - accuracy: 0.9112 - val_loss: 0.4512 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3744 - accuracy: 0.9143 - val_loss: 0.5352 - val_accuracy: 0.8498 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3728 - accuracy: 0.9144 - val_loss: 0.3529 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3782 - accuracy: 0.9137 - val_loss: 0.3672 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3493 - accuracy: 0.9220 - val_loss: 0.3549 - val_accuracy: 0.9182 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3349 - accuracy: 0.9256 - val_loss: 0.3520 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3221 - accuracy: 0.9231 - val_loss: 0.3431 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3115 - accuracy: 0.9286 - val_loss: 0.3258 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3044 - accuracy: 0.9267 - val_loss: 0.2969 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2941 - accuracy: 0.9303 - val_loss: 0.2947 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2891 - accuracy: 0.9280 - val_loss: 0.3112 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2815 - accuracy: 0.9300 - val_loss: 0.3056 - val_accuracy: 0.9188 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2757 - accuracy: 0.9291 - val_loss: 0.2867 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2750 - accuracy: 0.9287 - val_loss: 0.2736 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2667 - accuracy: 0.9320 - val_loss: 0.2727 - val_accuracy: 0.9280 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2610 - accuracy: 0.9332 - val_loss: 0.2710 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2655 - accuracy: 0.9315 - val_loss: 0.2700 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2590 - accuracy: 0.9317 - val_loss: 0.2693 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2632 - accuracy: 0.9318 - val_loss: 0.2694 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2602 - accuracy: 0.9321 - val_loss: 0.2681 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2575 - accuracy: 0.9338 - val_loss: 0.2686 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2543 - accuracy: 0.9338 - val_loss: 0.2669 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2554 - accuracy: 0.9340 - val_loss: 0.2660 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2618 - accuracy: 0.9309 - val_loss: 0.2669 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2576 - accuracy: 0.9338 - val_loss: 0.2659 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2551 - accuracy: 0.9346 - val_loss: 0.2655 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2579 - accuracy: 0.9344 - val_loss: 0.2654 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2555 - accuracy: 0.9319 - val_loss: 0.2653 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2559 - accuracy: 0.9342 - val_loss: 0.2651 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2541 - accuracy: 0.9357 - val_loss: 0.2650 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2579 - accuracy: 0.9319 - val_loss: 0.2650 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2598 - accuracy: 0.9318 - val_loss: 0.2651 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2589 - accuracy: 0.9319 - val_loss: 0.2651 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2828 - accuracy: 0.9269\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5827, TN:9881, FP:574, FN:664, loss0.282805860042572, acc0.9269444116605688, sn0.8977045139423817, sp0.9450980392156862, f10.9039714551659944, auc0.9745275086034955\n",
      "Average Test loss:  0.2844396770000458\n",
      "Average Accuracy:  0.9258704118966128\n",
      "Average Sensitivity:  0.9002773070405174\n",
      "Average Specificity:  0.9417599234815878\n",
      "Average F1 Score:  0.9029478903311\n",
      "Average AUC Score:  0.9742765449213753\n",
      "AUC for ROC curve 1: 0.9741\n",
      "AUC for ROC curve 2: 0.9741\n",
      "AUC for ROC curve 3: 0.9740\n",
      "AUC for ROC curve 4: 0.9740\n",
      "AUC for ROC curve 5: 0.9739\n",
      "AUC for ROC curve 6: 0.9739\n",
      "AUC for ROC curve 7: 0.9745\n",
      "AUC for ROC curve 8: 0.9745\n",
      "AUC for ROC curve 9: 0.9748\n",
      "AUC for ROC curve 10: 0.9748\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f9963-cfa7-47f5-8701-2a24e58e20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
