{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 18:16:29.279177: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "def create_model(input_a_shape,input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "\n",
    "    x1 = BatchNormalization()(input1)    \n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    \n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "#   CNN-3\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "\n",
    "    x3 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    attention1 = ResidualAttentionBlock(256, 256)\n",
    "    attention2 = ResidualAttentionBlock(64, 64)\n",
    "    x1 = attention1(x1)\n",
    "    x3 = attention2(x3)\n",
    "    \n",
    "    concat = keras.layers.concatenate([x1, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1,input3], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  # Convert to two categories\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 26s 53ms/step - loss: 3.3787 - accuracy: 0.6450 - val_loss: 3.5331 - val_accuracy: 0.4071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 2.4910 - accuracy: 0.8061 - val_loss: 2.3061 - val_accuracy: 0.7094 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.8328 - accuracy: 0.8417 - val_loss: 1.5687 - val_accuracy: 0.8456 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.3679 - accuracy: 0.8561 - val_loss: 1.2746 - val_accuracy: 0.8133 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.0378 - accuracy: 0.8686 - val_loss: 1.0576 - val_accuracy: 0.8223 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.8227 - accuracy: 0.8754 - val_loss: 0.8175 - val_accuracy: 0.8520 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6864 - accuracy: 0.8829 - val_loss: 1.0319 - val_accuracy: 0.7750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5915 - accuracy: 0.8926 - val_loss: 0.6494 - val_accuracy: 0.8568 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5356 - accuracy: 0.8975 - val_loss: 0.4816 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4998 - accuracy: 0.8963 - val_loss: 0.6879 - val_accuracy: 0.8470 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4763 - accuracy: 0.8950 - val_loss: 0.4630 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4522 - accuracy: 0.9029 - val_loss: 0.4657 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4364 - accuracy: 0.9021 - val_loss: 0.4399 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4319 - accuracy: 0.9052 - val_loss: 0.4263 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4206 - accuracy: 0.9058 - val_loss: 0.4087 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4129 - accuracy: 0.9071 - val_loss: 0.3682 - val_accuracy: 0.9232 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4119 - accuracy: 0.9067 - val_loss: 0.4263 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4146 - accuracy: 0.9015 - val_loss: 0.3879 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4060 - accuracy: 0.9056 - val_loss: 0.3820 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3998 - accuracy: 0.9061 - val_loss: 0.3708 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3978 - accuracy: 0.9067 - val_loss: 0.5096 - val_accuracy: 0.8648 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3909 - accuracy: 0.9065 - val_loss: 0.3725 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3902 - accuracy: 0.9081 - val_loss: 0.4387 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3944 - accuracy: 0.9079 - val_loss: 0.3978 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3917 - accuracy: 0.9084 - val_loss: 0.3968 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3951 - accuracy: 0.9054 - val_loss: 0.3881 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3878 - accuracy: 0.9058 - val_loss: 0.3558 - val_accuracy: 0.9232 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3869 - accuracy: 0.9093 - val_loss: 0.6288 - val_accuracy: 0.8161 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3867 - accuracy: 0.9079 - val_loss: 0.3573 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3835 - accuracy: 0.9089 - val_loss: 0.3545 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3836 - accuracy: 0.9082 - val_loss: 0.3790 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3804 - accuracy: 0.9111 - val_loss: 0.3832 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3887 - accuracy: 0.9091 - val_loss: 0.3547 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3747 - accuracy: 0.9097 - val_loss: 0.3683 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3716 - accuracy: 0.9147 - val_loss: 0.3558 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3823 - accuracy: 0.9108 - val_loss: 0.4107 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3888 - accuracy: 0.9075 - val_loss: 0.3730 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3699 - accuracy: 0.9117 - val_loss: 0.3956 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3760 - accuracy: 0.9110 - val_loss: 0.3483 - val_accuracy: 0.9248 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3824 - accuracy: 0.9088 - val_loss: 0.3559 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3772 - accuracy: 0.9081 - val_loss: 0.3718 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3733 - accuracy: 0.9097 - val_loss: 0.3754 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3755 - accuracy: 0.9102 - val_loss: 0.3779 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3867 - accuracy: 0.9071 - val_loss: 0.3385 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3703 - accuracy: 0.9128 - val_loss: 0.3399 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3791 - accuracy: 0.9107 - val_loss: 0.3702 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3721 - accuracy: 0.9123 - val_loss: 0.3551 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3696 - accuracy: 0.9103 - val_loss: 0.3533 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3765 - accuracy: 0.9104 - val_loss: 0.5504 - val_accuracy: 0.8313 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3669 - accuracy: 0.9122 - val_loss: 0.3302 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3646 - accuracy: 0.9121 - val_loss: 0.5421 - val_accuracy: 0.8564 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3680 - accuracy: 0.9130 - val_loss: 0.3689 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3734 - accuracy: 0.9094 - val_loss: 0.4670 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3742 - accuracy: 0.9104 - val_loss: 0.3439 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3628 - accuracy: 0.9142 - val_loss: 0.3537 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3649 - accuracy: 0.9120 - val_loss: 0.4010 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3685 - accuracy: 0.9114 - val_loss: 0.3380 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3710 - accuracy: 0.9100 - val_loss: 0.5072 - val_accuracy: 0.8508 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3759 - accuracy: 0.9122 - val_loss: 0.3426 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3635 - accuracy: 0.9120 - val_loss: 0.3753 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3673 - accuracy: 0.9132 - val_loss: 0.3648 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3683 - accuracy: 0.9132 - val_loss: 0.3472 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3672 - accuracy: 0.9120 - val_loss: 0.3671 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3682 - accuracy: 0.9115 - val_loss: 0.3853 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3655 - accuracy: 0.9133 - val_loss: 0.3385 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3667 - accuracy: 0.9108 - val_loss: 0.3627 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3665 - accuracy: 0.9119 - val_loss: 0.3644 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3649 - accuracy: 0.9111 - val_loss: 0.3440 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3602 - accuracy: 0.9151 - val_loss: 0.3525 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3669 - accuracy: 0.9117 - val_loss: 0.3386 - val_accuracy: 0.9246 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3688 - accuracy: 0.9119 - val_loss: 0.3428 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3333 - accuracy: 0.9211 - val_loss: 0.3158 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3186 - accuracy: 0.9262 - val_loss: 0.3256 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3080 - accuracy: 0.9250 - val_loss: 0.3112 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2966 - accuracy: 0.9273 - val_loss: 0.2929 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2933 - accuracy: 0.9276 - val_loss: 0.2944 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2828 - accuracy: 0.9293 - val_loss: 0.2789 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2787 - accuracy: 0.9302 - val_loss: 0.2767 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2784 - accuracy: 0.9271 - val_loss: 0.2673 - val_accuracy: 0.9306 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2698 - accuracy: 0.9319 - val_loss: 0.2585 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2635 - accuracy: 0.9311 - val_loss: 0.2631 - val_accuracy: 0.9288 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2579 - accuracy: 0.9319 - val_loss: 0.2579 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2610 - accuracy: 0.9320 - val_loss: 0.2562 - val_accuracy: 0.9326 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2561 - accuracy: 0.9324 - val_loss: 0.2558 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2570 - accuracy: 0.9328 - val_loss: 0.2553 - val_accuracy: 0.9332 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2552 - accuracy: 0.9318 - val_loss: 0.2557 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2541 - accuracy: 0.9316 - val_loss: 0.2547 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2519 - accuracy: 0.9333 - val_loss: 0.2543 - val_accuracy: 0.9336 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2578 - accuracy: 0.9306 - val_loss: 0.2536 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2500 - accuracy: 0.9348 - val_loss: 0.2529 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2525 - accuracy: 0.9332 - val_loss: 0.2534 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2486 - accuracy: 0.9325 - val_loss: 0.2532 - val_accuracy: 0.9328 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2502 - accuracy: 0.9338 - val_loss: 0.2529 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2524 - accuracy: 0.9350 - val_loss: 0.2528 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2516 - accuracy: 0.9327 - val_loss: 0.2529 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2521 - accuracy: 0.9348 - val_loss: 0.2530 - val_accuracy: 0.9328 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2504 - accuracy: 0.9338 - val_loss: 0.2528 - val_accuracy: 0.9330 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2490 - accuracy: 0.9337 - val_loss: 0.2527 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2537 - accuracy: 0.9332 - val_loss: 0.2528 - val_accuracy: 0.9330 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2517 - accuracy: 0.9323 - val_loss: 0.2528 - val_accuracy: 0.9328 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2789 - accuracy: 0.9258\n",
      "17/17 [==============================] - 2s 27ms/step\n",
      "TP:5824, TN:9866, FP:589, FN:667, loss0.2788645029067993, acc0.925882214091821, sn0.897242335541519, sp0.943663318986131, f10.9026658400495969, auc0.9746835573605538\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 20s 43ms/step - loss: 3.3633 - accuracy: 0.6541 - val_loss: 3.8753 - val_accuracy: 0.3923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 2.4736 - accuracy: 0.8079 - val_loss: 2.1255 - val_accuracy: 0.8303 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.8258 - accuracy: 0.8407 - val_loss: 1.5503 - val_accuracy: 0.8528 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.3501 - accuracy: 0.8586 - val_loss: 1.1531 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.0304 - accuracy: 0.8687 - val_loss: 0.8818 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.8238 - accuracy: 0.8766 - val_loss: 0.8328 - val_accuracy: 0.8301 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6924 - accuracy: 0.8811 - val_loss: 0.8734 - val_accuracy: 0.7984 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5907 - accuracy: 0.8876 - val_loss: 0.5826 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5368 - accuracy: 0.8898 - val_loss: 0.4774 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4901 - accuracy: 0.8962 - val_loss: 0.6449 - val_accuracy: 0.8466 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4728 - accuracy: 0.8969 - val_loss: 0.5897 - val_accuracy: 0.8456 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4458 - accuracy: 0.8998 - val_loss: 0.4227 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4387 - accuracy: 0.9015 - val_loss: 0.4050 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4384 - accuracy: 0.8997 - val_loss: 0.4031 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4257 - accuracy: 0.9016 - val_loss: 0.4033 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4121 - accuracy: 0.9026 - val_loss: 0.3668 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4095 - accuracy: 0.9039 - val_loss: 0.3741 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4043 - accuracy: 0.9063 - val_loss: 0.3753 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4075 - accuracy: 0.9023 - val_loss: 0.3698 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3950 - accuracy: 0.9086 - val_loss: 0.3756 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3928 - accuracy: 0.9071 - val_loss: 0.3683 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3960 - accuracy: 0.9063 - val_loss: 0.4748 - val_accuracy: 0.8684 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3981 - accuracy: 0.9049 - val_loss: 0.3542 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3905 - accuracy: 0.9075 - val_loss: 0.3682 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3869 - accuracy: 0.9114 - val_loss: 0.4515 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3920 - accuracy: 0.9084 - val_loss: 0.3530 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3850 - accuracy: 0.9087 - val_loss: 0.3758 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3809 - accuracy: 0.9108 - val_loss: 0.3641 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3897 - accuracy: 0.9077 - val_loss: 0.3946 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3817 - accuracy: 0.9135 - val_loss: 0.3524 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3861 - accuracy: 0.9097 - val_loss: 0.3600 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3925 - accuracy: 0.9080 - val_loss: 0.3611 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3863 - accuracy: 0.9089 - val_loss: 0.3803 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3830 - accuracy: 0.9095 - val_loss: 0.3636 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3835 - accuracy: 0.9082 - val_loss: 0.3700 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3910 - accuracy: 0.9104 - val_loss: 0.3528 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3767 - accuracy: 0.9125 - val_loss: 0.3630 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3821 - accuracy: 0.9107 - val_loss: 0.3503 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3764 - accuracy: 0.9114 - val_loss: 0.3454 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3851 - accuracy: 0.9089 - val_loss: 0.3733 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3700 - accuracy: 0.9132 - val_loss: 0.4000 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3786 - accuracy: 0.9099 - val_loss: 0.3476 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3786 - accuracy: 0.9112 - val_loss: 0.3928 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3770 - accuracy: 0.9096 - val_loss: 0.3544 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3733 - accuracy: 0.9132 - val_loss: 0.3483 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3729 - accuracy: 0.9138 - val_loss: 0.4327 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3690 - accuracy: 0.9125 - val_loss: 0.4211 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3724 - accuracy: 0.9152 - val_loss: 0.3753 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3823 - accuracy: 0.9069 - val_loss: 0.3943 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3654 - accuracy: 0.9149 - val_loss: 0.3408 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3696 - accuracy: 0.9109 - val_loss: 0.3561 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3715 - accuracy: 0.9130 - val_loss: 0.3479 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3716 - accuracy: 0.9101 - val_loss: 0.3940 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3636 - accuracy: 0.9122 - val_loss: 0.3343 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3673 - accuracy: 0.9104 - val_loss: 0.3766 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3641 - accuracy: 0.9131 - val_loss: 0.5817 - val_accuracy: 0.8209 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3737 - accuracy: 0.9095 - val_loss: 0.3336 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3660 - accuracy: 0.9138 - val_loss: 0.3534 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3676 - accuracy: 0.9136 - val_loss: 0.3434 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3729 - accuracy: 0.9082 - val_loss: 0.3746 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3654 - accuracy: 0.9097 - val_loss: 0.3391 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3640 - accuracy: 0.9152 - val_loss: 0.3474 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3639 - accuracy: 0.9146 - val_loss: 0.4232 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3716 - accuracy: 0.9115 - val_loss: 0.3768 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3761 - accuracy: 0.9127 - val_loss: 0.4128 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3706 - accuracy: 0.9119 - val_loss: 0.4327 - val_accuracy: 0.8805 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3647 - accuracy: 0.9150 - val_loss: 0.4378 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3725 - accuracy: 0.9144 - val_loss: 0.3554 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3627 - accuracy: 0.9179 - val_loss: 0.3483 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3681 - accuracy: 0.9152 - val_loss: 0.3790 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3695 - accuracy: 0.9134 - val_loss: 0.3447 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3371 - accuracy: 0.9217 - val_loss: 0.3142 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3226 - accuracy: 0.9244 - val_loss: 0.3146 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3117 - accuracy: 0.9244 - val_loss: 0.3205 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3016 - accuracy: 0.9264 - val_loss: 0.2919 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2921 - accuracy: 0.9284 - val_loss: 0.2948 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2838 - accuracy: 0.9305 - val_loss: 0.2863 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2812 - accuracy: 0.9307 - val_loss: 0.2773 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2755 - accuracy: 0.9283 - val_loss: 0.2739 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2713 - accuracy: 0.9324 - val_loss: 0.2711 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2645 - accuracy: 0.9311 - val_loss: 0.2631 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2575 - accuracy: 0.9347 - val_loss: 0.2632 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2558 - accuracy: 0.9340 - val_loss: 0.2609 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2551 - accuracy: 0.9356 - val_loss: 0.2615 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2579 - accuracy: 0.9333 - val_loss: 0.2595 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2538 - accuracy: 0.9346 - val_loss: 0.2601 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2505 - accuracy: 0.9363 - val_loss: 0.2601 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2570 - accuracy: 0.9317 - val_loss: 0.2599 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2543 - accuracy: 0.9323 - val_loss: 0.2598 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2511 - accuracy: 0.9340 - val_loss: 0.2590 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2533 - accuracy: 0.9358 - val_loss: 0.2577 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2508 - accuracy: 0.9334 - val_loss: 0.2587 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2517 - accuracy: 0.9356 - val_loss: 0.2593 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2532 - accuracy: 0.9331 - val_loss: 0.2591 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2509 - accuracy: 0.9338 - val_loss: 0.2593 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2514 - accuracy: 0.9354 - val_loss: 0.2595 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2512 - accuracy: 0.9339 - val_loss: 0.2592 - val_accuracy: 0.9328 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2488 - accuracy: 0.9366 - val_loss: 0.2591 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2521 - accuracy: 0.9346 - val_loss: 0.2590 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2489 - accuracy: 0.9348 - val_loss: 0.2591 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2877 - accuracy: 0.9248\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5884, TN:9787, FP:668, FN:607, loss0.28771737217903137, acc0.9247610055470318, sn0.9064859035587737, sp0.9361071257771402, f10.9022464157019091, auc0.9743439192301065\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 21s 47ms/step - loss: 3.3885 - accuracy: 0.6575 - val_loss: 3.6948 - val_accuracy: 0.4031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 2.5371 - accuracy: 0.7984 - val_loss: 2.7319 - val_accuracy: 0.4471 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.8842 - accuracy: 0.8408 - val_loss: 1.6082 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.4128 - accuracy: 0.8558 - val_loss: 1.2627 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.0813 - accuracy: 0.8679 - val_loss: 0.9260 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.8586 - accuracy: 0.8783 - val_loss: 0.8302 - val_accuracy: 0.8488 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.7172 - accuracy: 0.8805 - val_loss: 0.8094 - val_accuracy: 0.8281 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.6216 - accuracy: 0.8879 - val_loss: 0.5438 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5537 - accuracy: 0.8928 - val_loss: 0.5681 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5143 - accuracy: 0.8973 - val_loss: 0.4685 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4908 - accuracy: 0.8980 - val_loss: 0.4707 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4648 - accuracy: 0.8961 - val_loss: 0.4942 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4472 - accuracy: 0.8981 - val_loss: 0.5146 - val_accuracy: 0.8670 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4366 - accuracy: 0.9009 - val_loss: 0.4024 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4311 - accuracy: 0.9013 - val_loss: 0.3931 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4161 - accuracy: 0.9058 - val_loss: 0.4198 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4197 - accuracy: 0.9035 - val_loss: 0.3717 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4022 - accuracy: 0.9099 - val_loss: 0.3714 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4135 - accuracy: 0.9032 - val_loss: 0.3757 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4005 - accuracy: 0.9094 - val_loss: 0.3588 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4084 - accuracy: 0.9044 - val_loss: 0.3937 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3903 - accuracy: 0.9105 - val_loss: 0.3574 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4101 - accuracy: 0.9035 - val_loss: 0.3755 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3873 - accuracy: 0.9090 - val_loss: 0.3976 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3926 - accuracy: 0.9102 - val_loss: 0.4135 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3974 - accuracy: 0.9059 - val_loss: 0.3701 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3905 - accuracy: 0.9072 - val_loss: 0.3941 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3988 - accuracy: 0.9075 - val_loss: 0.3896 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3976 - accuracy: 0.9043 - val_loss: 0.3508 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3855 - accuracy: 0.9072 - val_loss: 0.3502 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3891 - accuracy: 0.9084 - val_loss: 0.3531 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3852 - accuracy: 0.9090 - val_loss: 0.3663 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3907 - accuracy: 0.9073 - val_loss: 0.3595 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3779 - accuracy: 0.9116 - val_loss: 0.3698 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3864 - accuracy: 0.9113 - val_loss: 0.3615 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3979 - accuracy: 0.9060 - val_loss: 0.3997 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3895 - accuracy: 0.9081 - val_loss: 0.3640 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3796 - accuracy: 0.9095 - val_loss: 0.4811 - val_accuracy: 0.8608 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3798 - accuracy: 0.9097 - val_loss: 0.3776 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3931 - accuracy: 0.9079 - val_loss: 0.3457 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3818 - accuracy: 0.9100 - val_loss: 0.3758 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3814 - accuracy: 0.9125 - val_loss: 0.3538 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3791 - accuracy: 0.9107 - val_loss: 0.3585 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3851 - accuracy: 0.9096 - val_loss: 0.3748 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3873 - accuracy: 0.9085 - val_loss: 0.4151 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3746 - accuracy: 0.9108 - val_loss: 0.3494 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3694 - accuracy: 0.9138 - val_loss: 0.3850 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3800 - accuracy: 0.9086 - val_loss: 0.4372 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3715 - accuracy: 0.9137 - val_loss: 0.4068 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3786 - accuracy: 0.9095 - val_loss: 0.3682 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3802 - accuracy: 0.9078 - val_loss: 0.3580 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3757 - accuracy: 0.9131 - val_loss: 0.3564 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3775 - accuracy: 0.9113 - val_loss: 0.3717 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3797 - accuracy: 0.9099 - val_loss: 0.3795 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3845 - accuracy: 0.9110 - val_loss: 0.3725 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3836 - accuracy: 0.9103 - val_loss: 0.3954 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3766 - accuracy: 0.9089 - val_loss: 0.5608 - val_accuracy: 0.8305 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3769 - accuracy: 0.9120 - val_loss: 0.3914 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3747 - accuracy: 0.9108 - val_loss: 0.3733 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3719 - accuracy: 0.9135 - val_loss: 0.3558 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3876 - accuracy: 0.9096 - val_loss: 0.3979 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3742 - accuracy: 0.9099 - val_loss: 0.3731 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3787 - accuracy: 0.9099 - val_loss: 0.3647 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3682 - accuracy: 0.9153 - val_loss: 0.3747 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3726 - accuracy: 0.9128 - val_loss: 0.3398 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3714 - accuracy: 0.9124 - val_loss: 0.4067 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3690 - accuracy: 0.9130 - val_loss: 0.3776 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3712 - accuracy: 0.9109 - val_loss: 0.4067 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3662 - accuracy: 0.9145 - val_loss: 0.3303 - val_accuracy: 0.9260 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3681 - accuracy: 0.9142 - val_loss: 0.3430 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3656 - accuracy: 0.9138 - val_loss: 0.3997 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3435 - accuracy: 0.9191 - val_loss: 0.3306 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3276 - accuracy: 0.9216 - val_loss: 0.3069 - val_accuracy: 0.9288 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3141 - accuracy: 0.9226 - val_loss: 0.3028 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3034 - accuracy: 0.9263 - val_loss: 0.3219 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2991 - accuracy: 0.9242 - val_loss: 0.3007 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2934 - accuracy: 0.9285 - val_loss: 0.2973 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2820 - accuracy: 0.9306 - val_loss: 0.2745 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2819 - accuracy: 0.9277 - val_loss: 0.2794 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2723 - accuracy: 0.9314 - val_loss: 0.2728 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2691 - accuracy: 0.9297 - val_loss: 0.2660 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2581 - accuracy: 0.9339 - val_loss: 0.2582 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2617 - accuracy: 0.9315 - val_loss: 0.2591 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2576 - accuracy: 0.9322 - val_loss: 0.2583 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2581 - accuracy: 0.9338 - val_loss: 0.2586 - val_accuracy: 0.9326 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2573 - accuracy: 0.9332 - val_loss: 0.2579 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2554 - accuracy: 0.9331 - val_loss: 0.2571 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2542 - accuracy: 0.9337 - val_loss: 0.2583 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2580 - accuracy: 0.9329 - val_loss: 0.2567 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2545 - accuracy: 0.9327 - val_loss: 0.2561 - val_accuracy: 0.9326 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2554 - accuracy: 0.9326 - val_loss: 0.2572 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2569 - accuracy: 0.9347 - val_loss: 0.2575 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2523 - accuracy: 0.9344 - val_loss: 0.2576 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2516 - accuracy: 0.9345 - val_loss: 0.2576 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2524 - accuracy: 0.9351 - val_loss: 0.2576 - val_accuracy: 0.9330 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2503 - accuracy: 0.9349 - val_loss: 0.2575 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2536 - accuracy: 0.9339 - val_loss: 0.2577 - val_accuracy: 0.9330 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2511 - accuracy: 0.9361 - val_loss: 0.2576 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2540 - accuracy: 0.9328 - val_loss: 0.2576 - val_accuracy: 0.9334 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2568 - accuracy: 0.9353 - val_loss: 0.2574 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2880 - accuracy: 0.9249\n",
      "17/17 [==============================] - 1s 18ms/step\n",
      "TP:5907, TN:9767, FP:688, FN:584, loss0.2880498468875885, acc0.9249380384751564, sn0.9100292712987214, sp0.9341941654710665, f10.9027968821641449, auc0.9744229456214287\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 44ms/step - loss: 3.3793 - accuracy: 0.6573 - val_loss: 3.6982 - val_accuracy: 0.3991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.4851 - accuracy: 0.8037 - val_loss: 2.3397 - val_accuracy: 0.6452 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 1.8333 - accuracy: 0.8364 - val_loss: 1.6884 - val_accuracy: 0.7752 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.3677 - accuracy: 0.8526 - val_loss: 1.1691 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 1.0487 - accuracy: 0.8588 - val_loss: 0.8870 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.8249 - accuracy: 0.8753 - val_loss: 0.7145 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.7052 - accuracy: 0.8771 - val_loss: 0.6317 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5975 - accuracy: 0.8873 - val_loss: 0.6732 - val_accuracy: 0.8389 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5388 - accuracy: 0.8917 - val_loss: 0.5268 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5064 - accuracy: 0.8935 - val_loss: 0.6006 - val_accuracy: 0.8680 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4816 - accuracy: 0.8955 - val_loss: 0.4781 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4588 - accuracy: 0.8962 - val_loss: 0.4080 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4361 - accuracy: 0.9004 - val_loss: 0.4125 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4311 - accuracy: 0.9002 - val_loss: 0.4105 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4311 - accuracy: 0.8998 - val_loss: 0.3874 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4104 - accuracy: 0.9062 - val_loss: 0.4801 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4100 - accuracy: 0.9058 - val_loss: 0.5734 - val_accuracy: 0.8406 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4135 - accuracy: 0.8995 - val_loss: 0.4324 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4006 - accuracy: 0.9036 - val_loss: 0.4604 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3987 - accuracy: 0.9051 - val_loss: 0.4294 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3961 - accuracy: 0.9056 - val_loss: 0.3635 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3929 - accuracy: 0.9070 - val_loss: 0.3594 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3887 - accuracy: 0.9087 - val_loss: 0.3699 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3922 - accuracy: 0.9078 - val_loss: 0.3969 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3888 - accuracy: 0.9064 - val_loss: 0.3670 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3905 - accuracy: 0.9062 - val_loss: 0.4406 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3823 - accuracy: 0.9092 - val_loss: 0.3832 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3865 - accuracy: 0.9097 - val_loss: 0.4005 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3785 - accuracy: 0.9072 - val_loss: 0.4224 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3808 - accuracy: 0.9086 - val_loss: 0.3540 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3796 - accuracy: 0.9102 - val_loss: 0.3827 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3847 - accuracy: 0.9090 - val_loss: 0.4032 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3849 - accuracy: 0.9038 - val_loss: 0.3887 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3786 - accuracy: 0.9091 - val_loss: 0.3751 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3812 - accuracy: 0.9101 - val_loss: 0.3466 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3798 - accuracy: 0.9113 - val_loss: 0.4076 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3810 - accuracy: 0.9106 - val_loss: 0.4329 - val_accuracy: 0.8829 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3797 - accuracy: 0.9097 - val_loss: 0.3544 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3745 - accuracy: 0.9098 - val_loss: 0.4330 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3770 - accuracy: 0.9123 - val_loss: 0.4695 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3762 - accuracy: 0.9112 - val_loss: 0.3389 - val_accuracy: 0.9244 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3709 - accuracy: 0.9116 - val_loss: 0.3557 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3727 - accuracy: 0.9145 - val_loss: 0.8134 - val_accuracy: 0.7698 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3846 - accuracy: 0.9051 - val_loss: 0.3631 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3735 - accuracy: 0.9091 - val_loss: 0.3465 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3757 - accuracy: 0.9124 - val_loss: 0.3571 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3735 - accuracy: 0.9130 - val_loss: 0.3436 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3687 - accuracy: 0.9179 - val_loss: 0.3637 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3733 - accuracy: 0.9091 - val_loss: 0.3769 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3711 - accuracy: 0.9120 - val_loss: 0.3994 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3704 - accuracy: 0.9113 - val_loss: 0.3608 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3737 - accuracy: 0.9109 - val_loss: 0.3912 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3716 - accuracy: 0.9130 - val_loss: 0.3472 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3895 - accuracy: 0.9076 - val_loss: 0.3676 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3713 - accuracy: 0.9137 - val_loss: 0.3897 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3802 - accuracy: 0.9122 - val_loss: 0.3524 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3732 - accuracy: 0.9120 - val_loss: 0.4055 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3799 - accuracy: 0.9131 - val_loss: 0.3537 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3730 - accuracy: 0.9142 - val_loss: 0.3605 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3710 - accuracy: 0.9138 - val_loss: 0.3687 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3799 - accuracy: 0.9126 - val_loss: 0.4355 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3771 - accuracy: 0.9097 - val_loss: 0.5522 - val_accuracy: 0.8484 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3688 - accuracy: 0.9118 - val_loss: 0.4325 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3658 - accuracy: 0.9138 - val_loss: 0.3409 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3783 - accuracy: 0.9113 - val_loss: 0.3578 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3688 - accuracy: 0.9112 - val_loss: 0.3588 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3632 - accuracy: 0.9137 - val_loss: 0.3754 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3740 - accuracy: 0.9106 - val_loss: 0.3767 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3621 - accuracy: 0.9158 - val_loss: 0.3516 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3763 - accuracy: 0.9124 - val_loss: 0.3454 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3675 - accuracy: 0.9103 - val_loss: 0.3611 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3387 - accuracy: 0.9203 - val_loss: 0.3165 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3218 - accuracy: 0.9239 - val_loss: 0.3186 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3087 - accuracy: 0.9277 - val_loss: 0.2998 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3011 - accuracy: 0.9285 - val_loss: 0.3010 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2905 - accuracy: 0.9295 - val_loss: 0.2876 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2868 - accuracy: 0.9307 - val_loss: 0.2801 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2799 - accuracy: 0.9285 - val_loss: 0.2749 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2733 - accuracy: 0.9293 - val_loss: 0.2678 - val_accuracy: 0.9304 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2730 - accuracy: 0.9285 - val_loss: 0.2690 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2628 - accuracy: 0.9306 - val_loss: 0.2641 - val_accuracy: 0.9296 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2581 - accuracy: 0.9309 - val_loss: 0.2597 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2547 - accuracy: 0.9338 - val_loss: 0.2575 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2549 - accuracy: 0.9361 - val_loss: 0.2578 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2511 - accuracy: 0.9363 - val_loss: 0.2567 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2528 - accuracy: 0.9337 - val_loss: 0.2578 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2528 - accuracy: 0.9338 - val_loss: 0.2564 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2517 - accuracy: 0.9343 - val_loss: 0.2563 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2509 - accuracy: 0.9340 - val_loss: 0.2563 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2492 - accuracy: 0.9349 - val_loss: 0.2567 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2552 - accuracy: 0.9322 - val_loss: 0.2560 - val_accuracy: 0.9326 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2482 - accuracy: 0.9361 - val_loss: 0.2558 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2506 - accuracy: 0.9343 - val_loss: 0.2561 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2528 - accuracy: 0.9331 - val_loss: 0.2562 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2505 - accuracy: 0.9352 - val_loss: 0.2559 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2522 - accuracy: 0.9339 - val_loss: 0.2562 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2489 - accuracy: 0.9339 - val_loss: 0.2562 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2509 - accuracy: 0.9332 - val_loss: 0.2560 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2468 - accuracy: 0.9369 - val_loss: 0.2560 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2473 - accuracy: 0.9348 - val_loss: 0.2560 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2834 - accuracy: 0.9243\n",
      "17/17 [==============================] - 1s 16ms/step\n",
      "TP:5875, TN:9789, FP:666, FN:616, loss0.2834317982196808, acc0.9243479287147409, sn0.9050993683561855, sp0.9362984218077475, f10.9016267648864335, auc0.9747066846999498\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 43ms/step - loss: 3.3619 - accuracy: 0.6607 - val_loss: 4.2301 - val_accuracy: 0.3887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 2.4618 - accuracy: 0.8171 - val_loss: 2.5351 - val_accuracy: 0.5509 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.8182 - accuracy: 0.8420 - val_loss: 1.6462 - val_accuracy: 0.8002 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.3469 - accuracy: 0.8570 - val_loss: 1.2894 - val_accuracy: 0.7924 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.0259 - accuracy: 0.8703 - val_loss: 0.8720 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.8197 - accuracy: 0.8746 - val_loss: 0.7884 - val_accuracy: 0.8578 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.6830 - accuracy: 0.8811 - val_loss: 0.6337 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5933 - accuracy: 0.8873 - val_loss: 0.5757 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5369 - accuracy: 0.8917 - val_loss: 0.5104 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5033 - accuracy: 0.8937 - val_loss: 0.4686 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4759 - accuracy: 0.8974 - val_loss: 0.4327 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4546 - accuracy: 0.9021 - val_loss: 0.4162 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4428 - accuracy: 0.9025 - val_loss: 0.4004 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4357 - accuracy: 0.9014 - val_loss: 0.3898 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.4225 - accuracy: 0.9064 - val_loss: 0.3904 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4226 - accuracy: 0.9044 - val_loss: 0.3953 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4038 - accuracy: 0.9072 - val_loss: 0.4016 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4121 - accuracy: 0.9047 - val_loss: 0.3712 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4048 - accuracy: 0.9051 - val_loss: 0.4292 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4062 - accuracy: 0.9074 - val_loss: 0.3647 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3982 - accuracy: 0.9085 - val_loss: 0.3726 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3989 - accuracy: 0.9067 - val_loss: 0.4283 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3963 - accuracy: 0.9071 - val_loss: 0.3834 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3959 - accuracy: 0.9058 - val_loss: 0.3655 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3920 - accuracy: 0.9076 - val_loss: 0.4002 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3923 - accuracy: 0.9096 - val_loss: 0.4877 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3920 - accuracy: 0.9060 - val_loss: 0.3674 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3843 - accuracy: 0.9121 - val_loss: 0.3504 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3884 - accuracy: 0.9122 - val_loss: 0.3509 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3813 - accuracy: 0.9126 - val_loss: 0.3862 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3789 - accuracy: 0.9140 - val_loss: 0.4472 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3829 - accuracy: 0.9106 - val_loss: 0.3715 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3893 - accuracy: 0.9066 - val_loss: 0.3709 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3767 - accuracy: 0.9126 - val_loss: 0.3705 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3864 - accuracy: 0.9069 - val_loss: 0.3435 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3770 - accuracy: 0.9095 - val_loss: 0.3595 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3753 - accuracy: 0.9121 - val_loss: 0.3399 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3785 - accuracy: 0.9116 - val_loss: 0.3633 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3792 - accuracy: 0.9110 - val_loss: 0.3737 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3806 - accuracy: 0.9071 - val_loss: 0.5260 - val_accuracy: 0.8263 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3684 - accuracy: 0.9102 - val_loss: 0.4112 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3763 - accuracy: 0.9081 - val_loss: 0.3448 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3726 - accuracy: 0.9095 - val_loss: 0.3527 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3753 - accuracy: 0.9122 - val_loss: 0.4258 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3689 - accuracy: 0.9114 - val_loss: 0.3860 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3717 - accuracy: 0.9110 - val_loss: 0.3572 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3657 - accuracy: 0.9151 - val_loss: 0.4373 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3782 - accuracy: 0.9092 - val_loss: 0.3896 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3688 - accuracy: 0.9135 - val_loss: 0.4089 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3692 - accuracy: 0.9121 - val_loss: 0.3338 - val_accuracy: 0.9258 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3680 - accuracy: 0.9151 - val_loss: 0.3326 - val_accuracy: 0.9270 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3713 - accuracy: 0.9106 - val_loss: 0.3679 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3647 - accuracy: 0.9128 - val_loss: 0.3409 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3620 - accuracy: 0.9122 - val_loss: 0.3697 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3667 - accuracy: 0.9114 - val_loss: 0.3846 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3681 - accuracy: 0.9109 - val_loss: 0.3565 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3695 - accuracy: 0.9109 - val_loss: 0.3696 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3623 - accuracy: 0.9156 - val_loss: 0.4303 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3733 - accuracy: 0.9142 - val_loss: 0.3594 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3686 - accuracy: 0.9126 - val_loss: 0.3587 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3618 - accuracy: 0.9143 - val_loss: 0.3760 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3732 - accuracy: 0.9094 - val_loss: 0.3511 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3738 - accuracy: 0.9089 - val_loss: 0.3432 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3628 - accuracy: 0.9160 - val_loss: 0.3439 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3680 - accuracy: 0.9141 - val_loss: 0.3257 - val_accuracy: 0.9254 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3609 - accuracy: 0.9118 - val_loss: 0.3404 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3621 - accuracy: 0.9113 - val_loss: 0.4347 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3630 - accuracy: 0.9149 - val_loss: 0.3801 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3650 - accuracy: 0.9144 - val_loss: 0.3606 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3631 - accuracy: 0.9134 - val_loss: 0.3586 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3645 - accuracy: 0.9120 - val_loss: 0.3408 - val_accuracy: 0.9232 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3341 - accuracy: 0.9232 - val_loss: 0.3204 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3208 - accuracy: 0.9243 - val_loss: 0.3143 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3102 - accuracy: 0.9264 - val_loss: 0.3228 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2935 - accuracy: 0.9270 - val_loss: 0.2987 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2897 - accuracy: 0.9293 - val_loss: 0.2844 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2839 - accuracy: 0.9284 - val_loss: 0.2809 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2779 - accuracy: 0.9297 - val_loss: 0.2778 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2726 - accuracy: 0.9301 - val_loss: 0.2791 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2697 - accuracy: 0.9308 - val_loss: 0.2778 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2691 - accuracy: 0.9289 - val_loss: 0.2609 - val_accuracy: 0.9310 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2552 - accuracy: 0.9351 - val_loss: 0.2584 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2591 - accuracy: 0.9319 - val_loss: 0.2589 - val_accuracy: 0.9354 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2503 - accuracy: 0.9353 - val_loss: 0.2588 - val_accuracy: 0.9352 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2561 - accuracy: 0.9321 - val_loss: 0.2589 - val_accuracy: 0.9332 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2503 - accuracy: 0.9353 - val_loss: 0.2586 - val_accuracy: 0.9346 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2578 - accuracy: 0.9314 - val_loss: 0.2577 - val_accuracy: 0.9354 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2517 - accuracy: 0.9336 - val_loss: 0.2579 - val_accuracy: 0.9358 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2510 - accuracy: 0.9347 - val_loss: 0.2573 - val_accuracy: 0.9344 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2504 - accuracy: 0.9365 - val_loss: 0.2562 - val_accuracy: 0.9344 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2481 - accuracy: 0.9359 - val_loss: 0.2556 - val_accuracy: 0.9352 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2478 - accuracy: 0.9356 - val_loss: 0.2562 - val_accuracy: 0.9348 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2490 - accuracy: 0.9360 - val_loss: 0.2566 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2496 - accuracy: 0.9357 - val_loss: 0.2570 - val_accuracy: 0.9348 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2453 - accuracy: 0.9361 - val_loss: 0.2566 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2492 - accuracy: 0.9332 - val_loss: 0.2569 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2509 - accuracy: 0.9340 - val_loss: 0.2568 - val_accuracy: 0.9352 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2520 - accuracy: 0.9333 - val_loss: 0.2567 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2454 - accuracy: 0.9361 - val_loss: 0.2568 - val_accuracy: 0.9346 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2488 - accuracy: 0.9352 - val_loss: 0.2568 - val_accuracy: 0.9344 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2874 - accuracy: 0.9255\n",
      "17/17 [==============================] - 1s 17ms/step\n",
      "TP:5882, TN:9802, FP:653, FN:609, loss0.28738659620285034, acc0.9255281482355718, sn0.9061777846248652, sp0.9375418460066953, f10.9031168432366038, auc0.9748052503407397\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 50ms/step - loss: 3.3889 - accuracy: 0.6572 - val_loss: 3.6083 - val_accuracy: 0.4797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.5395 - accuracy: 0.8057 - val_loss: 2.5092 - val_accuracy: 0.5846 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.8911 - accuracy: 0.8417 - val_loss: 1.6703 - val_accuracy: 0.8337 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.4071 - accuracy: 0.8591 - val_loss: 1.2318 - val_accuracy: 0.8476 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.0782 - accuracy: 0.8672 - val_loss: 0.9803 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.8470 - accuracy: 0.8760 - val_loss: 0.8001 - val_accuracy: 0.8604 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.7091 - accuracy: 0.8796 - val_loss: 0.6270 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.6072 - accuracy: 0.8872 - val_loss: 0.5838 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5450 - accuracy: 0.8962 - val_loss: 0.4826 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.5004 - accuracy: 0.8982 - val_loss: 0.4808 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4836 - accuracy: 0.9006 - val_loss: 0.4528 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4548 - accuracy: 0.9014 - val_loss: 0.4994 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4486 - accuracy: 0.9017 - val_loss: 0.3931 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4250 - accuracy: 0.9042 - val_loss: 0.4445 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4228 - accuracy: 0.9049 - val_loss: 0.3787 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4185 - accuracy: 0.9079 - val_loss: 0.4392 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4149 - accuracy: 0.9053 - val_loss: 0.4939 - val_accuracy: 0.8744 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4066 - accuracy: 0.9085 - val_loss: 0.4527 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4103 - accuracy: 0.9052 - val_loss: 0.3798 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4046 - accuracy: 0.9057 - val_loss: 0.3845 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3931 - accuracy: 0.9085 - val_loss: 0.5455 - val_accuracy: 0.8307 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3886 - accuracy: 0.9106 - val_loss: 0.4225 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4022 - accuracy: 0.9067 - val_loss: 0.3905 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3870 - accuracy: 0.9097 - val_loss: 0.3673 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3931 - accuracy: 0.9099 - val_loss: 0.3844 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3924 - accuracy: 0.9073 - val_loss: 0.3680 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3851 - accuracy: 0.9108 - val_loss: 0.3869 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3869 - accuracy: 0.9079 - val_loss: 0.4784 - val_accuracy: 0.8837 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3887 - accuracy: 0.9069 - val_loss: 0.5679 - val_accuracy: 0.8165 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3901 - accuracy: 0.9064 - val_loss: 0.4436 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3813 - accuracy: 0.9109 - val_loss: 0.4776 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3864 - accuracy: 0.9100 - val_loss: 0.3589 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3830 - accuracy: 0.9122 - val_loss: 0.3576 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3850 - accuracy: 0.9091 - val_loss: 0.3662 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3862 - accuracy: 0.9096 - val_loss: 0.3845 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3773 - accuracy: 0.9115 - val_loss: 0.3388 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3791 - accuracy: 0.9098 - val_loss: 0.5059 - val_accuracy: 0.8588 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3857 - accuracy: 0.9093 - val_loss: 0.3445 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3829 - accuracy: 0.9085 - val_loss: 0.4095 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3773 - accuracy: 0.9103 - val_loss: 0.3683 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3719 - accuracy: 0.9101 - val_loss: 0.3487 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3846 - accuracy: 0.9099 - val_loss: 0.3698 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3727 - accuracy: 0.9123 - val_loss: 0.3969 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3722 - accuracy: 0.9132 - val_loss: 0.3767 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3799 - accuracy: 0.9120 - val_loss: 0.4359 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3701 - accuracy: 0.9148 - val_loss: 0.3437 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3751 - accuracy: 0.9102 - val_loss: 0.4002 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3670 - accuracy: 0.9130 - val_loss: 0.4070 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3802 - accuracy: 0.9090 - val_loss: 0.3615 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3681 - accuracy: 0.9133 - val_loss: 0.3763 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3789 - accuracy: 0.9099 - val_loss: 0.3663 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3647 - accuracy: 0.9131 - val_loss: 0.3560 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3677 - accuracy: 0.9150 - val_loss: 0.3414 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3697 - accuracy: 0.9122 - val_loss: 0.3413 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3686 - accuracy: 0.9103 - val_loss: 0.3874 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3675 - accuracy: 0.9151 - val_loss: 0.3991 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3744 - accuracy: 0.9138 - val_loss: 0.4463 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3676 - accuracy: 0.9142 - val_loss: 0.3360 - val_accuracy: 0.9234 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3722 - accuracy: 0.9106 - val_loss: 0.3758 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3701 - accuracy: 0.9132 - val_loss: 0.3426 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3678 - accuracy: 0.9132 - val_loss: 0.3530 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3697 - accuracy: 0.9132 - val_loss: 0.3562 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3734 - accuracy: 0.9115 - val_loss: 0.3444 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3597 - accuracy: 0.9157 - val_loss: 0.4545 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3690 - accuracy: 0.9117 - val_loss: 0.3506 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3653 - accuracy: 0.9140 - val_loss: 0.5602 - val_accuracy: 0.8207 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3695 - accuracy: 0.9126 - val_loss: 0.3395 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3645 - accuracy: 0.9142 - val_loss: 0.3822 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3659 - accuracy: 0.9099 - val_loss: 0.3349 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3648 - accuracy: 0.9128 - val_loss: 0.4354 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3594 - accuracy: 0.9133 - val_loss: 0.3721 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3353 - accuracy: 0.9237 - val_loss: 0.3131 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3137 - accuracy: 0.9271 - val_loss: 0.3052 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3126 - accuracy: 0.9251 - val_loss: 0.3004 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2991 - accuracy: 0.9267 - val_loss: 0.2888 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2912 - accuracy: 0.9279 - val_loss: 0.2819 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2823 - accuracy: 0.9301 - val_loss: 0.2874 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2775 - accuracy: 0.9307 - val_loss: 0.2799 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2736 - accuracy: 0.9291 - val_loss: 0.2829 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2684 - accuracy: 0.9311 - val_loss: 0.2645 - val_accuracy: 0.9290 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2639 - accuracy: 0.9317 - val_loss: 0.2848 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2589 - accuracy: 0.9315 - val_loss: 0.2602 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2584 - accuracy: 0.9331 - val_loss: 0.2590 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2542 - accuracy: 0.9338 - val_loss: 0.2611 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2509 - accuracy: 0.9340 - val_loss: 0.2577 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2552 - accuracy: 0.9347 - val_loss: 0.2573 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2594 - accuracy: 0.9319 - val_loss: 0.2585 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2548 - accuracy: 0.9335 - val_loss: 0.2577 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2513 - accuracy: 0.9330 - val_loss: 0.2568 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2450 - accuracy: 0.9370 - val_loss: 0.2551 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2501 - accuracy: 0.9362 - val_loss: 0.2563 - val_accuracy: 0.9326 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2513 - accuracy: 0.9336 - val_loss: 0.2570 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2515 - accuracy: 0.9344 - val_loss: 0.2569 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2512 - accuracy: 0.9338 - val_loss: 0.2566 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2499 - accuracy: 0.9334 - val_loss: 0.2566 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2490 - accuracy: 0.9349 - val_loss: 0.2565 - val_accuracy: 0.9328 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2510 - accuracy: 0.9349 - val_loss: 0.2562 - val_accuracy: 0.9328 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2487 - accuracy: 0.9377 - val_loss: 0.2563 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2521 - accuracy: 0.9297 - val_loss: 0.2563 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2521 - accuracy: 0.9368 - val_loss: 0.2561 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2864 - accuracy: 0.9255\n",
      "17/17 [==============================] - 1s 17ms/step\n",
      "TP:5870, TN:9814, FP:641, FN:621, loss0.2863536775112152, acc0.9255281482355718, sn0.9043290710214142, sp0.9386896221903396, f10.9029380095369943, auc0.9744423743547793\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 52ms/step - loss: 3.3929 - accuracy: 0.6548 - val_loss: 3.4831 - val_accuracy: 0.4069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 2.5111 - accuracy: 0.8074 - val_loss: 2.6728 - val_accuracy: 0.5287 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 1.8651 - accuracy: 0.8413 - val_loss: 1.8843 - val_accuracy: 0.6450 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 1.3855 - accuracy: 0.8582 - val_loss: 1.2148 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.0577 - accuracy: 0.8678 - val_loss: 0.9148 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.8387 - accuracy: 0.8761 - val_loss: 0.7384 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.7022 - accuracy: 0.8796 - val_loss: 0.6085 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.6101 - accuracy: 0.8861 - val_loss: 0.5627 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.5454 - accuracy: 0.8912 - val_loss: 0.4934 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4995 - accuracy: 0.8981 - val_loss: 0.6750 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4823 - accuracy: 0.9018 - val_loss: 0.4423 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4527 - accuracy: 0.9002 - val_loss: 0.4855 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4415 - accuracy: 0.9009 - val_loss: 0.4701 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4309 - accuracy: 0.9043 - val_loss: 0.4198 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4319 - accuracy: 0.9017 - val_loss: 0.4412 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4150 - accuracy: 0.9063 - val_loss: 0.4380 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4152 - accuracy: 0.9025 - val_loss: 0.3796 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4049 - accuracy: 0.9054 - val_loss: 0.3691 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4017 - accuracy: 0.9057 - val_loss: 0.4139 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4061 - accuracy: 0.9029 - val_loss: 0.3722 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3963 - accuracy: 0.9065 - val_loss: 0.3633 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3923 - accuracy: 0.9078 - val_loss: 0.6246 - val_accuracy: 0.8480 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3944 - accuracy: 0.9079 - val_loss: 0.4197 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3902 - accuracy: 0.9071 - val_loss: 0.3638 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3966 - accuracy: 0.9059 - val_loss: 0.3497 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3907 - accuracy: 0.9077 - val_loss: 0.4051 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3937 - accuracy: 0.9034 - val_loss: 0.4519 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3930 - accuracy: 0.9071 - val_loss: 0.3717 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3829 - accuracy: 0.9119 - val_loss: 0.3486 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3892 - accuracy: 0.9082 - val_loss: 0.4195 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3847 - accuracy: 0.9092 - val_loss: 0.3590 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3919 - accuracy: 0.9090 - val_loss: 0.4303 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3844 - accuracy: 0.9071 - val_loss: 0.3688 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3831 - accuracy: 0.9096 - val_loss: 0.3597 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3842 - accuracy: 0.9079 - val_loss: 0.3548 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3798 - accuracy: 0.9093 - val_loss: 0.3719 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3858 - accuracy: 0.9108 - val_loss: 0.3530 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3841 - accuracy: 0.9087 - val_loss: 0.3787 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3786 - accuracy: 0.9126 - val_loss: 0.4010 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3780 - accuracy: 0.9086 - val_loss: 0.3555 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3779 - accuracy: 0.9125 - val_loss: 0.3536 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3811 - accuracy: 0.9107 - val_loss: 0.3869 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3823 - accuracy: 0.9087 - val_loss: 0.4741 - val_accuracy: 0.8765 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3783 - accuracy: 0.9115 - val_loss: 0.4090 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3730 - accuracy: 0.9127 - val_loss: 0.3621 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3803 - accuracy: 0.9092 - val_loss: 0.4182 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3869 - accuracy: 0.9077 - val_loss: 0.3811 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3646 - accuracy: 0.9116 - val_loss: 0.3503 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3856 - accuracy: 0.9075 - val_loss: 0.3597 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3683 - accuracy: 0.9130 - val_loss: 0.3649 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3683 - accuracy: 0.9144 - val_loss: 0.3651 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3698 - accuracy: 0.9138 - val_loss: 0.3940 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3799 - accuracy: 0.9111 - val_loss: 0.3346 - val_accuracy: 0.9254 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3688 - accuracy: 0.9150 - val_loss: 0.4169 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3756 - accuracy: 0.9100 - val_loss: 0.3465 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3686 - accuracy: 0.9119 - val_loss: 0.3498 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3711 - accuracy: 0.9145 - val_loss: 0.3782 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3783 - accuracy: 0.9131 - val_loss: 0.4331 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3722 - accuracy: 0.9092 - val_loss: 0.3326 - val_accuracy: 0.9258 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3720 - accuracy: 0.9125 - val_loss: 0.3929 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3722 - accuracy: 0.9127 - val_loss: 0.3460 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3713 - accuracy: 0.9132 - val_loss: 0.3390 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3666 - accuracy: 0.9123 - val_loss: 0.4253 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3681 - accuracy: 0.9139 - val_loss: 0.3659 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3617 - accuracy: 0.9151 - val_loss: 0.3724 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3665 - accuracy: 0.9138 - val_loss: 0.4039 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3738 - accuracy: 0.9115 - val_loss: 0.3660 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3684 - accuracy: 0.9107 - val_loss: 0.3441 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3634 - accuracy: 0.9164 - val_loss: 0.3342 - val_accuracy: 0.9244 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3654 - accuracy: 0.9139 - val_loss: 0.4371 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3681 - accuracy: 0.9120 - val_loss: 0.3588 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3361 - accuracy: 0.9186 - val_loss: 0.3361 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3198 - accuracy: 0.9249 - val_loss: 0.3148 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3049 - accuracy: 0.9264 - val_loss: 0.2989 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2996 - accuracy: 0.9261 - val_loss: 0.2933 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2864 - accuracy: 0.9308 - val_loss: 0.2911 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2852 - accuracy: 0.9275 - val_loss: 0.2910 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2799 - accuracy: 0.9273 - val_loss: 0.2780 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2757 - accuracy: 0.9277 - val_loss: 0.2723 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2657 - accuracy: 0.9324 - val_loss: 0.2659 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2634 - accuracy: 0.9328 - val_loss: 0.2634 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2580 - accuracy: 0.9337 - val_loss: 0.2600 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2598 - accuracy: 0.9326 - val_loss: 0.2597 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2549 - accuracy: 0.9333 - val_loss: 0.2585 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2514 - accuracy: 0.9344 - val_loss: 0.2584 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2527 - accuracy: 0.9347 - val_loss: 0.2602 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2527 - accuracy: 0.9344 - val_loss: 0.2576 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2507 - accuracy: 0.9349 - val_loss: 0.2592 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2506 - accuracy: 0.9344 - val_loss: 0.2597 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2508 - accuracy: 0.9322 - val_loss: 0.2584 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2494 - accuracy: 0.9360 - val_loss: 0.2603 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2482 - accuracy: 0.9333 - val_loss: 0.2597 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2501 - accuracy: 0.9349 - val_loss: 0.2590 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2484 - accuracy: 0.9338 - val_loss: 0.2590 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2525 - accuracy: 0.9323 - val_loss: 0.2591 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2514 - accuracy: 0.9338 - val_loss: 0.2590 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2510 - accuracy: 0.9355 - val_loss: 0.2589 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2483 - accuracy: 0.9367 - val_loss: 0.2586 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2494 - accuracy: 0.9373 - val_loss: 0.2591 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2473 - accuracy: 0.9366 - val_loss: 0.2591 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.2873 - accuracy: 0.9235\n",
      "17/17 [==============================] - 1s 13ms/step\n",
      "TP:5880, TN:9770, FP:685, FN:611, loss0.28729796409606934, acc0.9235217750501593, sn0.9058696656909567, sp0.9344811095169775, f10.9007352941176471, auc0.9744402156066292\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 20s 35ms/step - loss: 3.3386 - accuracy: 0.6580 - val_loss: 4.0939 - val_accuracy: 0.4043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 2.4060 - accuracy: 0.8080 - val_loss: 2.9608 - val_accuracy: 0.4083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 1.7451 - accuracy: 0.8400 - val_loss: 1.5871 - val_accuracy: 0.7852 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 1.2723 - accuracy: 0.8597 - val_loss: 1.0850 - val_accuracy: 0.8746 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.9698 - accuracy: 0.8677 - val_loss: 0.8476 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.7829 - accuracy: 0.8753 - val_loss: 0.7266 - val_accuracy: 0.8702 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6582 - accuracy: 0.8824 - val_loss: 0.6381 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5860 - accuracy: 0.8883 - val_loss: 0.5124 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5286 - accuracy: 0.8950 - val_loss: 0.4883 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4990 - accuracy: 0.8963 - val_loss: 0.5570 - val_accuracy: 0.8738 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4780 - accuracy: 0.8979 - val_loss: 0.4452 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4707 - accuracy: 0.8971 - val_loss: 0.5849 - val_accuracy: 0.8410 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4426 - accuracy: 0.9042 - val_loss: 0.4338 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4416 - accuracy: 0.9045 - val_loss: 0.5647 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4357 - accuracy: 0.8999 - val_loss: 0.4095 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4222 - accuracy: 0.9057 - val_loss: 0.3870 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4195 - accuracy: 0.9038 - val_loss: 0.3844 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.4205 - accuracy: 0.9036 - val_loss: 0.4952 - val_accuracy: 0.8783 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4126 - accuracy: 0.9093 - val_loss: 0.4694 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4035 - accuracy: 0.9061 - val_loss: 0.4947 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4000 - accuracy: 0.9102 - val_loss: 0.7118 - val_accuracy: 0.7732 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4023 - accuracy: 0.9068 - val_loss: 0.4903 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4087 - accuracy: 0.9069 - val_loss: 0.3973 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3947 - accuracy: 0.9096 - val_loss: 0.3969 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3912 - accuracy: 0.9100 - val_loss: 0.3618 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4020 - accuracy: 0.9067 - val_loss: 0.4358 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3958 - accuracy: 0.9079 - val_loss: 0.5580 - val_accuracy: 0.8277 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3899 - accuracy: 0.9072 - val_loss: 0.3575 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3927 - accuracy: 0.9114 - val_loss: 0.4353 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3920 - accuracy: 0.9094 - val_loss: 0.5596 - val_accuracy: 0.8385 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3885 - accuracy: 0.9093 - val_loss: 0.3615 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3868 - accuracy: 0.9097 - val_loss: 0.3573 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3855 - accuracy: 0.9089 - val_loss: 0.3559 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3877 - accuracy: 0.9090 - val_loss: 0.4118 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3782 - accuracy: 0.9117 - val_loss: 0.4195 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3871 - accuracy: 0.9081 - val_loss: 0.4048 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3896 - accuracy: 0.9085 - val_loss: 0.3700 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3808 - accuracy: 0.9121 - val_loss: 0.3558 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3843 - accuracy: 0.9144 - val_loss: 0.3758 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3817 - accuracy: 0.9132 - val_loss: 0.4248 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3802 - accuracy: 0.9127 - val_loss: 0.3502 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3924 - accuracy: 0.9138 - val_loss: 0.3712 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3777 - accuracy: 0.9139 - val_loss: 0.3686 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3860 - accuracy: 0.9096 - val_loss: 0.3515 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3753 - accuracy: 0.9113 - val_loss: 0.3621 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3902 - accuracy: 0.9112 - val_loss: 0.3584 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3697 - accuracy: 0.9120 - val_loss: 0.3916 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3808 - accuracy: 0.9096 - val_loss: 0.6376 - val_accuracy: 0.8183 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3782 - accuracy: 0.9126 - val_loss: 0.3663 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3884 - accuracy: 0.9095 - val_loss: 0.3991 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3739 - accuracy: 0.9127 - val_loss: 0.4169 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3761 - accuracy: 0.9118 - val_loss: 0.4451 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3808 - accuracy: 0.9099 - val_loss: 0.3419 - val_accuracy: 0.9246 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3725 - accuracy: 0.9113 - val_loss: 0.5013 - val_accuracy: 0.8504 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3688 - accuracy: 0.9157 - val_loss: 0.4866 - val_accuracy: 0.8706 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3779 - accuracy: 0.9102 - val_loss: 0.3694 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3810 - accuracy: 0.9085 - val_loss: 0.3959 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3702 - accuracy: 0.9135 - val_loss: 0.3418 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3720 - accuracy: 0.9124 - val_loss: 0.3638 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3671 - accuracy: 0.9132 - val_loss: 0.3450 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3675 - accuracy: 0.9181 - val_loss: 0.3605 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3791 - accuracy: 0.9102 - val_loss: 0.3819 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3704 - accuracy: 0.9120 - val_loss: 0.3935 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3686 - accuracy: 0.9108 - val_loss: 0.3721 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3680 - accuracy: 0.9134 - val_loss: 0.4235 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3592 - accuracy: 0.9139 - val_loss: 0.3514 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3803 - accuracy: 0.9094 - val_loss: 0.4544 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3739 - accuracy: 0.9124 - val_loss: 0.3342 - val_accuracy: 0.9232 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3653 - accuracy: 0.9151 - val_loss: 0.3631 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3643 - accuracy: 0.9155 - val_loss: 0.3836 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3701 - accuracy: 0.9138 - val_loss: 0.4045 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3405 - accuracy: 0.9215 - val_loss: 0.3238 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3252 - accuracy: 0.9274 - val_loss: 0.3095 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3104 - accuracy: 0.9267 - val_loss: 0.3042 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3032 - accuracy: 0.9291 - val_loss: 0.2980 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2945 - accuracy: 0.9271 - val_loss: 0.2951 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2842 - accuracy: 0.9314 - val_loss: 0.2784 - val_accuracy: 0.9320 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2849 - accuracy: 0.9303 - val_loss: 0.2884 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2793 - accuracy: 0.9275 - val_loss: 0.2776 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2699 - accuracy: 0.9313 - val_loss: 0.2787 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2705 - accuracy: 0.9287 - val_loss: 0.2726 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2612 - accuracy: 0.9328 - val_loss: 0.2608 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2610 - accuracy: 0.9327 - val_loss: 0.2604 - val_accuracy: 0.9336 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2557 - accuracy: 0.9350 - val_loss: 0.2604 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2549 - accuracy: 0.9331 - val_loss: 0.2604 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2556 - accuracy: 0.9331 - val_loss: 0.2590 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2541 - accuracy: 0.9346 - val_loss: 0.2596 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2512 - accuracy: 0.9370 - val_loss: 0.2591 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2513 - accuracy: 0.9355 - val_loss: 0.2593 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2552 - accuracy: 0.9341 - val_loss: 0.2587 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2525 - accuracy: 0.9322 - val_loss: 0.2587 - val_accuracy: 0.9330 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2478 - accuracy: 0.9370 - val_loss: 0.2587 - val_accuracy: 0.9336 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2506 - accuracy: 0.9337 - val_loss: 0.2584 - val_accuracy: 0.9330 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2479 - accuracy: 0.9391 - val_loss: 0.2583 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2478 - accuracy: 0.9378 - val_loss: 0.2580 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2460 - accuracy: 0.9365 - val_loss: 0.2579 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2501 - accuracy: 0.9365 - val_loss: 0.2579 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2515 - accuracy: 0.9360 - val_loss: 0.2582 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2510 - accuracy: 0.9370 - val_loss: 0.2580 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2504 - accuracy: 0.9363 - val_loss: 0.2582 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.2915 - accuracy: 0.9249\n",
      "17/17 [==============================] - 1s 13ms/step\n",
      "TP:5842, TN:9832, FP:623, FN:649, loss0.29149606823921204, acc0.9249380384751564, sn0.9000154059466954, sp0.9404112864658059, f10.9018215498610681, auc0.9739509902280912\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 19s 35ms/step - loss: 3.3588 - accuracy: 0.6597 - val_loss: 3.9942 - val_accuracy: 0.3977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 2.4655 - accuracy: 0.8151 - val_loss: 2.7403 - val_accuracy: 0.4565 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 1.8143 - accuracy: 0.8437 - val_loss: 1.5893 - val_accuracy: 0.8363 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 1.3455 - accuracy: 0.8606 - val_loss: 1.1650 - val_accuracy: 0.8658 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 1.0323 - accuracy: 0.8656 - val_loss: 0.8837 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.8223 - accuracy: 0.8748 - val_loss: 0.7493 - val_accuracy: 0.8771 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.6800 - accuracy: 0.8859 - val_loss: 0.6139 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5967 - accuracy: 0.8906 - val_loss: 0.6883 - val_accuracy: 0.8474 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5317 - accuracy: 0.8941 - val_loss: 0.6727 - val_accuracy: 0.8430 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5027 - accuracy: 0.8981 - val_loss: 0.5063 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4766 - accuracy: 0.8973 - val_loss: 0.4897 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4586 - accuracy: 0.8981 - val_loss: 0.4461 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4473 - accuracy: 0.8985 - val_loss: 0.3999 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4296 - accuracy: 0.9032 - val_loss: 0.4138 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4219 - accuracy: 0.9052 - val_loss: 0.4188 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4150 - accuracy: 0.9058 - val_loss: 0.3997 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4039 - accuracy: 0.9068 - val_loss: 0.4569 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4130 - accuracy: 0.9057 - val_loss: 0.3913 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4114 - accuracy: 0.9072 - val_loss: 0.3682 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4039 - accuracy: 0.9073 - val_loss: 0.4733 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3916 - accuracy: 0.9097 - val_loss: 0.4014 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3981 - accuracy: 0.9082 - val_loss: 0.3649 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3999 - accuracy: 0.9081 - val_loss: 0.3777 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3942 - accuracy: 0.9081 - val_loss: 0.4728 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3970 - accuracy: 0.9053 - val_loss: 0.5324 - val_accuracy: 0.8462 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3877 - accuracy: 0.9050 - val_loss: 0.3382 - val_accuracy: 0.9230 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3878 - accuracy: 0.9057 - val_loss: 0.3994 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3905 - accuracy: 0.9067 - val_loss: 0.4075 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3843 - accuracy: 0.9066 - val_loss: 0.4085 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3823 - accuracy: 0.9121 - val_loss: 0.3911 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3833 - accuracy: 0.9099 - val_loss: 0.3568 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3851 - accuracy: 0.9099 - val_loss: 0.3903 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3895 - accuracy: 0.9091 - val_loss: 0.3636 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3801 - accuracy: 0.9102 - val_loss: 0.3707 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3854 - accuracy: 0.9110 - val_loss: 0.4496 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3870 - accuracy: 0.9120 - val_loss: 0.4098 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3862 - accuracy: 0.9094 - val_loss: 0.3816 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3883 - accuracy: 0.9095 - val_loss: 0.4480 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3899 - accuracy: 0.9093 - val_loss: 0.5758 - val_accuracy: 0.8420 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3818 - accuracy: 0.9108 - val_loss: 0.3619 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3782 - accuracy: 0.9121 - val_loss: 0.3642 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3819 - accuracy: 0.9139 - val_loss: 0.3625 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3885 - accuracy: 0.9081 - val_loss: 0.3707 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3809 - accuracy: 0.9102 - val_loss: 0.4102 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3771 - accuracy: 0.9120 - val_loss: 0.4073 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3809 - accuracy: 0.9088 - val_loss: 0.3578 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3812 - accuracy: 0.9117 - val_loss: 0.3467 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3768 - accuracy: 0.9106 - val_loss: 0.3538 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3670 - accuracy: 0.9143 - val_loss: 0.5328 - val_accuracy: 0.8375 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3724 - accuracy: 0.9118 - val_loss: 0.3612 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3735 - accuracy: 0.9122 - val_loss: 0.6430 - val_accuracy: 0.7848 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3838 - accuracy: 0.9093 - val_loss: 0.3692 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3695 - accuracy: 0.9123 - val_loss: 0.4148 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3766 - accuracy: 0.9103 - val_loss: 0.3514 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3713 - accuracy: 0.9104 - val_loss: 0.3749 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3689 - accuracy: 0.9124 - val_loss: 0.3832 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3731 - accuracy: 0.9098 - val_loss: 0.3612 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3636 - accuracy: 0.9140 - val_loss: 0.3785 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3720 - accuracy: 0.9108 - val_loss: 0.3407 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3681 - accuracy: 0.9124 - val_loss: 0.3513 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3642 - accuracy: 0.9146 - val_loss: 0.4161 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3682 - accuracy: 0.9136 - val_loss: 0.4017 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3719 - accuracy: 0.9125 - val_loss: 0.3595 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3693 - accuracy: 0.9119 - val_loss: 0.4820 - val_accuracy: 0.8708 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3680 - accuracy: 0.9130 - val_loss: 0.3573 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3698 - accuracy: 0.9126 - val_loss: 0.4967 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3666 - accuracy: 0.9144 - val_loss: 0.3521 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3657 - accuracy: 0.9108 - val_loss: 0.3384 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3654 - accuracy: 0.9125 - val_loss: 0.3556 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3676 - accuracy: 0.9143 - val_loss: 0.3614 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3644 - accuracy: 0.9144 - val_loss: 0.3320 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3356 - accuracy: 0.9220 - val_loss: 0.3159 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3219 - accuracy: 0.9209 - val_loss: 0.3074 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3099 - accuracy: 0.9261 - val_loss: 0.3097 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2959 - accuracy: 0.9288 - val_loss: 0.3179 - val_accuracy: 0.9172 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2931 - accuracy: 0.9285 - val_loss: 0.2992 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2871 - accuracy: 0.9277 - val_loss: 0.2773 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2779 - accuracy: 0.9305 - val_loss: 0.2763 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2726 - accuracy: 0.9333 - val_loss: 0.2701 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2703 - accuracy: 0.9289 - val_loss: 0.2678 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2643 - accuracy: 0.9323 - val_loss: 0.2611 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2577 - accuracy: 0.9341 - val_loss: 0.2570 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2605 - accuracy: 0.9316 - val_loss: 0.2586 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2554 - accuracy: 0.9344 - val_loss: 0.2585 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2528 - accuracy: 0.9363 - val_loss: 0.2571 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2568 - accuracy: 0.9327 - val_loss: 0.2572 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2577 - accuracy: 0.9344 - val_loss: 0.2582 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2522 - accuracy: 0.9334 - val_loss: 0.2583 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2525 - accuracy: 0.9350 - val_loss: 0.2571 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2530 - accuracy: 0.9337 - val_loss: 0.2560 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2536 - accuracy: 0.9344 - val_loss: 0.2560 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2524 - accuracy: 0.9337 - val_loss: 0.2563 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2450 - accuracy: 0.9364 - val_loss: 0.2560 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2490 - accuracy: 0.9342 - val_loss: 0.2559 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2527 - accuracy: 0.9340 - val_loss: 0.2561 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2521 - accuracy: 0.9332 - val_loss: 0.2560 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.2513 - accuracy: 0.9340 - val_loss: 0.2561 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2526 - accuracy: 0.9356 - val_loss: 0.2558 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2520 - accuracy: 0.9336 - val_loss: 0.2559 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2506 - accuracy: 0.9344 - val_loss: 0.2560 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.2854 - accuracy: 0.9259\n",
      "17/17 [==============================] - 1s 13ms/step\n",
      "TP:5895, TN:9795, FP:660, FN:596, loss0.2854126989841461, acc0.925882214091821, sn0.9081805576952704, sp0.9368723098995696, f10.9037252797792426, auc0.9745275675454246\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 37ms/step - loss: 3.3490 - accuracy: 0.6702 - val_loss: 3.8188 - val_accuracy: 0.4067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 2.4752 - accuracy: 0.8086 - val_loss: 2.1670 - val_accuracy: 0.7976 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 1.8258 - accuracy: 0.8414 - val_loss: 1.5660 - val_accuracy: 0.8470 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.3588 - accuracy: 0.8542 - val_loss: 1.1631 - val_accuracy: 0.8644 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 1.0387 - accuracy: 0.8669 - val_loss: 0.8802 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.8337 - accuracy: 0.8735 - val_loss: 0.7259 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6874 - accuracy: 0.8821 - val_loss: 0.8570 - val_accuracy: 0.8107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.6000 - accuracy: 0.8907 - val_loss: 0.6465 - val_accuracy: 0.8771 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5434 - accuracy: 0.8919 - val_loss: 0.6380 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5000 - accuracy: 0.8979 - val_loss: 0.5759 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4726 - accuracy: 0.9008 - val_loss: 0.5058 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4583 - accuracy: 0.9016 - val_loss: 0.4338 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4446 - accuracy: 0.9008 - val_loss: 0.4091 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4340 - accuracy: 0.9037 - val_loss: 0.3926 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4276 - accuracy: 0.9028 - val_loss: 0.4074 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.4189 - accuracy: 0.9059 - val_loss: 0.4727 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4163 - accuracy: 0.9047 - val_loss: 0.4058 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4134 - accuracy: 0.9046 - val_loss: 0.3689 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4029 - accuracy: 0.9081 - val_loss: 0.4839 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4043 - accuracy: 0.9073 - val_loss: 0.3590 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.4088 - accuracy: 0.9030 - val_loss: 0.4506 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4028 - accuracy: 0.9077 - val_loss: 0.3838 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3922 - accuracy: 0.9085 - val_loss: 0.3581 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3871 - accuracy: 0.9104 - val_loss: 0.6117 - val_accuracy: 0.8285 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4045 - accuracy: 0.9056 - val_loss: 0.3776 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3866 - accuracy: 0.9096 - val_loss: 0.3773 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3922 - accuracy: 0.9073 - val_loss: 0.3862 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3892 - accuracy: 0.9073 - val_loss: 0.3549 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3893 - accuracy: 0.9089 - val_loss: 0.4160 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3814 - accuracy: 0.9108 - val_loss: 0.3544 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3807 - accuracy: 0.9112 - val_loss: 0.3613 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3813 - accuracy: 0.9091 - val_loss: 0.6241 - val_accuracy: 0.8221 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3837 - accuracy: 0.9108 - val_loss: 0.4228 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3798 - accuracy: 0.9094 - val_loss: 0.5394 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3847 - accuracy: 0.9110 - val_loss: 0.3569 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3808 - accuracy: 0.9112 - val_loss: 0.3858 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3812 - accuracy: 0.9108 - val_loss: 0.3639 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3866 - accuracy: 0.9100 - val_loss: 0.3657 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3765 - accuracy: 0.9103 - val_loss: 0.3663 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3756 - accuracy: 0.9146 - val_loss: 0.4044 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3859 - accuracy: 0.9085 - val_loss: 0.3609 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3781 - accuracy: 0.9139 - val_loss: 0.3607 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3888 - accuracy: 0.9097 - val_loss: 0.3483 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3813 - accuracy: 0.9112 - val_loss: 0.4124 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3702 - accuracy: 0.9118 - val_loss: 0.3693 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3881 - accuracy: 0.9102 - val_loss: 0.3537 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3749 - accuracy: 0.9124 - val_loss: 0.3588 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3793 - accuracy: 0.9104 - val_loss: 0.3629 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3820 - accuracy: 0.9107 - val_loss: 0.4045 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3801 - accuracy: 0.9118 - val_loss: 0.3730 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3691 - accuracy: 0.9138 - val_loss: 0.4008 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3713 - accuracy: 0.9130 - val_loss: 0.4064 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3884 - accuracy: 0.9065 - val_loss: 0.3575 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3690 - accuracy: 0.9113 - val_loss: 0.3417 - val_accuracy: 0.9228 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3730 - accuracy: 0.9101 - val_loss: 0.6006 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3736 - accuracy: 0.9120 - val_loss: 0.3474 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3708 - accuracy: 0.9142 - val_loss: 0.3365 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3757 - accuracy: 0.9119 - val_loss: 0.4495 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3769 - accuracy: 0.9106 - val_loss: 0.3989 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3710 - accuracy: 0.9155 - val_loss: 0.3630 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3653 - accuracy: 0.9157 - val_loss: 0.3600 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3684 - accuracy: 0.9120 - val_loss: 0.3686 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3720 - accuracy: 0.9111 - val_loss: 0.3434 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3731 - accuracy: 0.9158 - val_loss: 0.3523 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3657 - accuracy: 0.9126 - val_loss: 0.3530 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3711 - accuracy: 0.9124 - val_loss: 0.3663 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3631 - accuracy: 0.9139 - val_loss: 0.3686 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3716 - accuracy: 0.9153 - val_loss: 0.3728 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3650 - accuracy: 0.9148 - val_loss: 0.3629 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3664 - accuracy: 0.9114 - val_loss: 0.3435 - val_accuracy: 0.9234 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3696 - accuracy: 0.9150 - val_loss: 0.3619 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3426 - accuracy: 0.9202 - val_loss: 0.3426 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3232 - accuracy: 0.9255 - val_loss: 0.3087 - val_accuracy: 0.9296 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3110 - accuracy: 0.9284 - val_loss: 0.2991 - val_accuracy: 0.9296 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3013 - accuracy: 0.9266 - val_loss: 0.2947 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2981 - accuracy: 0.9290 - val_loss: 0.2871 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2859 - accuracy: 0.9299 - val_loss: 0.2819 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2802 - accuracy: 0.9323 - val_loss: 0.2740 - val_accuracy: 0.9304 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2755 - accuracy: 0.9285 - val_loss: 0.2714 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2708 - accuracy: 0.9302 - val_loss: 0.2660 - val_accuracy: 0.9290 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2631 - accuracy: 0.9320 - val_loss: 0.2695 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2585 - accuracy: 0.9335 - val_loss: 0.2610 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2606 - accuracy: 0.9352 - val_loss: 0.2593 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2573 - accuracy: 0.9340 - val_loss: 0.2582 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2583 - accuracy: 0.9358 - val_loss: 0.2577 - val_accuracy: 0.9332 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2530 - accuracy: 0.9365 - val_loss: 0.2573 - val_accuracy: 0.9340 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2562 - accuracy: 0.9351 - val_loss: 0.2577 - val_accuracy: 0.9344 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2546 - accuracy: 0.9331 - val_loss: 0.2572 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2516 - accuracy: 0.9330 - val_loss: 0.2558 - val_accuracy: 0.9346 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2534 - accuracy: 0.9339 - val_loss: 0.2563 - val_accuracy: 0.9340 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2528 - accuracy: 0.9340 - val_loss: 0.2560 - val_accuracy: 0.9336 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.2538 - accuracy: 0.9338 - val_loss: 0.2557 - val_accuracy: 0.9340 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2512 - accuracy: 0.9328 - val_loss: 0.2558 - val_accuracy: 0.9342 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2511 - accuracy: 0.9366 - val_loss: 0.2555 - val_accuracy: 0.9344 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2481 - accuracy: 0.9368 - val_loss: 0.2554 - val_accuracy: 0.9344 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2530 - accuracy: 0.9331 - val_loss: 0.2554 - val_accuracy: 0.9344 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2453 - accuracy: 0.9379 - val_loss: 0.2552 - val_accuracy: 0.9342 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.2462 - accuracy: 0.9361 - val_loss: 0.2553 - val_accuracy: 0.9340 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.2492 - accuracy: 0.9359 - val_loss: 0.2552 - val_accuracy: 0.9336 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.2510 - accuracy: 0.9348 - val_loss: 0.2552 - val_accuracy: 0.9340 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.2836 - accuracy: 0.9251\n",
      "17/17 [==============================] - 1s 13ms/step\n",
      "TP:5809, TN:9868, FP:587, FN:682, loss0.28356751799583435, acc0.925115071403281, sn0.8949314435372053, sp0.9438546150167384, f10.901528672305424, auc0.9745998450858749\n",
      "Average Test loss:  0.2859578043222427\n",
      "Average Accuracy:  0.9250442582320311\n",
      "Average Sensitivity:  0.9038360807271607\n",
      "Average Specificity:  0.9382113821138212\n",
      "Average F1 Score:  0.9023201551639065\n",
      "Average AUC Score:  0.9744923350073581\n",
      "AUC for ROC curve 1: 0.9747\n",
      "AUC for ROC curve 2: 0.9747\n",
      "AUC for ROC curve 3: 0.9743\n",
      "AUC for ROC curve 4: 0.9743\n",
      "AUC for ROC curve 5: 0.9744\n",
      "AUC for ROC curve 6: 0.9744\n",
      "AUC for ROC curve 7: 0.9747\n",
      "AUC for ROC curve 8: 0.9747\n",
      "AUC for ROC curve 9: 0.9748\n",
      "AUC for ROC curve 10: 0.9748\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f9963-cfa7-47f5-8701-2a24e58e20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
