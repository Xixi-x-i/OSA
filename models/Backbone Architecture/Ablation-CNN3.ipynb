{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 18:27:36.563853: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = BatchNormalization()(input1)\n",
    "    x1 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=5)(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=6)(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "    x2 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=6)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    # CNN-1\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "    x3 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    \n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=2)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "\n",
    "    attention = ResidualAttentionBlock(64, 64)\n",
    "    x1 = attention(x1)\n",
    "    x2 = attention(x2)\n",
    "    x3 = attention(x3)\n",
    "\n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1,input2,input3], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  # Convert to two categories\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 12s 27ms/step - loss: 1.2955 - accuracy: 0.5910 - val_loss: 1.1966 - val_accuracy: 0.5995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 1.1148 - accuracy: 0.6210 - val_loss: 1.2063 - val_accuracy: 0.3881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.9800 - accuracy: 0.6527 - val_loss: 1.1525 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8821 - accuracy: 0.6750 - val_loss: 1.0513 - val_accuracy: 0.4041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.7987 - accuracy: 0.7055 - val_loss: 0.8565 - val_accuracy: 0.5493 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.7323 - accuracy: 0.7253 - val_loss: 0.7111 - val_accuracy: 0.7784 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.6781 - accuracy: 0.7438 - val_loss: 0.6350 - val_accuracy: 0.8008 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.6402 - accuracy: 0.7571 - val_loss: 0.5924 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5991 - accuracy: 0.7713 - val_loss: 0.5476 - val_accuracy: 0.8117 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5678 - accuracy: 0.7850 - val_loss: 0.5251 - val_accuracy: 0.8149 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5502 - accuracy: 0.7921 - val_loss: 0.4865 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5291 - accuracy: 0.8022 - val_loss: 0.4767 - val_accuracy: 0.8271 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5039 - accuracy: 0.8084 - val_loss: 0.4406 - val_accuracy: 0.8406 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4906 - accuracy: 0.8128 - val_loss: 0.4259 - val_accuracy: 0.8488 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4705 - accuracy: 0.8215 - val_loss: 0.4179 - val_accuracy: 0.8452 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4515 - accuracy: 0.8320 - val_loss: 0.3925 - val_accuracy: 0.8614 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4420 - accuracy: 0.8349 - val_loss: 0.3685 - val_accuracy: 0.8738 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4254 - accuracy: 0.8409 - val_loss: 0.3582 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4188 - accuracy: 0.8474 - val_loss: 0.3476 - val_accuracy: 0.8805 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4067 - accuracy: 0.8498 - val_loss: 0.3380 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3981 - accuracy: 0.8579 - val_loss: 0.3418 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3984 - accuracy: 0.8525 - val_loss: 0.3347 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3909 - accuracy: 0.8566 - val_loss: 0.3333 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3840 - accuracy: 0.8591 - val_loss: 0.3177 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3869 - accuracy: 0.8543 - val_loss: 0.3326 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3828 - accuracy: 0.8584 - val_loss: 0.3193 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3757 - accuracy: 0.8611 - val_loss: 0.3197 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3791 - accuracy: 0.8586 - val_loss: 0.3126 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3732 - accuracy: 0.8620 - val_loss: 0.3228 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3669 - accuracy: 0.8646 - val_loss: 0.3140 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3697 - accuracy: 0.8631 - val_loss: 0.3116 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3573 - accuracy: 0.8686 - val_loss: 0.3056 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3673 - accuracy: 0.8645 - val_loss: 0.3167 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3651 - accuracy: 0.8653 - val_loss: 0.3065 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3635 - accuracy: 0.8661 - val_loss: 0.3233 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3613 - accuracy: 0.8680 - val_loss: 0.3162 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3571 - accuracy: 0.8706 - val_loss: 0.3200 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3603 - accuracy: 0.8661 - val_loss: 0.3128 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3594 - accuracy: 0.8693 - val_loss: 0.3126 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3617 - accuracy: 0.8649 - val_loss: 0.2985 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3588 - accuracy: 0.8685 - val_loss: 0.2932 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3574 - accuracy: 0.8668 - val_loss: 0.2982 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3530 - accuracy: 0.8714 - val_loss: 0.3238 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3516 - accuracy: 0.8694 - val_loss: 0.3072 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3513 - accuracy: 0.8714 - val_loss: 0.3155 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3494 - accuracy: 0.8710 - val_loss: 0.2924 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3516 - accuracy: 0.8690 - val_loss: 0.3010 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3424 - accuracy: 0.8727 - val_loss: 0.2859 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3472 - accuracy: 0.8735 - val_loss: 0.3227 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3485 - accuracy: 0.8718 - val_loss: 0.3108 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3455 - accuracy: 0.8738 - val_loss: 0.3003 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3499 - accuracy: 0.8703 - val_loss: 0.3005 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3413 - accuracy: 0.8732 - val_loss: 0.3137 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3501 - accuracy: 0.8685 - val_loss: 0.3003 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3430 - accuracy: 0.8753 - val_loss: 0.2886 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3424 - accuracy: 0.8742 - val_loss: 0.3201 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3450 - accuracy: 0.8719 - val_loss: 0.2891 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3454 - accuracy: 0.8749 - val_loss: 0.3057 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3482 - accuracy: 0.8692 - val_loss: 0.3130 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3454 - accuracy: 0.8717 - val_loss: 0.3272 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3412 - accuracy: 0.8752 - val_loss: 0.3132 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3480 - accuracy: 0.8700 - val_loss: 0.2921 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3409 - accuracy: 0.8718 - val_loss: 0.2944 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3451 - accuracy: 0.8726 - val_loss: 0.3173 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3384 - accuracy: 0.8733 - val_loss: 0.2885 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3385 - accuracy: 0.8744 - val_loss: 0.2905 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3416 - accuracy: 0.8738 - val_loss: 0.2974 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3392 - accuracy: 0.8707 - val_loss: 0.2962 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3350 - accuracy: 0.8750 - val_loss: 0.3080 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3376 - accuracy: 0.8735 - val_loss: 0.2920 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3320 - accuracy: 0.8760 - val_loss: 0.2839 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3362 - accuracy: 0.8751 - val_loss: 0.2926 - val_accuracy: 0.8969 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3281 - accuracy: 0.8783 - val_loss: 0.2915 - val_accuracy: 0.8981 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3303 - accuracy: 0.8785 - val_loss: 0.2919 - val_accuracy: 0.8983 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3286 - accuracy: 0.8782 - val_loss: 0.2892 - val_accuracy: 0.8979 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3291 - accuracy: 0.8825 - val_loss: 0.2885 - val_accuracy: 0.8979 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3263 - accuracy: 0.8813 - val_loss: 0.2887 - val_accuracy: 0.8975 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3299 - accuracy: 0.8779 - val_loss: 0.2890 - val_accuracy: 0.8983 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3284 - accuracy: 0.8804 - val_loss: 0.2874 - val_accuracy: 0.8971 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3194 - accuracy: 0.8818 - val_loss: 0.2892 - val_accuracy: 0.8979 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3257 - accuracy: 0.8791 - val_loss: 0.2850 - val_accuracy: 0.8999 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3240 - accuracy: 0.8791 - val_loss: 0.2859 - val_accuracy: 0.8991 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3229 - accuracy: 0.8831 - val_loss: 0.2856 - val_accuracy: 0.8997 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3219 - accuracy: 0.8810 - val_loss: 0.2864 - val_accuracy: 0.8989 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3233 - accuracy: 0.8803 - val_loss: 0.2865 - val_accuracy: 0.8989 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3148 - accuracy: 0.8809 - val_loss: 0.2862 - val_accuracy: 0.8991 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3278 - accuracy: 0.8793 - val_loss: 0.2853 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3266 - accuracy: 0.8814 - val_loss: 0.2858 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3177 - accuracy: 0.8838 - val_loss: 0.2862 - val_accuracy: 0.8985 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3282 - accuracy: 0.8765 - val_loss: 0.2852 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3198 - accuracy: 0.8824 - val_loss: 0.2860 - val_accuracy: 0.8985 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3236 - accuracy: 0.8820 - val_loss: 0.2863 - val_accuracy: 0.8983 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3255 - accuracy: 0.8771 - val_loss: 0.2870 - val_accuracy: 0.8979 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3281 - accuracy: 0.8803 - val_loss: 0.2863 - val_accuracy: 0.8983 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3149 - accuracy: 0.8849 - val_loss: 0.2865 - val_accuracy: 0.8981 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3260 - accuracy: 0.8786 - val_loss: 0.2866 - val_accuracy: 0.8985 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3176 - accuracy: 0.8826 - val_loss: 0.2864 - val_accuracy: 0.8985 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3230 - accuracy: 0.8790 - val_loss: 0.2863 - val_accuracy: 0.8985 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3176 - accuracy: 0.8836 - val_loss: 0.2864 - val_accuracy: 0.8985 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3252 - accuracy: 0.8800 - val_loss: 0.2856 - val_accuracy: 0.8989 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2748 - accuracy: 0.9020\n",
      "17/17 [==============================] - 1s 7ms/step\n",
      "TP:5716, TN:9570, FP:885, FN:775, loss0.2747778594493866, acc0.9020417797710374, sn0.8806039131104606, sp0.9153515064562411, f10.8732050106935534, auc0.9620128624551038\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 10s 24ms/step - loss: 1.2800 - accuracy: 0.5870 - val_loss: 1.3044 - val_accuracy: 0.3893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 1.1037 - accuracy: 0.6181 - val_loss: 1.3668 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.9819 - accuracy: 0.6350 - val_loss: 1.2839 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.8926 - accuracy: 0.6458 - val_loss: 1.1035 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.8157 - accuracy: 0.6730 - val_loss: 0.9950 - val_accuracy: 0.3915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.7457 - accuracy: 0.7034 - val_loss: 0.8113 - val_accuracy: 0.5762 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.6807 - accuracy: 0.7382 - val_loss: 0.6910 - val_accuracy: 0.7339 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.6407 - accuracy: 0.7531 - val_loss: 0.5902 - val_accuracy: 0.8209 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.6040 - accuracy: 0.7663 - val_loss: 0.5312 - val_accuracy: 0.8177 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.5732 - accuracy: 0.7799 - val_loss: 0.5055 - val_accuracy: 0.8143 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.5499 - accuracy: 0.7889 - val_loss: 0.4851 - val_accuracy: 0.8243 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.5244 - accuracy: 0.8019 - val_loss: 0.4645 - val_accuracy: 0.8418 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.5120 - accuracy: 0.8031 - val_loss: 0.4425 - val_accuracy: 0.8359 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4838 - accuracy: 0.8190 - val_loss: 0.4278 - val_accuracy: 0.8418 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4695 - accuracy: 0.8214 - val_loss: 0.4223 - val_accuracy: 0.8434 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4510 - accuracy: 0.8325 - val_loss: 0.3985 - val_accuracy: 0.8592 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.4388 - accuracy: 0.8361 - val_loss: 0.3966 - val_accuracy: 0.8598 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4355 - accuracy: 0.8376 - val_loss: 0.3703 - val_accuracy: 0.8746 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4273 - accuracy: 0.8412 - val_loss: 0.3616 - val_accuracy: 0.8751 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4175 - accuracy: 0.8467 - val_loss: 0.3527 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.4156 - accuracy: 0.8492 - val_loss: 0.3547 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3997 - accuracy: 0.8553 - val_loss: 0.3454 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.4082 - accuracy: 0.8482 - val_loss: 0.3502 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3904 - accuracy: 0.8566 - val_loss: 0.3369 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3900 - accuracy: 0.8576 - val_loss: 0.3274 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3878 - accuracy: 0.8590 - val_loss: 0.3342 - val_accuracy: 0.8837 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3901 - accuracy: 0.8565 - val_loss: 0.3413 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3779 - accuracy: 0.8627 - val_loss: 0.3361 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3772 - accuracy: 0.8617 - val_loss: 0.3518 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3726 - accuracy: 0.8615 - val_loss: 0.3151 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3653 - accuracy: 0.8671 - val_loss: 0.3229 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3728 - accuracy: 0.8632 - val_loss: 0.3354 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3714 - accuracy: 0.8655 - val_loss: 0.3186 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3626 - accuracy: 0.8661 - val_loss: 0.3093 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3600 - accuracy: 0.8649 - val_loss: 0.3317 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3619 - accuracy: 0.8624 - val_loss: 0.3091 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3681 - accuracy: 0.8634 - val_loss: 0.3048 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3618 - accuracy: 0.8643 - val_loss: 0.3194 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3535 - accuracy: 0.8692 - val_loss: 0.3181 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3614 - accuracy: 0.8665 - val_loss: 0.2956 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3535 - accuracy: 0.8699 - val_loss: 0.3231 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3528 - accuracy: 0.8689 - val_loss: 0.3065 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3535 - accuracy: 0.8685 - val_loss: 0.3035 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3651 - accuracy: 0.8616 - val_loss: 0.3161 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3549 - accuracy: 0.8702 - val_loss: 0.2979 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3584 - accuracy: 0.8645 - val_loss: 0.3134 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3545 - accuracy: 0.8665 - val_loss: 0.3004 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3561 - accuracy: 0.8679 - val_loss: 0.3038 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3442 - accuracy: 0.8722 - val_loss: 0.3020 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3518 - accuracy: 0.8692 - val_loss: 0.2942 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3559 - accuracy: 0.8674 - val_loss: 0.3115 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3500 - accuracy: 0.8713 - val_loss: 0.2861 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3516 - accuracy: 0.8679 - val_loss: 0.3053 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3454 - accuracy: 0.8704 - val_loss: 0.2889 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3482 - accuracy: 0.8714 - val_loss: 0.2878 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3457 - accuracy: 0.8707 - val_loss: 0.2877 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3488 - accuracy: 0.8694 - val_loss: 0.3074 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3472 - accuracy: 0.8672 - val_loss: 0.3217 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3385 - accuracy: 0.8764 - val_loss: 0.2855 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3444 - accuracy: 0.8711 - val_loss: 0.2896 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3432 - accuracy: 0.8735 - val_loss: 0.2919 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3467 - accuracy: 0.8702 - val_loss: 0.2965 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3437 - accuracy: 0.8743 - val_loss: 0.3120 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3380 - accuracy: 0.8776 - val_loss: 0.3258 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3409 - accuracy: 0.8726 - val_loss: 0.3033 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3447 - accuracy: 0.8727 - val_loss: 0.2913 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3337 - accuracy: 0.8782 - val_loss: 0.2834 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3391 - accuracy: 0.8748 - val_loss: 0.2864 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3434 - accuracy: 0.8732 - val_loss: 0.3101 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3419 - accuracy: 0.8712 - val_loss: 0.2992 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3378 - accuracy: 0.8738 - val_loss: 0.2987 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3371 - accuracy: 0.8750 - val_loss: 0.2950 - val_accuracy: 0.8951 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3383 - accuracy: 0.8735 - val_loss: 0.2909 - val_accuracy: 0.8961 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3297 - accuracy: 0.8781 - val_loss: 0.2913 - val_accuracy: 0.8953 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3279 - accuracy: 0.8751 - val_loss: 0.2938 - val_accuracy: 0.8945 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3290 - accuracy: 0.8781 - val_loss: 0.2949 - val_accuracy: 0.8945 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3330 - accuracy: 0.8751 - val_loss: 0.2911 - val_accuracy: 0.8955 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3341 - accuracy: 0.8748 - val_loss: 0.2918 - val_accuracy: 0.8953 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3239 - accuracy: 0.8826 - val_loss: 0.2880 - val_accuracy: 0.8969 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3287 - accuracy: 0.8779 - val_loss: 0.2883 - val_accuracy: 0.8961 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3206 - accuracy: 0.8802 - val_loss: 0.2950 - val_accuracy: 0.8937 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3240 - accuracy: 0.8808 - val_loss: 0.2949 - val_accuracy: 0.8953 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3235 - accuracy: 0.8815 - val_loss: 0.2938 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3271 - accuracy: 0.8790 - val_loss: 0.2933 - val_accuracy: 0.8959 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3254 - accuracy: 0.8790 - val_loss: 0.2916 - val_accuracy: 0.8965 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3267 - accuracy: 0.8776 - val_loss: 0.2918 - val_accuracy: 0.8963 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3231 - accuracy: 0.8779 - val_loss: 0.2916 - val_accuracy: 0.8965 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3213 - accuracy: 0.8812 - val_loss: 0.2917 - val_accuracy: 0.8967 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3256 - accuracy: 0.8762 - val_loss: 0.2911 - val_accuracy: 0.8957 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3215 - accuracy: 0.8809 - val_loss: 0.2913 - val_accuracy: 0.8961 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3187 - accuracy: 0.8826 - val_loss: 0.2898 - val_accuracy: 0.8971 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3277 - accuracy: 0.8781 - val_loss: 0.2907 - val_accuracy: 0.8967 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3228 - accuracy: 0.8788 - val_loss: 0.2914 - val_accuracy: 0.8963 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3267 - accuracy: 0.8791 - val_loss: 0.2905 - val_accuracy: 0.8969 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 1s 16ms/step - loss: 0.3201 - accuracy: 0.8830 - val_loss: 0.2903 - val_accuracy: 0.8967 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3210 - accuracy: 0.8801 - val_loss: 0.2912 - val_accuracy: 0.8963 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3301 - accuracy: 0.8743 - val_loss: 0.2905 - val_accuracy: 0.8967 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3199 - accuracy: 0.8798 - val_loss: 0.2908 - val_accuracy: 0.8969 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 16ms/step - loss: 0.3240 - accuracy: 0.8808 - val_loss: 0.2909 - val_accuracy: 0.8969 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3264 - accuracy: 0.8785 - val_loss: 0.2917 - val_accuracy: 0.8961 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2802 - accuracy: 0.8994\n",
      "17/17 [==============================] - 1s 4ms/step\n",
      "TP:5736, TN:9506, FP:949, FN:755, loss0.2801694869995117, acc0.8994452968252095, sn0.8836851024495456, sp0.9092300334768053, f10.8706739526411658, auc0.960883815069403\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 10s 24ms/step - loss: 1.2915 - accuracy: 0.5885 - val_loss: 1.2489 - val_accuracy: 0.4651 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 1.1048 - accuracy: 0.6190 - val_loss: 1.2987 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.9742 - accuracy: 0.6432 - val_loss: 1.2320 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8756 - accuracy: 0.6548 - val_loss: 1.1338 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8005 - accuracy: 0.6825 - val_loss: 0.9835 - val_accuracy: 0.3981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.7312 - accuracy: 0.7184 - val_loss: 0.8429 - val_accuracy: 0.4876 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.6735 - accuracy: 0.7450 - val_loss: 0.6703 - val_accuracy: 0.7563 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.6320 - accuracy: 0.7549 - val_loss: 0.6388 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5936 - accuracy: 0.7768 - val_loss: 0.5467 - val_accuracy: 0.8205 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5676 - accuracy: 0.7826 - val_loss: 0.5155 - val_accuracy: 0.8167 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5446 - accuracy: 0.7942 - val_loss: 0.4855 - val_accuracy: 0.8335 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5226 - accuracy: 0.8015 - val_loss: 0.4614 - val_accuracy: 0.8394 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5026 - accuracy: 0.8092 - val_loss: 0.4486 - val_accuracy: 0.8404 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4807 - accuracy: 0.8204 - val_loss: 0.4220 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4614 - accuracy: 0.8300 - val_loss: 0.4029 - val_accuracy: 0.8578 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4470 - accuracy: 0.8337 - val_loss: 0.3932 - val_accuracy: 0.8650 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4401 - accuracy: 0.8372 - val_loss: 0.3808 - val_accuracy: 0.8670 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4349 - accuracy: 0.8375 - val_loss: 0.3710 - val_accuracy: 0.8718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4189 - accuracy: 0.8462 - val_loss: 0.3681 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4141 - accuracy: 0.8481 - val_loss: 0.3550 - val_accuracy: 0.8753 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4072 - accuracy: 0.8507 - val_loss: 0.3568 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3997 - accuracy: 0.8523 - val_loss: 0.3396 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3953 - accuracy: 0.8558 - val_loss: 0.3435 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3847 - accuracy: 0.8593 - val_loss: 0.3374 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3885 - accuracy: 0.8573 - val_loss: 0.3498 - val_accuracy: 0.8785 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3857 - accuracy: 0.8581 - val_loss: 0.3330 - val_accuracy: 0.8837 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3810 - accuracy: 0.8637 - val_loss: 0.3298 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3827 - accuracy: 0.8614 - val_loss: 0.3346 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3785 - accuracy: 0.8593 - val_loss: 0.3258 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3794 - accuracy: 0.8561 - val_loss: 0.3305 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3696 - accuracy: 0.8632 - val_loss: 0.3360 - val_accuracy: 0.8783 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3746 - accuracy: 0.8619 - val_loss: 0.3290 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3737 - accuracy: 0.8630 - val_loss: 0.3290 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3677 - accuracy: 0.8648 - val_loss: 0.3181 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3747 - accuracy: 0.8605 - val_loss: 0.3282 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3625 - accuracy: 0.8643 - val_loss: 0.3027 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3593 - accuracy: 0.8639 - val_loss: 0.3122 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3618 - accuracy: 0.8689 - val_loss: 0.3086 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3599 - accuracy: 0.8656 - val_loss: 0.3309 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3615 - accuracy: 0.8661 - val_loss: 0.3052 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3589 - accuracy: 0.8664 - val_loss: 0.3049 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3551 - accuracy: 0.8673 - val_loss: 0.3058 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3535 - accuracy: 0.8693 - val_loss: 0.3068 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3643 - accuracy: 0.8632 - val_loss: 0.3034 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3564 - accuracy: 0.8698 - val_loss: 0.3067 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3525 - accuracy: 0.8668 - val_loss: 0.2950 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3596 - accuracy: 0.8649 - val_loss: 0.3119 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3506 - accuracy: 0.8715 - val_loss: 0.2982 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3534 - accuracy: 0.8691 - val_loss: 0.3218 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3517 - accuracy: 0.8711 - val_loss: 0.3189 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3547 - accuracy: 0.8685 - val_loss: 0.3226 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3499 - accuracy: 0.8699 - val_loss: 0.2938 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3499 - accuracy: 0.8723 - val_loss: 0.2976 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3516 - accuracy: 0.8709 - val_loss: 0.2999 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3500 - accuracy: 0.8702 - val_loss: 0.3284 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3471 - accuracy: 0.8756 - val_loss: 0.3052 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3535 - accuracy: 0.8670 - val_loss: 0.2989 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3490 - accuracy: 0.8715 - val_loss: 0.3112 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3469 - accuracy: 0.8690 - val_loss: 0.2937 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3492 - accuracy: 0.8684 - val_loss: 0.3041 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3471 - accuracy: 0.8678 - val_loss: 0.2953 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3445 - accuracy: 0.8718 - val_loss: 0.3024 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3430 - accuracy: 0.8702 - val_loss: 0.2830 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3430 - accuracy: 0.8726 - val_loss: 0.2995 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3450 - accuracy: 0.8704 - val_loss: 0.2829 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3389 - accuracy: 0.8741 - val_loss: 0.2871 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3449 - accuracy: 0.8741 - val_loss: 0.3554 - val_accuracy: 0.8634 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3431 - accuracy: 0.8746 - val_loss: 0.2890 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3397 - accuracy: 0.8730 - val_loss: 0.2856 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3353 - accuracy: 0.8737 - val_loss: 0.3009 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3445 - accuracy: 0.8689 - val_loss: 0.2870 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3371 - accuracy: 0.8764 - val_loss: 0.2934 - val_accuracy: 0.8955 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3266 - accuracy: 0.8767 - val_loss: 0.2963 - val_accuracy: 0.8915 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3341 - accuracy: 0.8752 - val_loss: 0.2965 - val_accuracy: 0.8951 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3357 - accuracy: 0.8720 - val_loss: 0.2932 - val_accuracy: 0.8963 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3277 - accuracy: 0.8773 - val_loss: 0.2935 - val_accuracy: 0.8945 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3307 - accuracy: 0.8806 - val_loss: 0.2908 - val_accuracy: 0.8959 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3334 - accuracy: 0.8773 - val_loss: 0.2897 - val_accuracy: 0.8957 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3289 - accuracy: 0.8782 - val_loss: 0.2900 - val_accuracy: 0.8965 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3342 - accuracy: 0.8771 - val_loss: 0.2929 - val_accuracy: 0.8951 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3348 - accuracy: 0.8761 - val_loss: 0.2896 - val_accuracy: 0.8961 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3279 - accuracy: 0.8801 - val_loss: 0.2912 - val_accuracy: 0.8959 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3261 - accuracy: 0.8808 - val_loss: 0.2912 - val_accuracy: 0.8945 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3218 - accuracy: 0.8819 - val_loss: 0.2918 - val_accuracy: 0.8947 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3260 - accuracy: 0.8757 - val_loss: 0.2918 - val_accuracy: 0.8943 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3265 - accuracy: 0.8784 - val_loss: 0.2921 - val_accuracy: 0.8941 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3253 - accuracy: 0.8776 - val_loss: 0.2924 - val_accuracy: 0.8941 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3237 - accuracy: 0.8799 - val_loss: 0.2918 - val_accuracy: 0.8945 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3288 - accuracy: 0.8776 - val_loss: 0.2918 - val_accuracy: 0.8941 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3198 - accuracy: 0.8818 - val_loss: 0.2903 - val_accuracy: 0.8953 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3289 - accuracy: 0.8772 - val_loss: 0.2907 - val_accuracy: 0.8949 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3254 - accuracy: 0.8791 - val_loss: 0.2907 - val_accuracy: 0.8949 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3288 - accuracy: 0.8786 - val_loss: 0.2912 - val_accuracy: 0.8947 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3274 - accuracy: 0.8809 - val_loss: 0.2917 - val_accuracy: 0.8943 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3297 - accuracy: 0.8806 - val_loss: 0.2919 - val_accuracy: 0.8943 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3299 - accuracy: 0.8749 - val_loss: 0.2919 - val_accuracy: 0.8943 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3330 - accuracy: 0.8763 - val_loss: 0.2924 - val_accuracy: 0.8943 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3216 - accuracy: 0.8814 - val_loss: 0.2916 - val_accuracy: 0.8945 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3255 - accuracy: 0.8785 - val_loss: 0.2909 - val_accuracy: 0.8951 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3220 - accuracy: 0.8826 - val_loss: 0.2919 - val_accuracy: 0.8943 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2830 - accuracy: 0.8978\n",
      "17/17 [==============================] - 1s 5ms/step\n",
      "TP:5753, TN:9461, FP:994, FN:738, loss0.2830077111721039, acc0.8977929894960462, sn0.8863041133877677, sp0.9049258727881396, f10.8691645263634991, auc0.961099726723114\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 11s 25ms/step - loss: 1.2982 - accuracy: 0.5890 - val_loss: 1.2701 - val_accuracy: 0.3957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 1.1182 - accuracy: 0.6155 - val_loss: 1.2524 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.9918 - accuracy: 0.6424 - val_loss: 1.1982 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8935 - accuracy: 0.6668 - val_loss: 1.1429 - val_accuracy: 0.3885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8027 - accuracy: 0.7094 - val_loss: 1.0232 - val_accuracy: 0.4053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.7302 - accuracy: 0.7383 - val_loss: 0.8405 - val_accuracy: 0.5423 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.6703 - accuracy: 0.7600 - val_loss: 0.7052 - val_accuracy: 0.7058 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.6308 - accuracy: 0.7704 - val_loss: 0.6016 - val_accuracy: 0.8039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5877 - accuracy: 0.7887 - val_loss: 0.5675 - val_accuracy: 0.8077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5643 - accuracy: 0.7897 - val_loss: 0.5123 - val_accuracy: 0.8367 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5361 - accuracy: 0.8019 - val_loss: 0.4918 - val_accuracy: 0.8430 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5157 - accuracy: 0.8095 - val_loss: 0.4748 - val_accuracy: 0.8464 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4933 - accuracy: 0.8202 - val_loss: 0.4399 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4722 - accuracy: 0.8290 - val_loss: 0.4034 - val_accuracy: 0.8674 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4508 - accuracy: 0.8377 - val_loss: 0.3858 - val_accuracy: 0.8742 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4404 - accuracy: 0.8394 - val_loss: 0.3992 - val_accuracy: 0.8664 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4303 - accuracy: 0.8445 - val_loss: 0.3617 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4253 - accuracy: 0.8441 - val_loss: 0.3568 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4191 - accuracy: 0.8470 - val_loss: 0.3514 - val_accuracy: 0.8829 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4128 - accuracy: 0.8492 - val_loss: 0.3481 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4078 - accuracy: 0.8478 - val_loss: 0.3330 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.4009 - accuracy: 0.8527 - val_loss: 0.3337 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3972 - accuracy: 0.8534 - val_loss: 0.3293 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3934 - accuracy: 0.8582 - val_loss: 0.3275 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3848 - accuracy: 0.8574 - val_loss: 0.3310 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3889 - accuracy: 0.8561 - val_loss: 0.3502 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3872 - accuracy: 0.8578 - val_loss: 0.3254 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3791 - accuracy: 0.8633 - val_loss: 0.3204 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3829 - accuracy: 0.8601 - val_loss: 0.3196 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3718 - accuracy: 0.8629 - val_loss: 0.3143 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3705 - accuracy: 0.8667 - val_loss: 0.3228 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3707 - accuracy: 0.8633 - val_loss: 0.3195 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3695 - accuracy: 0.8609 - val_loss: 0.3439 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3725 - accuracy: 0.8623 - val_loss: 0.3183 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3680 - accuracy: 0.8638 - val_loss: 0.3018 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3710 - accuracy: 0.8640 - val_loss: 0.3295 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3697 - accuracy: 0.8628 - val_loss: 0.3159 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3641 - accuracy: 0.8630 - val_loss: 0.3075 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3643 - accuracy: 0.8644 - val_loss: 0.3057 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3610 - accuracy: 0.8697 - val_loss: 0.3072 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3561 - accuracy: 0.8666 - val_loss: 0.3033 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3583 - accuracy: 0.8699 - val_loss: 0.2929 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3583 - accuracy: 0.8697 - val_loss: 0.2939 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3641 - accuracy: 0.8640 - val_loss: 0.3152 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3558 - accuracy: 0.8697 - val_loss: 0.2964 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3578 - accuracy: 0.8669 - val_loss: 0.3056 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3577 - accuracy: 0.8662 - val_loss: 0.2988 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3539 - accuracy: 0.8663 - val_loss: 0.3073 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3547 - accuracy: 0.8692 - val_loss: 0.3142 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3533 - accuracy: 0.8685 - val_loss: 0.3162 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3543 - accuracy: 0.8679 - val_loss: 0.3064 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3517 - accuracy: 0.8685 - val_loss: 0.2973 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3507 - accuracy: 0.8710 - val_loss: 0.3171 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3487 - accuracy: 0.8695 - val_loss: 0.3033 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3485 - accuracy: 0.8728 - val_loss: 0.3092 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3461 - accuracy: 0.8719 - val_loss: 0.2826 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3529 - accuracy: 0.8696 - val_loss: 0.3055 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3505 - accuracy: 0.8708 - val_loss: 0.3058 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3534 - accuracy: 0.8673 - val_loss: 0.2968 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3423 - accuracy: 0.8758 - val_loss: 0.2860 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3464 - accuracy: 0.8702 - val_loss: 0.2944 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3437 - accuracy: 0.8735 - val_loss: 0.2892 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3390 - accuracy: 0.8738 - val_loss: 0.2897 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3413 - accuracy: 0.8737 - val_loss: 0.2934 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3395 - accuracy: 0.8739 - val_loss: 0.3001 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3441 - accuracy: 0.8743 - val_loss: 0.2810 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3532 - accuracy: 0.8668 - val_loss: 0.3084 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3421 - accuracy: 0.8729 - val_loss: 0.3020 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3410 - accuracy: 0.8706 - val_loss: 0.3072 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3461 - accuracy: 0.8702 - val_loss: 0.2848 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3406 - accuracy: 0.8741 - val_loss: 0.3072 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3370 - accuracy: 0.8769 - val_loss: 0.2985 - val_accuracy: 0.8959 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3384 - accuracy: 0.8764 - val_loss: 0.2937 - val_accuracy: 0.8969 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3336 - accuracy: 0.8797 - val_loss: 0.2924 - val_accuracy: 0.8991 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3354 - accuracy: 0.8755 - val_loss: 0.2900 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3287 - accuracy: 0.8785 - val_loss: 0.2891 - val_accuracy: 0.8987 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3238 - accuracy: 0.8813 - val_loss: 0.2906 - val_accuracy: 0.8977 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3268 - accuracy: 0.8809 - val_loss: 0.2841 - val_accuracy: 0.9011 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3242 - accuracy: 0.8766 - val_loss: 0.2857 - val_accuracy: 0.9009 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3297 - accuracy: 0.8794 - val_loss: 0.2871 - val_accuracy: 0.8987 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3214 - accuracy: 0.8779 - val_loss: 0.2831 - val_accuracy: 0.9013 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3287 - accuracy: 0.8755 - val_loss: 0.2841 - val_accuracy: 0.9003 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3231 - accuracy: 0.8800 - val_loss: 0.2847 - val_accuracy: 0.9009 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3258 - accuracy: 0.8793 - val_loss: 0.2857 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3230 - accuracy: 0.8803 - val_loss: 0.2869 - val_accuracy: 0.8991 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3285 - accuracy: 0.8779 - val_loss: 0.2869 - val_accuracy: 0.8993 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3243 - accuracy: 0.8797 - val_loss: 0.2861 - val_accuracy: 0.9003 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3268 - accuracy: 0.8794 - val_loss: 0.2855 - val_accuracy: 0.9007 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3257 - accuracy: 0.8807 - val_loss: 0.2864 - val_accuracy: 0.8995 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3262 - accuracy: 0.8780 - val_loss: 0.2866 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3265 - accuracy: 0.8774 - val_loss: 0.2872 - val_accuracy: 0.8983 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3300 - accuracy: 0.8763 - val_loss: 0.2878 - val_accuracy: 0.8979 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3264 - accuracy: 0.8797 - val_loss: 0.2876 - val_accuracy: 0.8981 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3253 - accuracy: 0.8820 - val_loss: 0.2879 - val_accuracy: 0.8975 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3268 - accuracy: 0.8787 - val_loss: 0.2875 - val_accuracy: 0.8979 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3326 - accuracy: 0.8745 - val_loss: 0.2872 - val_accuracy: 0.8981 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3266 - accuracy: 0.8800 - val_loss: 0.2872 - val_accuracy: 0.8985 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3182 - accuracy: 0.8820 - val_loss: 0.2888 - val_accuracy: 0.8973 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3266 - accuracy: 0.8794 - val_loss: 0.2868 - val_accuracy: 0.8993 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3239 - accuracy: 0.8789 - val_loss: 0.2868 - val_accuracy: 0.8993 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2739 - accuracy: 0.9014\n",
      "17/17 [==============================] - 1s 5ms/step\n",
      "TP:5742, TN:9533, FP:922, FN:749, loss0.27388301491737366, acc0.9013926590345804, sn0.884609459251271, sp0.9118125298900048, f10.872976054732041, auc0.9627714156105195\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 11s 24ms/step - loss: 1.2808 - accuracy: 0.5920 - val_loss: 1.2550 - val_accuracy: 0.4597 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 1.0951 - accuracy: 0.6207 - val_loss: 1.2278 - val_accuracy: 0.3881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.9719 - accuracy: 0.6444 - val_loss: 1.1476 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8693 - accuracy: 0.6742 - val_loss: 1.0149 - val_accuracy: 0.3937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.7920 - accuracy: 0.7036 - val_loss: 0.8850 - val_accuracy: 0.4805 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.7199 - accuracy: 0.7296 - val_loss: 0.8371 - val_accuracy: 0.5604 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.6756 - accuracy: 0.7463 - val_loss: 0.7086 - val_accuracy: 0.6911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.6279 - accuracy: 0.7601 - val_loss: 0.6560 - val_accuracy: 0.7274 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5987 - accuracy: 0.7688 - val_loss: 0.6309 - val_accuracy: 0.7367 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5692 - accuracy: 0.7798 - val_loss: 0.6528 - val_accuracy: 0.7096 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5496 - accuracy: 0.7847 - val_loss: 0.6009 - val_accuracy: 0.7421 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5333 - accuracy: 0.7931 - val_loss: 0.5181 - val_accuracy: 0.8039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5077 - accuracy: 0.8020 - val_loss: 0.4669 - val_accuracy: 0.8355 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4941 - accuracy: 0.8078 - val_loss: 0.4557 - val_accuracy: 0.8313 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4821 - accuracy: 0.8103 - val_loss: 0.4116 - val_accuracy: 0.8572 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4640 - accuracy: 0.8190 - val_loss: 0.4338 - val_accuracy: 0.8430 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4495 - accuracy: 0.8286 - val_loss: 0.3804 - val_accuracy: 0.8664 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4419 - accuracy: 0.8326 - val_loss: 0.4113 - val_accuracy: 0.8606 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4286 - accuracy: 0.8380 - val_loss: 0.3673 - val_accuracy: 0.8738 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4174 - accuracy: 0.8435 - val_loss: 0.3536 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4122 - accuracy: 0.8460 - val_loss: 0.3459 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4093 - accuracy: 0.8484 - val_loss: 0.3379 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4069 - accuracy: 0.8493 - val_loss: 0.3520 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3922 - accuracy: 0.8553 - val_loss: 0.3338 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3960 - accuracy: 0.8538 - val_loss: 0.3310 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3944 - accuracy: 0.8510 - val_loss: 0.3287 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3907 - accuracy: 0.8533 - val_loss: 0.3290 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3961 - accuracy: 0.8531 - val_loss: 0.3258 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3805 - accuracy: 0.8607 - val_loss: 0.3433 - val_accuracy: 0.8811 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3830 - accuracy: 0.8584 - val_loss: 0.3222 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3752 - accuracy: 0.8596 - val_loss: 0.3418 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3783 - accuracy: 0.8595 - val_loss: 0.3328 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3716 - accuracy: 0.8608 - val_loss: 0.3235 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3666 - accuracy: 0.8645 - val_loss: 0.3262 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3660 - accuracy: 0.8639 - val_loss: 0.3211 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3692 - accuracy: 0.8622 - val_loss: 0.3341 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3600 - accuracy: 0.8667 - val_loss: 0.3450 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3644 - accuracy: 0.8626 - val_loss: 0.3145 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3636 - accuracy: 0.8674 - val_loss: 0.3162 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3642 - accuracy: 0.8652 - val_loss: 0.3089 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3611 - accuracy: 0.8649 - val_loss: 0.3155 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3568 - accuracy: 0.8663 - val_loss: 0.3058 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3544 - accuracy: 0.8669 - val_loss: 0.3118 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3528 - accuracy: 0.8687 - val_loss: 0.3249 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3564 - accuracy: 0.8675 - val_loss: 0.3019 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3551 - accuracy: 0.8684 - val_loss: 0.3218 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3558 - accuracy: 0.8679 - val_loss: 0.3022 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3568 - accuracy: 0.8669 - val_loss: 0.3115 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3494 - accuracy: 0.8713 - val_loss: 0.3293 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3511 - accuracy: 0.8686 - val_loss: 0.3001 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3465 - accuracy: 0.8697 - val_loss: 0.3073 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3478 - accuracy: 0.8696 - val_loss: 0.3206 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3479 - accuracy: 0.8657 - val_loss: 0.2824 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3482 - accuracy: 0.8690 - val_loss: 0.2984 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3491 - accuracy: 0.8697 - val_loss: 0.3167 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3519 - accuracy: 0.8678 - val_loss: 0.2997 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3437 - accuracy: 0.8708 - val_loss: 0.2978 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3506 - accuracy: 0.8693 - val_loss: 0.3035 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3445 - accuracy: 0.8726 - val_loss: 0.3139 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3419 - accuracy: 0.8737 - val_loss: 0.2886 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3381 - accuracy: 0.8755 - val_loss: 0.3098 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3464 - accuracy: 0.8702 - val_loss: 0.2860 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3347 - accuracy: 0.8779 - val_loss: 0.3004 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3402 - accuracy: 0.8748 - val_loss: 0.2903 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3371 - accuracy: 0.8734 - val_loss: 0.2995 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3431 - accuracy: 0.8738 - val_loss: 0.2895 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3441 - accuracy: 0.8715 - val_loss: 0.2940 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3381 - accuracy: 0.8723 - val_loss: 0.2960 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3420 - accuracy: 0.8746 - val_loss: 0.2963 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3401 - accuracy: 0.8725 - val_loss: 0.2949 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3387 - accuracy: 0.8724 - val_loss: 0.2860 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3349 - accuracy: 0.8743 - val_loss: 0.2876 - val_accuracy: 0.9001 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3277 - accuracy: 0.8815 - val_loss: 0.2855 - val_accuracy: 0.9009 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3352 - accuracy: 0.8761 - val_loss: 0.2885 - val_accuracy: 0.9001 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3304 - accuracy: 0.8785 - val_loss: 0.2852 - val_accuracy: 0.8995 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3298 - accuracy: 0.8764 - val_loss: 0.2912 - val_accuracy: 0.8991 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3286 - accuracy: 0.8803 - val_loss: 0.2875 - val_accuracy: 0.9011 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3273 - accuracy: 0.8781 - val_loss: 0.2868 - val_accuracy: 0.9009 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3273 - accuracy: 0.8798 - val_loss: 0.2861 - val_accuracy: 0.9015 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3258 - accuracy: 0.8803 - val_loss: 0.2836 - val_accuracy: 0.9015 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3233 - accuracy: 0.8804 - val_loss: 0.2844 - val_accuracy: 0.9021 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3272 - accuracy: 0.8804 - val_loss: 0.2830 - val_accuracy: 0.9017 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3252 - accuracy: 0.8774 - val_loss: 0.2831 - val_accuracy: 0.9013 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3240 - accuracy: 0.8798 - val_loss: 0.2823 - val_accuracy: 0.9017 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 17ms/step - loss: 0.3265 - accuracy: 0.8796 - val_loss: 0.2828 - val_accuracy: 0.9015 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3194 - accuracy: 0.8867 - val_loss: 0.2831 - val_accuracy: 0.9011 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3314 - accuracy: 0.8761 - val_loss: 0.2829 - val_accuracy: 0.9011 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3292 - accuracy: 0.8766 - val_loss: 0.2825 - val_accuracy: 0.9011 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3219 - accuracy: 0.8793 - val_loss: 0.2829 - val_accuracy: 0.9013 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3253 - accuracy: 0.8775 - val_loss: 0.2834 - val_accuracy: 0.9015 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3205 - accuracy: 0.8788 - val_loss: 0.2836 - val_accuracy: 0.9015 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3235 - accuracy: 0.8806 - val_loss: 0.2836 - val_accuracy: 0.9017 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3273 - accuracy: 0.8799 - val_loss: 0.2839 - val_accuracy: 0.9015 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3280 - accuracy: 0.8796 - val_loss: 0.2836 - val_accuracy: 0.9017 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3283 - accuracy: 0.8784 - val_loss: 0.2832 - val_accuracy: 0.9011 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3259 - accuracy: 0.8821 - val_loss: 0.2835 - val_accuracy: 0.9017 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3268 - accuracy: 0.8832 - val_loss: 0.2828 - val_accuracy: 0.9013 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3205 - accuracy: 0.8840 - val_loss: 0.2828 - val_accuracy: 0.9011 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3219 - accuracy: 0.8830 - val_loss: 0.2826 - val_accuracy: 0.9009 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3218 - accuracy: 0.8824 - val_loss: 0.2836 - val_accuracy: 0.9013 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2679 - accuracy: 0.9045\n",
      "17/17 [==============================] - 1s 4ms/step\n",
      "TP:5690, TN:9637, FP:818, FN:801, loss0.2678748369216919, acc0.9044612297887407, sn0.8765983669696503, sp0.9217599234815878, f10.8754519578429111, auc0.962927184393415\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 10s 25ms/step - loss: 1.2661 - accuracy: 0.5948 - val_loss: 1.3310 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 1.0833 - accuracy: 0.6283 - val_loss: 1.3201 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.9624 - accuracy: 0.6491 - val_loss: 1.1645 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8625 - accuracy: 0.6798 - val_loss: 1.0013 - val_accuracy: 0.4212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.7838 - accuracy: 0.7132 - val_loss: 0.7749 - val_accuracy: 0.7226 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.7101 - accuracy: 0.7403 - val_loss: 0.6621 - val_accuracy: 0.7956 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.6621 - accuracy: 0.7514 - val_loss: 0.6063 - val_accuracy: 0.7978 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.6258 - accuracy: 0.7660 - val_loss: 0.5608 - val_accuracy: 0.8071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5944 - accuracy: 0.7755 - val_loss: 0.5630 - val_accuracy: 0.7854 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5660 - accuracy: 0.7822 - val_loss: 0.5393 - val_accuracy: 0.7974 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5493 - accuracy: 0.7920 - val_loss: 0.5222 - val_accuracy: 0.7996 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5305 - accuracy: 0.7952 - val_loss: 0.4898 - val_accuracy: 0.8179 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5140 - accuracy: 0.8008 - val_loss: 0.4744 - val_accuracy: 0.8163 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4922 - accuracy: 0.8094 - val_loss: 0.4387 - val_accuracy: 0.8444 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4812 - accuracy: 0.8150 - val_loss: 0.4157 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4661 - accuracy: 0.8205 - val_loss: 0.4088 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4511 - accuracy: 0.8284 - val_loss: 0.3786 - val_accuracy: 0.8694 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4392 - accuracy: 0.8341 - val_loss: 0.3742 - val_accuracy: 0.8652 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4264 - accuracy: 0.8432 - val_loss: 0.3699 - val_accuracy: 0.8708 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4251 - accuracy: 0.8404 - val_loss: 0.3650 - val_accuracy: 0.8722 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4154 - accuracy: 0.8443 - val_loss: 0.3568 - val_accuracy: 0.8744 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4141 - accuracy: 0.8439 - val_loss: 0.3558 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4018 - accuracy: 0.8517 - val_loss: 0.3459 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3933 - accuracy: 0.8550 - val_loss: 0.3344 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3936 - accuracy: 0.8519 - val_loss: 0.3325 - val_accuracy: 0.8819 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3854 - accuracy: 0.8569 - val_loss: 0.3314 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3899 - accuracy: 0.8546 - val_loss: 0.3584 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3819 - accuracy: 0.8584 - val_loss: 0.3433 - val_accuracy: 0.8759 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3776 - accuracy: 0.8643 - val_loss: 0.3386 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3798 - accuracy: 0.8595 - val_loss: 0.3440 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3748 - accuracy: 0.8604 - val_loss: 0.3131 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3733 - accuracy: 0.8620 - val_loss: 0.3266 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3753 - accuracy: 0.8588 - val_loss: 0.3170 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3669 - accuracy: 0.8630 - val_loss: 0.3291 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3724 - accuracy: 0.8611 - val_loss: 0.3040 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3638 - accuracy: 0.8679 - val_loss: 0.3067 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3587 - accuracy: 0.8696 - val_loss: 0.3148 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3641 - accuracy: 0.8663 - val_loss: 0.3507 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3681 - accuracy: 0.8653 - val_loss: 0.3195 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3576 - accuracy: 0.8666 - val_loss: 0.3112 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3650 - accuracy: 0.8632 - val_loss: 0.3084 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3606 - accuracy: 0.8696 - val_loss: 0.2908 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3560 - accuracy: 0.8645 - val_loss: 0.3068 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3553 - accuracy: 0.8659 - val_loss: 0.3150 - val_accuracy: 0.8853 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3559 - accuracy: 0.8686 - val_loss: 0.2941 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3591 - accuracy: 0.8643 - val_loss: 0.3029 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3558 - accuracy: 0.8683 - val_loss: 0.2994 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3525 - accuracy: 0.8711 - val_loss: 0.2917 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3533 - accuracy: 0.8726 - val_loss: 0.2925 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3536 - accuracy: 0.8679 - val_loss: 0.2953 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3478 - accuracy: 0.8737 - val_loss: 0.2868 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3522 - accuracy: 0.8679 - val_loss: 0.3079 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3555 - accuracy: 0.8692 - val_loss: 0.3013 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3459 - accuracy: 0.8704 - val_loss: 0.2996 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3544 - accuracy: 0.8705 - val_loss: 0.3032 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3522 - accuracy: 0.8697 - val_loss: 0.3173 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3474 - accuracy: 0.8705 - val_loss: 0.2935 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3498 - accuracy: 0.8719 - val_loss: 0.2919 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3458 - accuracy: 0.8721 - val_loss: 0.2848 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3549 - accuracy: 0.8649 - val_loss: 0.2998 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3440 - accuracy: 0.8720 - val_loss: 0.2860 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3436 - accuracy: 0.8742 - val_loss: 0.2918 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3438 - accuracy: 0.8702 - val_loss: 0.2826 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3508 - accuracy: 0.8683 - val_loss: 0.2986 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3496 - accuracy: 0.8664 - val_loss: 0.2904 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3404 - accuracy: 0.8749 - val_loss: 0.2931 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3404 - accuracy: 0.8748 - val_loss: 0.2869 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3419 - accuracy: 0.8743 - val_loss: 0.2911 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3461 - accuracy: 0.8726 - val_loss: 0.3271 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3413 - accuracy: 0.8764 - val_loss: 0.3131 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3382 - accuracy: 0.8746 - val_loss: 0.2771 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3367 - accuracy: 0.8759 - val_loss: 0.2903 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3355 - accuracy: 0.8765 - val_loss: 0.2912 - val_accuracy: 0.9001 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3349 - accuracy: 0.8756 - val_loss: 0.2900 - val_accuracy: 0.8977 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3318 - accuracy: 0.8782 - val_loss: 0.2905 - val_accuracy: 0.8983 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3338 - accuracy: 0.8766 - val_loss: 0.2916 - val_accuracy: 0.8985 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3364 - accuracy: 0.8757 - val_loss: 0.2869 - val_accuracy: 0.8999 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3252 - accuracy: 0.8808 - val_loss: 0.2841 - val_accuracy: 0.9011 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3244 - accuracy: 0.8807 - val_loss: 0.2864 - val_accuracy: 0.8991 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3268 - accuracy: 0.8786 - val_loss: 0.2856 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3291 - accuracy: 0.8830 - val_loss: 0.2878 - val_accuracy: 0.8993 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3289 - accuracy: 0.8777 - val_loss: 0.2871 - val_accuracy: 0.8993 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3199 - accuracy: 0.8814 - val_loss: 0.2881 - val_accuracy: 0.9001 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3220 - accuracy: 0.8824 - val_loss: 0.2891 - val_accuracy: 0.8997 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3278 - accuracy: 0.8783 - val_loss: 0.2885 - val_accuracy: 0.8993 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3276 - accuracy: 0.8761 - val_loss: 0.2891 - val_accuracy: 0.8993 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3291 - accuracy: 0.8772 - val_loss: 0.2890 - val_accuracy: 0.8989 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3275 - accuracy: 0.8773 - val_loss: 0.2872 - val_accuracy: 0.8993 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3264 - accuracy: 0.8761 - val_loss: 0.2868 - val_accuracy: 0.8987 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3246 - accuracy: 0.8774 - val_loss: 0.2869 - val_accuracy: 0.8987 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3283 - accuracy: 0.8786 - val_loss: 0.2882 - val_accuracy: 0.8987 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3235 - accuracy: 0.8833 - val_loss: 0.2880 - val_accuracy: 0.8989 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3183 - accuracy: 0.8838 - val_loss: 0.2876 - val_accuracy: 0.8985 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3247 - accuracy: 0.8814 - val_loss: 0.2885 - val_accuracy: 0.8987 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3288 - accuracy: 0.8773 - val_loss: 0.2889 - val_accuracy: 0.8993 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3273 - accuracy: 0.8762 - val_loss: 0.2882 - val_accuracy: 0.8991 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3329 - accuracy: 0.8772 - val_loss: 0.2874 - val_accuracy: 0.8989 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3224 - accuracy: 0.8802 - val_loss: 0.2881 - val_accuracy: 0.8991 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3219 - accuracy: 0.8799 - val_loss: 0.2875 - val_accuracy: 0.8989 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3232 - accuracy: 0.8814 - val_loss: 0.2887 - val_accuracy: 0.8991 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2801 - accuracy: 0.9000\n",
      "17/17 [==============================] - 1s 5ms/step\n",
      "TP:5841, TN:9411, FP:1044, FN:650, loss0.280123770236969, acc0.900035406585625, sn0.8998613464797411, sp0.9001434720229555, f10.8733552631578948, auc0.9635613700786161\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 11s 25ms/step - loss: 1.2639 - accuracy: 0.5946 - val_loss: 1.3737 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 1.0878 - accuracy: 0.6142 - val_loss: 1.3125 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.9657 - accuracy: 0.6288 - val_loss: 1.1937 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8632 - accuracy: 0.6649 - val_loss: 0.9854 - val_accuracy: 0.4517 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.7740 - accuracy: 0.7088 - val_loss: 0.8274 - val_accuracy: 0.5903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.7013 - accuracy: 0.7400 - val_loss: 0.6653 - val_accuracy: 0.7726 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.6471 - accuracy: 0.7611 - val_loss: 0.6204 - val_accuracy: 0.7796 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.6118 - accuracy: 0.7672 - val_loss: 0.5753 - val_accuracy: 0.8014 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5810 - accuracy: 0.7772 - val_loss: 0.5442 - val_accuracy: 0.8057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5491 - accuracy: 0.7877 - val_loss: 0.5222 - val_accuracy: 0.8095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5257 - accuracy: 0.7972 - val_loss: 0.4923 - val_accuracy: 0.8187 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4977 - accuracy: 0.8111 - val_loss: 0.4387 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4810 - accuracy: 0.8170 - val_loss: 0.4195 - val_accuracy: 0.8552 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4611 - accuracy: 0.8280 - val_loss: 0.4036 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4483 - accuracy: 0.8317 - val_loss: 0.3786 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4327 - accuracy: 0.8406 - val_loss: 0.3669 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4295 - accuracy: 0.8408 - val_loss: 0.3550 - val_accuracy: 0.8765 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4185 - accuracy: 0.8469 - val_loss: 0.3525 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4145 - accuracy: 0.8462 - val_loss: 0.3530 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4008 - accuracy: 0.8542 - val_loss: 0.3399 - val_accuracy: 0.8853 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3961 - accuracy: 0.8526 - val_loss: 0.3418 - val_accuracy: 0.8819 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3916 - accuracy: 0.8578 - val_loss: 0.3483 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3962 - accuracy: 0.8540 - val_loss: 0.3273 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3810 - accuracy: 0.8643 - val_loss: 0.3240 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3869 - accuracy: 0.8556 - val_loss: 0.3485 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3758 - accuracy: 0.8614 - val_loss: 0.3352 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3777 - accuracy: 0.8587 - val_loss: 0.3158 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3727 - accuracy: 0.8641 - val_loss: 0.3148 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3757 - accuracy: 0.8602 - val_loss: 0.3154 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3781 - accuracy: 0.8608 - val_loss: 0.3404 - val_accuracy: 0.8783 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3639 - accuracy: 0.8661 - val_loss: 0.3278 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3668 - accuracy: 0.8630 - val_loss: 0.3038 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3693 - accuracy: 0.8649 - val_loss: 0.3104 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3675 - accuracy: 0.8622 - val_loss: 0.3069 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3693 - accuracy: 0.8614 - val_loss: 0.3190 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3543 - accuracy: 0.8685 - val_loss: 0.3190 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3666 - accuracy: 0.8619 - val_loss: 0.3155 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3598 - accuracy: 0.8674 - val_loss: 0.3206 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3573 - accuracy: 0.8662 - val_loss: 0.3134 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3637 - accuracy: 0.8645 - val_loss: 0.2917 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3589 - accuracy: 0.8667 - val_loss: 0.3099 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3603 - accuracy: 0.8692 - val_loss: 0.3136 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3508 - accuracy: 0.8698 - val_loss: 0.3108 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3462 - accuracy: 0.8704 - val_loss: 0.2932 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3494 - accuracy: 0.8693 - val_loss: 0.2926 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3489 - accuracy: 0.8700 - val_loss: 0.3050 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3526 - accuracy: 0.8656 - val_loss: 0.2923 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3526 - accuracy: 0.8682 - val_loss: 0.3165 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3464 - accuracy: 0.8733 - val_loss: 0.3080 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3475 - accuracy: 0.8688 - val_loss: 0.2962 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3457 - accuracy: 0.8696 - val_loss: 0.2954 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3463 - accuracy: 0.8731 - val_loss: 0.3124 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3451 - accuracy: 0.8711 - val_loss: 0.3223 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3512 - accuracy: 0.8706 - val_loss: 0.3139 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3466 - accuracy: 0.8739 - val_loss: 0.3574 - val_accuracy: 0.8672 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3425 - accuracy: 0.8749 - val_loss: 0.2864 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3392 - accuracy: 0.8782 - val_loss: 0.2875 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3411 - accuracy: 0.8734 - val_loss: 0.2999 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3424 - accuracy: 0.8742 - val_loss: 0.3134 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3405 - accuracy: 0.8737 - val_loss: 0.2969 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3397 - accuracy: 0.8720 - val_loss: 0.3524 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3480 - accuracy: 0.8746 - val_loss: 0.3037 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3347 - accuracy: 0.8770 - val_loss: 0.3071 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3391 - accuracy: 0.8751 - val_loss: 0.3118 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3408 - accuracy: 0.8740 - val_loss: 0.2904 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3396 - accuracy: 0.8720 - val_loss: 0.3031 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3471 - accuracy: 0.8699 - val_loss: 0.3607 - val_accuracy: 0.8706 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3417 - accuracy: 0.8737 - val_loss: 0.3043 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3317 - accuracy: 0.8796 - val_loss: 0.3141 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3401 - accuracy: 0.8719 - val_loss: 0.3061 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3413 - accuracy: 0.8719 - val_loss: 0.2986 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3347 - accuracy: 0.8773 - val_loss: 0.3042 - val_accuracy: 0.8927 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3319 - accuracy: 0.8807 - val_loss: 0.3077 - val_accuracy: 0.8927 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3335 - accuracy: 0.8761 - val_loss: 0.3036 - val_accuracy: 0.8955 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3283 - accuracy: 0.8761 - val_loss: 0.3074 - val_accuracy: 0.8913 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3349 - accuracy: 0.8773 - val_loss: 0.3073 - val_accuracy: 0.8919 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3276 - accuracy: 0.8814 - val_loss: 0.3020 - val_accuracy: 0.8949 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3301 - accuracy: 0.8801 - val_loss: 0.3005 - val_accuracy: 0.8947 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3258 - accuracy: 0.8805 - val_loss: 0.2992 - val_accuracy: 0.8959 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3237 - accuracy: 0.8819 - val_loss: 0.2991 - val_accuracy: 0.8965 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3235 - accuracy: 0.8827 - val_loss: 0.2969 - val_accuracy: 0.8975 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3236 - accuracy: 0.8836 - val_loss: 0.3002 - val_accuracy: 0.8967 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3220 - accuracy: 0.8812 - val_loss: 0.3020 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3232 - accuracy: 0.8800 - val_loss: 0.3012 - val_accuracy: 0.8959 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3263 - accuracy: 0.8817 - val_loss: 0.3016 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3226 - accuracy: 0.8814 - val_loss: 0.3018 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3227 - accuracy: 0.8790 - val_loss: 0.3015 - val_accuracy: 0.8953 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3256 - accuracy: 0.8749 - val_loss: 0.3004 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3212 - accuracy: 0.8838 - val_loss: 0.3006 - val_accuracy: 0.8959 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3202 - accuracy: 0.8824 - val_loss: 0.3019 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3244 - accuracy: 0.8804 - val_loss: 0.3005 - val_accuracy: 0.8953 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3219 - accuracy: 0.8813 - val_loss: 0.2996 - val_accuracy: 0.8959 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3231 - accuracy: 0.8813 - val_loss: 0.3005 - val_accuracy: 0.8955 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3270 - accuracy: 0.8820 - val_loss: 0.3001 - val_accuracy: 0.8959 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3160 - accuracy: 0.8845 - val_loss: 0.3003 - val_accuracy: 0.8955 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3206 - accuracy: 0.8790 - val_loss: 0.3002 - val_accuracy: 0.8955 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3170 - accuracy: 0.8848 - val_loss: 0.3005 - val_accuracy: 0.8957 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3222 - accuracy: 0.8788 - val_loss: 0.3005 - val_accuracy: 0.8955 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3242 - accuracy: 0.8798 - val_loss: 0.2999 - val_accuracy: 0.8957 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3209 - accuracy: 0.8814 - val_loss: 0.3006 - val_accuracy: 0.8957 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 7ms/step - loss: 0.2861 - accuracy: 0.8986\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5825, TN:9403, FP:1052, FN:666, loss0.2861054539680481, acc0.8986191431606279, sn0.8973963950084732, sp0.8993782879005261, f10.8714841412327946, auc0.9618145891736497\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 12s 33ms/step - loss: 1.3016 - accuracy: 0.5863 - val_loss: 1.3087 - val_accuracy: 0.3885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 1.1132 - accuracy: 0.6193 - val_loss: 1.3562 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.9909 - accuracy: 0.6363 - val_loss: 1.3179 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.9018 - accuracy: 0.6525 - val_loss: 1.1433 - val_accuracy: 0.3881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.8214 - accuracy: 0.6752 - val_loss: 1.0484 - val_accuracy: 0.4473 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.7593 - accuracy: 0.7019 - val_loss: 0.9849 - val_accuracy: 0.4908 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.7073 - accuracy: 0.7211 - val_loss: 0.8205 - val_accuracy: 0.5907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6571 - accuracy: 0.7419 - val_loss: 0.7343 - val_accuracy: 0.6562 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6152 - accuracy: 0.7588 - val_loss: 0.6456 - val_accuracy: 0.7206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.5940 - accuracy: 0.7645 - val_loss: 0.5609 - val_accuracy: 0.8071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5732 - accuracy: 0.7747 - val_loss: 0.5737 - val_accuracy: 0.7661 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5453 - accuracy: 0.7872 - val_loss: 0.5437 - val_accuracy: 0.7868 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.5161 - accuracy: 0.7982 - val_loss: 0.4918 - val_accuracy: 0.8231 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4951 - accuracy: 0.8081 - val_loss: 0.4262 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4793 - accuracy: 0.8137 - val_loss: 0.4174 - val_accuracy: 0.8510 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4576 - accuracy: 0.8245 - val_loss: 0.3851 - val_accuracy: 0.8612 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4470 - accuracy: 0.8286 - val_loss: 0.3770 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4390 - accuracy: 0.8388 - val_loss: 0.3665 - val_accuracy: 0.8704 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4208 - accuracy: 0.8433 - val_loss: 0.3626 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4172 - accuracy: 0.8438 - val_loss: 0.3538 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4132 - accuracy: 0.8455 - val_loss: 0.3429 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4067 - accuracy: 0.8496 - val_loss: 0.3445 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4019 - accuracy: 0.8502 - val_loss: 0.3347 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4008 - accuracy: 0.8496 - val_loss: 0.3351 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3938 - accuracy: 0.8554 - val_loss: 0.3422 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3876 - accuracy: 0.8579 - val_loss: 0.3270 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3812 - accuracy: 0.8603 - val_loss: 0.3405 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3749 - accuracy: 0.8609 - val_loss: 0.3338 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3791 - accuracy: 0.8597 - val_loss: 0.3281 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3813 - accuracy: 0.8613 - val_loss: 0.3274 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3728 - accuracy: 0.8632 - val_loss: 0.3186 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3718 - accuracy: 0.8618 - val_loss: 0.3139 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3738 - accuracy: 0.8638 - val_loss: 0.3429 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3712 - accuracy: 0.8626 - val_loss: 0.3226 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3640 - accuracy: 0.8648 - val_loss: 0.3143 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3688 - accuracy: 0.8631 - val_loss: 0.3256 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3616 - accuracy: 0.8679 - val_loss: 0.3169 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3589 - accuracy: 0.8671 - val_loss: 0.3242 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3538 - accuracy: 0.8702 - val_loss: 0.3085 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3605 - accuracy: 0.8643 - val_loss: 0.3081 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3515 - accuracy: 0.8701 - val_loss: 0.2990 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3512 - accuracy: 0.8714 - val_loss: 0.3129 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3521 - accuracy: 0.8709 - val_loss: 0.3095 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3587 - accuracy: 0.8658 - val_loss: 0.3052 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3466 - accuracy: 0.8750 - val_loss: 0.3146 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3530 - accuracy: 0.8693 - val_loss: 0.2925 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3477 - accuracy: 0.8740 - val_loss: 0.2949 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3467 - accuracy: 0.8716 - val_loss: 0.2938 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3464 - accuracy: 0.8740 - val_loss: 0.2904 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3498 - accuracy: 0.8701 - val_loss: 0.2965 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3481 - accuracy: 0.8685 - val_loss: 0.3194 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3501 - accuracy: 0.8701 - val_loss: 0.2989 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3461 - accuracy: 0.8712 - val_loss: 0.2902 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3461 - accuracy: 0.8739 - val_loss: 0.2944 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3452 - accuracy: 0.8744 - val_loss: 0.2911 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3451 - accuracy: 0.8699 - val_loss: 0.3025 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3449 - accuracy: 0.8725 - val_loss: 0.2953 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3421 - accuracy: 0.8710 - val_loss: 0.3024 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3406 - accuracy: 0.8726 - val_loss: 0.2968 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3421 - accuracy: 0.8731 - val_loss: 0.3065 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3373 - accuracy: 0.8737 - val_loss: 0.3265 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3375 - accuracy: 0.8731 - val_loss: 0.3097 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3384 - accuracy: 0.8720 - val_loss: 0.3187 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3376 - accuracy: 0.8764 - val_loss: 0.2968 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3381 - accuracy: 0.8725 - val_loss: 0.2884 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3399 - accuracy: 0.8713 - val_loss: 0.3141 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3434 - accuracy: 0.8708 - val_loss: 0.2953 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3403 - accuracy: 0.8727 - val_loss: 0.2919 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3428 - accuracy: 0.8728 - val_loss: 0.2832 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3344 - accuracy: 0.8755 - val_loss: 0.2880 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3328 - accuracy: 0.8775 - val_loss: 0.2837 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3376 - accuracy: 0.8748 - val_loss: 0.2889 - val_accuracy: 0.8993 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3286 - accuracy: 0.8802 - val_loss: 0.2872 - val_accuracy: 0.8981 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3286 - accuracy: 0.8803 - val_loss: 0.2838 - val_accuracy: 0.9015 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3267 - accuracy: 0.8801 - val_loss: 0.2866 - val_accuracy: 0.8981 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3234 - accuracy: 0.8827 - val_loss: 0.2860 - val_accuracy: 0.8997 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3246 - accuracy: 0.8789 - val_loss: 0.2875 - val_accuracy: 0.8985 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3189 - accuracy: 0.8845 - val_loss: 0.2872 - val_accuracy: 0.8989 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3254 - accuracy: 0.8796 - val_loss: 0.2905 - val_accuracy: 0.8973 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3212 - accuracy: 0.8802 - val_loss: 0.2910 - val_accuracy: 0.8977 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3259 - accuracy: 0.8781 - val_loss: 0.2838 - val_accuracy: 0.8987 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3155 - accuracy: 0.8820 - val_loss: 0.2846 - val_accuracy: 0.8987 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3245 - accuracy: 0.8814 - val_loss: 0.2852 - val_accuracy: 0.8987 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3197 - accuracy: 0.8826 - val_loss: 0.2852 - val_accuracy: 0.8991 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3166 - accuracy: 0.8837 - val_loss: 0.2854 - val_accuracy: 0.8987 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3186 - accuracy: 0.8829 - val_loss: 0.2844 - val_accuracy: 0.8997 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3182 - accuracy: 0.8835 - val_loss: 0.2854 - val_accuracy: 0.8997 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3183 - accuracy: 0.8825 - val_loss: 0.2861 - val_accuracy: 0.8995 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3196 - accuracy: 0.8829 - val_loss: 0.2858 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3181 - accuracy: 0.8831 - val_loss: 0.2857 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3210 - accuracy: 0.8802 - val_loss: 0.2849 - val_accuracy: 0.8997 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3185 - accuracy: 0.8823 - val_loss: 0.2854 - val_accuracy: 0.8997 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3186 - accuracy: 0.8825 - val_loss: 0.2854 - val_accuracy: 0.8999 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3209 - accuracy: 0.8828 - val_loss: 0.2855 - val_accuracy: 0.9001 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3211 - accuracy: 0.8851 - val_loss: 0.2854 - val_accuracy: 0.9001 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3145 - accuracy: 0.8838 - val_loss: 0.2864 - val_accuracy: 0.8989 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3207 - accuracy: 0.8809 - val_loss: 0.2855 - val_accuracy: 0.8999 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3149 - accuracy: 0.8866 - val_loss: 0.2849 - val_accuracy: 0.8997 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3183 - accuracy: 0.8842 - val_loss: 0.2844 - val_accuracy: 0.8999 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3205 - accuracy: 0.8803 - val_loss: 0.2852 - val_accuracy: 0.8999 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2760 - accuracy: 0.9015\n",
      "17/17 [==============================] - 1s 4ms/step\n",
      "TP:5770, TN:9507, FP:948, FN:721, loss0.27597981691360474, acc0.9015106809866635, sn0.8889231243259899, sp0.909325681492109, f10.8736467559996972, auc0.9623706915383924\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 10s 25ms/step - loss: 1.2668 - accuracy: 0.5889 - val_loss: 1.2379 - val_accuracy: 0.3915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 1.0796 - accuracy: 0.6254 - val_loss: 1.2151 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.9480 - accuracy: 0.6527 - val_loss: 1.0996 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.8494 - accuracy: 0.6833 - val_loss: 0.9574 - val_accuracy: 0.4485 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.7620 - accuracy: 0.7201 - val_loss: 0.8789 - val_accuracy: 0.5120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.7035 - accuracy: 0.7428 - val_loss: 0.7022 - val_accuracy: 0.7333 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.6518 - accuracy: 0.7578 - val_loss: 0.6248 - val_accuracy: 0.7810 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.6157 - accuracy: 0.7720 - val_loss: 0.5636 - val_accuracy: 0.8165 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5825 - accuracy: 0.7819 - val_loss: 0.5492 - val_accuracy: 0.8127 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5525 - accuracy: 0.7930 - val_loss: 0.4899 - val_accuracy: 0.8305 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.5373 - accuracy: 0.7952 - val_loss: 0.4561 - val_accuracy: 0.8394 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5204 - accuracy: 0.8021 - val_loss: 0.4549 - val_accuracy: 0.8345 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.5064 - accuracy: 0.8063 - val_loss: 0.4584 - val_accuracy: 0.8357 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4921 - accuracy: 0.8091 - val_loss: 0.4236 - val_accuracy: 0.8496 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4759 - accuracy: 0.8145 - val_loss: 0.4140 - val_accuracy: 0.8542 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4651 - accuracy: 0.8196 - val_loss: 0.4113 - val_accuracy: 0.8530 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4595 - accuracy: 0.8242 - val_loss: 0.4091 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4483 - accuracy: 0.8280 - val_loss: 0.3922 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4426 - accuracy: 0.8272 - val_loss: 0.4029 - val_accuracy: 0.8534 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4320 - accuracy: 0.8358 - val_loss: 0.3725 - val_accuracy: 0.8668 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4224 - accuracy: 0.8365 - val_loss: 0.3708 - val_accuracy: 0.8638 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4225 - accuracy: 0.8396 - val_loss: 0.3686 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.4117 - accuracy: 0.8458 - val_loss: 0.3649 - val_accuracy: 0.8648 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4103 - accuracy: 0.8421 - val_loss: 0.3547 - val_accuracy: 0.8708 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.4023 - accuracy: 0.8455 - val_loss: 0.3445 - val_accuracy: 0.8706 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3936 - accuracy: 0.8514 - val_loss: 0.3459 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3926 - accuracy: 0.8480 - val_loss: 0.3401 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3914 - accuracy: 0.8534 - val_loss: 0.3533 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3864 - accuracy: 0.8520 - val_loss: 0.3594 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3808 - accuracy: 0.8550 - val_loss: 0.3300 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3835 - accuracy: 0.8569 - val_loss: 0.3344 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3748 - accuracy: 0.8618 - val_loss: 0.3314 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3707 - accuracy: 0.8602 - val_loss: 0.3159 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3786 - accuracy: 0.8576 - val_loss: 0.3260 - val_accuracy: 0.8829 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3771 - accuracy: 0.8572 - val_loss: 0.3302 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3642 - accuracy: 0.8633 - val_loss: 0.3228 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3713 - accuracy: 0.8623 - val_loss: 0.3141 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3667 - accuracy: 0.8620 - val_loss: 0.3531 - val_accuracy: 0.8777 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3668 - accuracy: 0.8644 - val_loss: 0.3263 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3616 - accuracy: 0.8673 - val_loss: 0.3090 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3677 - accuracy: 0.8671 - val_loss: 0.3237 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3702 - accuracy: 0.8618 - val_loss: 0.3258 - val_accuracy: 0.8837 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3645 - accuracy: 0.8657 - val_loss: 0.3179 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3580 - accuracy: 0.8685 - val_loss: 0.3177 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3563 - accuracy: 0.8668 - val_loss: 0.3235 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3652 - accuracy: 0.8630 - val_loss: 0.3041 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3598 - accuracy: 0.8644 - val_loss: 0.3183 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3475 - accuracy: 0.8729 - val_loss: 0.3288 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3548 - accuracy: 0.8705 - val_loss: 0.3145 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3470 - accuracy: 0.8714 - val_loss: 0.3091 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3593 - accuracy: 0.8632 - val_loss: 0.3172 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3521 - accuracy: 0.8679 - val_loss: 0.3177 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3517 - accuracy: 0.8667 - val_loss: 0.3250 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3484 - accuracy: 0.8664 - val_loss: 0.3061 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3537 - accuracy: 0.8691 - val_loss: 0.3484 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3504 - accuracy: 0.8716 - val_loss: 0.3403 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3485 - accuracy: 0.8702 - val_loss: 0.3024 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3507 - accuracy: 0.8724 - val_loss: 0.3146 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3481 - accuracy: 0.8696 - val_loss: 0.3024 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 18ms/step - loss: 0.3474 - accuracy: 0.8706 - val_loss: 0.3104 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3498 - accuracy: 0.8722 - val_loss: 0.3215 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3493 - accuracy: 0.8681 - val_loss: 0.2997 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3479 - accuracy: 0.8697 - val_loss: 0.3026 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3439 - accuracy: 0.8690 - val_loss: 0.3307 - val_accuracy: 0.8811 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3383 - accuracy: 0.8713 - val_loss: 0.3100 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3518 - accuracy: 0.8679 - val_loss: 0.3118 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3445 - accuracy: 0.8708 - val_loss: 0.2976 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3449 - accuracy: 0.8693 - val_loss: 0.3031 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3330 - accuracy: 0.8773 - val_loss: 0.3118 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3481 - accuracy: 0.8699 - val_loss: 0.3068 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3431 - accuracy: 0.8721 - val_loss: 0.3115 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3385 - accuracy: 0.8732 - val_loss: 0.3132 - val_accuracy: 0.8867 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3388 - accuracy: 0.8754 - val_loss: 0.3045 - val_accuracy: 0.8913 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3335 - accuracy: 0.8764 - val_loss: 0.3091 - val_accuracy: 0.8895 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3300 - accuracy: 0.8798 - val_loss: 0.3110 - val_accuracy: 0.8893 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3378 - accuracy: 0.8770 - val_loss: 0.3034 - val_accuracy: 0.8903 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3332 - accuracy: 0.8779 - val_loss: 0.3034 - val_accuracy: 0.8907 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3299 - accuracy: 0.8792 - val_loss: 0.3011 - val_accuracy: 0.8933 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3281 - accuracy: 0.8773 - val_loss: 0.2993 - val_accuracy: 0.8931 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3289 - accuracy: 0.8792 - val_loss: 0.3004 - val_accuracy: 0.8931 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3321 - accuracy: 0.8750 - val_loss: 0.3013 - val_accuracy: 0.8919 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3256 - accuracy: 0.8778 - val_loss: 0.3026 - val_accuracy: 0.8893 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3236 - accuracy: 0.8815 - val_loss: 0.3045 - val_accuracy: 0.8891 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3317 - accuracy: 0.8763 - val_loss: 0.3026 - val_accuracy: 0.8897 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3252 - accuracy: 0.8803 - val_loss: 0.3033 - val_accuracy: 0.8897 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3223 - accuracy: 0.8810 - val_loss: 0.3030 - val_accuracy: 0.8895 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3255 - accuracy: 0.8783 - val_loss: 0.3027 - val_accuracy: 0.8897 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3304 - accuracy: 0.8753 - val_loss: 0.3027 - val_accuracy: 0.8901 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3299 - accuracy: 0.8771 - val_loss: 0.3003 - val_accuracy: 0.8901 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3211 - accuracy: 0.8773 - val_loss: 0.3018 - val_accuracy: 0.8905 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3259 - accuracy: 0.8770 - val_loss: 0.3015 - val_accuracy: 0.8909 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3297 - accuracy: 0.8732 - val_loss: 0.3017 - val_accuracy: 0.8901 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3275 - accuracy: 0.8790 - val_loss: 0.3020 - val_accuracy: 0.8899 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3267 - accuracy: 0.8779 - val_loss: 0.3013 - val_accuracy: 0.8905 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3258 - accuracy: 0.8767 - val_loss: 0.3010 - val_accuracy: 0.8903 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3213 - accuracy: 0.8815 - val_loss: 0.3011 - val_accuracy: 0.8903 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3302 - accuracy: 0.8770 - val_loss: 0.3015 - val_accuracy: 0.8901 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3305 - accuracy: 0.8762 - val_loss: 0.3020 - val_accuracy: 0.8899 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3269 - accuracy: 0.8795 - val_loss: 0.3017 - val_accuracy: 0.8905 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3307 - accuracy: 0.8779 - val_loss: 0.3012 - val_accuracy: 0.8901 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2817 - accuracy: 0.8989\n",
      "17/17 [==============================] - 1s 12ms/step\n",
      "TP:5643, TN:9590, FP:865, FN:848, loss0.2817079424858093, acc0.8989141980408356, sn0.8693575720228008, sp0.9172644667623147, f10.8682206323563352, auc0.9580850312476953\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 12s 35ms/step - loss: 1.3097 - accuracy: 0.5810 - val_loss: 1.2612 - val_accuracy: 0.4541 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.1283 - accuracy: 0.6097 - val_loss: 1.2476 - val_accuracy: 0.3879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 1.0058 - accuracy: 0.6322 - val_loss: 1.1630 - val_accuracy: 0.3881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.9030 - accuracy: 0.6591 - val_loss: 1.0997 - val_accuracy: 0.3911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.8193 - accuracy: 0.6873 - val_loss: 0.9518 - val_accuracy: 0.4549 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.7445 - accuracy: 0.7232 - val_loss: 0.8137 - val_accuracy: 0.5750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.6849 - accuracy: 0.7419 - val_loss: 0.7007 - val_accuracy: 0.7162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.6443 - accuracy: 0.7565 - val_loss: 0.6113 - val_accuracy: 0.7908 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.6035 - accuracy: 0.7739 - val_loss: 0.5458 - val_accuracy: 0.8209 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5773 - accuracy: 0.7811 - val_loss: 0.5203 - val_accuracy: 0.8201 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5502 - accuracy: 0.7885 - val_loss: 0.4915 - val_accuracy: 0.8249 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.5307 - accuracy: 0.7952 - val_loss: 0.4651 - val_accuracy: 0.8406 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.5044 - accuracy: 0.8070 - val_loss: 0.4439 - val_accuracy: 0.8418 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4909 - accuracy: 0.8144 - val_loss: 0.4508 - val_accuracy: 0.8375 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4680 - accuracy: 0.8259 - val_loss: 0.4161 - val_accuracy: 0.8582 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4534 - accuracy: 0.8278 - val_loss: 0.3902 - val_accuracy: 0.8652 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4396 - accuracy: 0.8337 - val_loss: 0.3791 - val_accuracy: 0.8732 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4251 - accuracy: 0.8452 - val_loss: 0.3571 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4222 - accuracy: 0.8429 - val_loss: 0.3566 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4148 - accuracy: 0.8502 - val_loss: 0.3466 - val_accuracy: 0.8829 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4114 - accuracy: 0.8508 - val_loss: 0.3561 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4044 - accuracy: 0.8524 - val_loss: 0.3551 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3982 - accuracy: 0.8506 - val_loss: 0.3472 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3930 - accuracy: 0.8556 - val_loss: 0.3259 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3873 - accuracy: 0.8567 - val_loss: 0.3287 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3766 - accuracy: 0.8611 - val_loss: 0.3280 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3778 - accuracy: 0.8596 - val_loss: 0.3224 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3798 - accuracy: 0.8592 - val_loss: 0.3213 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3732 - accuracy: 0.8616 - val_loss: 0.3198 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3731 - accuracy: 0.8617 - val_loss: 0.3160 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3702 - accuracy: 0.8661 - val_loss: 0.3232 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3777 - accuracy: 0.8600 - val_loss: 0.3101 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3720 - accuracy: 0.8671 - val_loss: 0.3268 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3622 - accuracy: 0.8664 - val_loss: 0.3132 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3660 - accuracy: 0.8653 - val_loss: 0.3097 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3639 - accuracy: 0.8628 - val_loss: 0.2994 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3625 - accuracy: 0.8651 - val_loss: 0.3275 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3570 - accuracy: 0.8671 - val_loss: 0.3098 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3560 - accuracy: 0.8665 - val_loss: 0.3162 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3579 - accuracy: 0.8690 - val_loss: 0.3283 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3587 - accuracy: 0.8675 - val_loss: 0.3058 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3592 - accuracy: 0.8684 - val_loss: 0.3121 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3569 - accuracy: 0.8693 - val_loss: 0.3231 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3589 - accuracy: 0.8678 - val_loss: 0.3393 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3574 - accuracy: 0.8695 - val_loss: 0.3260 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3509 - accuracy: 0.8694 - val_loss: 0.3061 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.3553 - accuracy: 0.8698 - val_loss: 0.3084 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3505 - accuracy: 0.8678 - val_loss: 0.3104 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3476 - accuracy: 0.8707 - val_loss: 0.3165 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3475 - accuracy: 0.8743 - val_loss: 0.3115 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3484 - accuracy: 0.8715 - val_loss: 0.3060 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3445 - accuracy: 0.8737 - val_loss: 0.2919 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3496 - accuracy: 0.8686 - val_loss: 0.3177 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3501 - accuracy: 0.8708 - val_loss: 0.3274 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3491 - accuracy: 0.8723 - val_loss: 0.3320 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.3541 - accuracy: 0.8662 - val_loss: 0.3273 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3475 - accuracy: 0.8735 - val_loss: 0.2973 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3512 - accuracy: 0.8708 - val_loss: 0.3158 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3517 - accuracy: 0.8692 - val_loss: 0.2941 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3465 - accuracy: 0.8714 - val_loss: 0.2931 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3518 - accuracy: 0.8680 - val_loss: 0.2870 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3430 - accuracy: 0.8766 - val_loss: 0.2981 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3421 - accuracy: 0.8726 - val_loss: 0.2906 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3369 - accuracy: 0.8746 - val_loss: 0.3085 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3411 - accuracy: 0.8741 - val_loss: 0.3215 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3421 - accuracy: 0.8733 - val_loss: 0.3130 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3390 - accuracy: 0.8722 - val_loss: 0.2976 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3403 - accuracy: 0.8718 - val_loss: 0.2981 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3433 - accuracy: 0.8718 - val_loss: 0.3017 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3460 - accuracy: 0.8714 - val_loss: 0.3033 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3415 - accuracy: 0.8735 - val_loss: 0.3079 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3355 - accuracy: 0.8739 - val_loss: 0.3050 - val_accuracy: 0.8919 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3338 - accuracy: 0.8779 - val_loss: 0.3018 - val_accuracy: 0.8937 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3343 - accuracy: 0.8767 - val_loss: 0.2945 - val_accuracy: 0.8957 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3314 - accuracy: 0.8784 - val_loss: 0.2951 - val_accuracy: 0.8967 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3346 - accuracy: 0.8753 - val_loss: 0.2965 - val_accuracy: 0.8957 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3336 - accuracy: 0.8776 - val_loss: 0.2949 - val_accuracy: 0.8957 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3354 - accuracy: 0.8753 - val_loss: 0.2948 - val_accuracy: 0.8965 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3279 - accuracy: 0.8817 - val_loss: 0.2928 - val_accuracy: 0.8971 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3297 - accuracy: 0.8791 - val_loss: 0.2916 - val_accuracy: 0.8965 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3299 - accuracy: 0.8808 - val_loss: 0.2905 - val_accuracy: 0.8967 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3353 - accuracy: 0.8733 - val_loss: 0.2925 - val_accuracy: 0.8953 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3255 - accuracy: 0.8776 - val_loss: 0.2926 - val_accuracy: 0.8947 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3246 - accuracy: 0.8802 - val_loss: 0.2921 - val_accuracy: 0.8953 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3292 - accuracy: 0.8767 - val_loss: 0.2928 - val_accuracy: 0.8957 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3267 - accuracy: 0.8770 - val_loss: 0.2938 - val_accuracy: 0.8953 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3228 - accuracy: 0.8808 - val_loss: 0.2934 - val_accuracy: 0.8957 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3330 - accuracy: 0.8730 - val_loss: 0.2934 - val_accuracy: 0.8957 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3280 - accuracy: 0.8789 - val_loss: 0.2933 - val_accuracy: 0.8957 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3261 - accuracy: 0.8802 - val_loss: 0.2935 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3309 - accuracy: 0.8753 - val_loss: 0.2932 - val_accuracy: 0.8955 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3292 - accuracy: 0.8749 - val_loss: 0.2936 - val_accuracy: 0.8959 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3264 - accuracy: 0.8807 - val_loss: 0.2937 - val_accuracy: 0.8961 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3323 - accuracy: 0.8781 - val_loss: 0.2938 - val_accuracy: 0.8961 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3269 - accuracy: 0.8769 - val_loss: 0.2926 - val_accuracy: 0.8961 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3290 - accuracy: 0.8785 - val_loss: 0.2924 - val_accuracy: 0.8959 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 20ms/step - loss: 0.3318 - accuracy: 0.8798 - val_loss: 0.2927 - val_accuracy: 0.8961 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3229 - accuracy: 0.8823 - val_loss: 0.2931 - val_accuracy: 0.8963 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3299 - accuracy: 0.8808 - val_loss: 0.2936 - val_accuracy: 0.8959 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 19ms/step - loss: 0.3285 - accuracy: 0.8787 - val_loss: 0.2922 - val_accuracy: 0.8963 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2787 - accuracy: 0.8996\n",
      "17/17 [==============================] - 1s 5ms/step\n",
      "TP:5765, TN:9479, FP:976, FN:726, loss0.2787197232246399, acc0.8995633187772926, sn0.8881528269912186, sp0.9066475370636059, f10.8713724304715841, auc0.9622018302795151\n",
      "Average Test loss:  0.27823496162891387\n",
      "Average Accuracy:  0.9003776702466659\n",
      "Average Sensitivity:  0.885549221999692\n",
      "Average Specificity:  0.909583931133429\n",
      "Average F1 Score:  0.8719550725491476\n",
      "Average AUC Score:  0.9617728516569424\n",
      "AUC for ROC curve 1: 0.9620\n",
      "AUC for ROC curve 2: 0.9620\n",
      "AUC for ROC curve 3: 0.9609\n",
      "AUC for ROC curve 4: 0.9609\n",
      "AUC for ROC curve 5: 0.9611\n",
      "AUC for ROC curve 6: 0.9611\n",
      "AUC for ROC curve 7: 0.9628\n",
      "AUC for ROC curve 8: 0.9628\n",
      "AUC for ROC curve 9: 0.9629\n",
      "AUC for ROC curve 10: 0.9629\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe10e8b-939b-47f7-afac-fe9ec003c4d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lxx",
   "language": "python",
   "name": "lxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
