{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 16:25:21.733961: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D,LSTM\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    \n",
    "    # CNN-1\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "    x3 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(256, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)  \n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    attention = ResidualAttentionBlock(256, 256)\n",
    "    x1 = attention(x1)\n",
    "    x2 = attention(x2)\n",
    "    x3 = attention(x3)\n",
    "    \n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    squeeze = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1,input2,input3], outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2) \n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 14s 38ms/step - loss: 7.7164 - accuracy: 0.6583 - val_loss: 6.3345 - val_accuracy: 0.6815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 5.0737 - accuracy: 0.7866 - val_loss: 4.0272 - val_accuracy: 0.7956 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 3.2163 - accuracy: 0.8236 - val_loss: 2.5491 - val_accuracy: 0.8189 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 2.0649 - accuracy: 0.8426 - val_loss: 1.8430 - val_accuracy: 0.7718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 1.4089 - accuracy: 0.8525 - val_loss: 1.4643 - val_accuracy: 0.7535 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 1.0227 - accuracy: 0.8606 - val_loss: 1.0556 - val_accuracy: 0.8034 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.8108 - accuracy: 0.8690 - val_loss: 0.7929 - val_accuracy: 0.8474 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.6904 - accuracy: 0.8761 - val_loss: 0.6672 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.6316 - accuracy: 0.8755 - val_loss: 0.5757 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5804 - accuracy: 0.8833 - val_loss: 0.5660 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5618 - accuracy: 0.8862 - val_loss: 0.5515 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5339 - accuracy: 0.8870 - val_loss: 0.5831 - val_accuracy: 0.8678 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5172 - accuracy: 0.8869 - val_loss: 0.5660 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5069 - accuracy: 0.8896 - val_loss: 0.4894 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5085 - accuracy: 0.8914 - val_loss: 0.4916 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4924 - accuracy: 0.8920 - val_loss: 0.5560 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4815 - accuracy: 0.8991 - val_loss: 0.5071 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4659 - accuracy: 0.9002 - val_loss: 0.5004 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4658 - accuracy: 0.9013 - val_loss: 0.4532 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4535 - accuracy: 0.9010 - val_loss: 0.4399 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4578 - accuracy: 0.8973 - val_loss: 0.5313 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4444 - accuracy: 0.8991 - val_loss: 0.4976 - val_accuracy: 0.8753 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4251 - accuracy: 0.9044 - val_loss: 0.4209 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4226 - accuracy: 0.9035 - val_loss: 0.5064 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4185 - accuracy: 0.9055 - val_loss: 0.4999 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4315 - accuracy: 0.9014 - val_loss: 0.4196 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4188 - accuracy: 0.9031 - val_loss: 0.4979 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4171 - accuracy: 0.9055 - val_loss: 0.4833 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4138 - accuracy: 0.9079 - val_loss: 0.4933 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4108 - accuracy: 0.9075 - val_loss: 0.4573 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4210 - accuracy: 0.9024 - val_loss: 0.4032 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4187 - accuracy: 0.9066 - val_loss: 0.4063 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4147 - accuracy: 0.9052 - val_loss: 0.4275 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4206 - accuracy: 0.9069 - val_loss: 0.4834 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4083 - accuracy: 0.9103 - val_loss: 0.4547 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4119 - accuracy: 0.9068 - val_loss: 0.3954 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4017 - accuracy: 0.9119 - val_loss: 0.4771 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4205 - accuracy: 0.9117 - val_loss: 0.4387 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4320 - accuracy: 0.9091 - val_loss: 0.4517 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4194 - accuracy: 0.9116 - val_loss: 0.4339 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4311 - accuracy: 0.9089 - val_loss: 0.5024 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4305 - accuracy: 0.9102 - val_loss: 0.4255 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4240 - accuracy: 0.9083 - val_loss: 0.4613 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4204 - accuracy: 0.9112 - val_loss: 0.4099 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4180 - accuracy: 0.9071 - val_loss: 0.4582 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4186 - accuracy: 0.9148 - val_loss: 0.5471 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4188 - accuracy: 0.9142 - val_loss: 0.4301 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4310 - accuracy: 0.9078 - val_loss: 0.3989 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4080 - accuracy: 0.9098 - val_loss: 0.3832 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4009 - accuracy: 0.9098 - val_loss: 0.4246 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4308 - accuracy: 0.9056 - val_loss: 0.4301 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3926 - accuracy: 0.9177 - val_loss: 0.4284 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3990 - accuracy: 0.9134 - val_loss: 0.4054 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4102 - accuracy: 0.9080 - val_loss: 0.3923 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4000 - accuracy: 0.9125 - val_loss: 0.4375 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3988 - accuracy: 0.9138 - val_loss: 0.3947 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4131 - accuracy: 0.9116 - val_loss: 0.4398 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4149 - accuracy: 0.9118 - val_loss: 0.4053 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4158 - accuracy: 0.9100 - val_loss: 0.3970 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4062 - accuracy: 0.9158 - val_loss: 0.5265 - val_accuracy: 0.8684 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4058 - accuracy: 0.9155 - val_loss: 0.5455 - val_accuracy: 0.8684 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4122 - accuracy: 0.9107 - val_loss: 0.5974 - val_accuracy: 0.8446 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4138 - accuracy: 0.9090 - val_loss: 0.3964 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4071 - accuracy: 0.9168 - val_loss: 0.5102 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4136 - accuracy: 0.9108 - val_loss: 0.3960 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4062 - accuracy: 0.9132 - val_loss: 0.3844 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4077 - accuracy: 0.9132 - val_loss: 0.5505 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4022 - accuracy: 0.9113 - val_loss: 0.3950 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4185 - accuracy: 0.9127 - val_loss: 0.4535 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4103 - accuracy: 0.9138 - val_loss: 0.4383 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4034 - accuracy: 0.9155 - val_loss: 0.3854 - val_accuracy: 0.9234 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3755 - accuracy: 0.9215 - val_loss: 0.3663 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3445 - accuracy: 0.9252 - val_loss: 0.3469 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3240 - accuracy: 0.9281 - val_loss: 0.3610 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3178 - accuracy: 0.9272 - val_loss: 0.3163 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3028 - accuracy: 0.9291 - val_loss: 0.3418 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2950 - accuracy: 0.9287 - val_loss: 0.3138 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2869 - accuracy: 0.9308 - val_loss: 0.3105 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2832 - accuracy: 0.9303 - val_loss: 0.2860 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2737 - accuracy: 0.9328 - val_loss: 0.2881 - val_accuracy: 0.9308 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2777 - accuracy: 0.9297 - val_loss: 0.2837 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2682 - accuracy: 0.9322 - val_loss: 0.2828 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2648 - accuracy: 0.9325 - val_loss: 0.2793 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2602 - accuracy: 0.9359 - val_loss: 0.2761 - val_accuracy: 0.9328 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2608 - accuracy: 0.9338 - val_loss: 0.2758 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2619 - accuracy: 0.9335 - val_loss: 0.2763 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2672 - accuracy: 0.9327 - val_loss: 0.2745 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2593 - accuracy: 0.9349 - val_loss: 0.2756 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2575 - accuracy: 0.9349 - val_loss: 0.2748 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2602 - accuracy: 0.9316 - val_loss: 0.2737 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2600 - accuracy: 0.9344 - val_loss: 0.2736 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2556 - accuracy: 0.9356 - val_loss: 0.2731 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2557 - accuracy: 0.9344 - val_loss: 0.2727 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2541 - accuracy: 0.9351 - val_loss: 0.2727 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2545 - accuracy: 0.9354 - val_loss: 0.2725 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2564 - accuracy: 0.9352 - val_loss: 0.2724 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2584 - accuracy: 0.9342 - val_loss: 0.2724 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2579 - accuracy: 0.9348 - val_loss: 0.2722 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2545 - accuracy: 0.9369 - val_loss: 0.2721 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2538 - accuracy: 0.9366 - val_loss: 0.2722 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 6ms/step - loss: 0.2890 - accuracy: 0.9248\n",
      "17/17 [==============================] - 2s 46ms/step\n",
      "TP:5732, TN:9940, FP:515, FN:759, loss0.2889785170555115, acc0.9248200165230733, sn0.8830688645817285, sp0.9507412721186035, f10.8999842989480294, auc0.9737612414231205\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 14s 32ms/step - loss: 7.8567 - accuracy: 0.6592 - val_loss: 6.6301 - val_accuracy: 0.6003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 5.3766 - accuracy: 0.7693 - val_loss: 4.3886 - val_accuracy: 0.7489 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 3.5009 - accuracy: 0.8202 - val_loss: 2.8069 - val_accuracy: 0.8133 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 2.2593 - accuracy: 0.8471 - val_loss: 1.9059 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 1.5282 - accuracy: 0.8554 - val_loss: 1.2992 - val_accuracy: 0.8420 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 1.0757 - accuracy: 0.8736 - val_loss: 1.0430 - val_accuracy: 0.8464 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.8382 - accuracy: 0.8749 - val_loss: 0.9386 - val_accuracy: 0.8365 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.6838 - accuracy: 0.8822 - val_loss: 0.6303 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5988 - accuracy: 0.8859 - val_loss: 0.6873 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5546 - accuracy: 0.8848 - val_loss: 0.5388 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.5302 - accuracy: 0.8896 - val_loss: 0.5489 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4985 - accuracy: 0.8911 - val_loss: 0.6023 - val_accuracy: 0.8608 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4938 - accuracy: 0.8911 - val_loss: 0.4553 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4676 - accuracy: 0.8978 - val_loss: 0.5895 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4721 - accuracy: 0.8938 - val_loss: 0.4571 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4544 - accuracy: 0.8983 - val_loss: 0.4930 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4523 - accuracy: 0.8984 - val_loss: 0.5383 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4482 - accuracy: 0.8991 - val_loss: 0.5004 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4409 - accuracy: 0.9028 - val_loss: 0.4605 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4514 - accuracy: 0.8986 - val_loss: 0.4772 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4474 - accuracy: 0.9002 - val_loss: 0.4146 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4366 - accuracy: 0.9026 - val_loss: 0.4380 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4419 - accuracy: 0.9040 - val_loss: 0.4922 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4389 - accuracy: 0.9047 - val_loss: 0.4356 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4450 - accuracy: 0.9034 - val_loss: 0.4098 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4391 - accuracy: 0.9027 - val_loss: 0.4226 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4222 - accuracy: 0.9082 - val_loss: 0.5102 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4247 - accuracy: 0.9070 - val_loss: 0.4068 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4296 - accuracy: 0.9043 - val_loss: 0.4455 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4282 - accuracy: 0.9070 - val_loss: 0.4134 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4253 - accuracy: 0.9025 - val_loss: 0.4852 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4188 - accuracy: 0.9048 - val_loss: 0.4527 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4187 - accuracy: 0.9070 - val_loss: 0.3972 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4212 - accuracy: 0.9097 - val_loss: 0.4007 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4236 - accuracy: 0.9090 - val_loss: 0.3894 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4108 - accuracy: 0.9099 - val_loss: 0.4064 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4299 - accuracy: 0.9049 - val_loss: 0.4171 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4087 - accuracy: 0.9103 - val_loss: 0.3940 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4094 - accuracy: 0.9091 - val_loss: 0.5508 - val_accuracy: 0.8662 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4212 - accuracy: 0.9038 - val_loss: 0.4321 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4161 - accuracy: 0.9118 - val_loss: 0.5357 - val_accuracy: 0.8678 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4145 - accuracy: 0.9097 - val_loss: 0.4282 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4100 - accuracy: 0.9095 - val_loss: 0.4396 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4124 - accuracy: 0.9089 - val_loss: 0.4161 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4044 - accuracy: 0.9114 - val_loss: 0.4062 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4115 - accuracy: 0.9075 - val_loss: 0.4303 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4071 - accuracy: 0.9121 - val_loss: 0.4349 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4177 - accuracy: 0.9073 - val_loss: 0.5038 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4125 - accuracy: 0.9122 - val_loss: 0.4589 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4153 - accuracy: 0.9073 - val_loss: 0.4139 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4135 - accuracy: 0.9135 - val_loss: 0.4320 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4199 - accuracy: 0.9109 - val_loss: 0.5096 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4101 - accuracy: 0.9133 - val_loss: 0.4625 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4089 - accuracy: 0.9120 - val_loss: 0.4238 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4121 - accuracy: 0.9107 - val_loss: 0.4474 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4162 - accuracy: 0.9131 - val_loss: 0.4700 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4234 - accuracy: 0.9114 - val_loss: 0.4917 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4133 - accuracy: 0.9102 - val_loss: 0.4942 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3968 - accuracy: 0.9114 - val_loss: 0.4213 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4045 - accuracy: 0.9132 - val_loss: 0.4434 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4095 - accuracy: 0.9134 - val_loss: 0.3881 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4078 - accuracy: 0.9128 - val_loss: 0.5144 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4248 - accuracy: 0.9127 - val_loss: 0.4832 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4169 - accuracy: 0.9133 - val_loss: 0.4192 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4259 - accuracy: 0.9138 - val_loss: 0.4480 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4208 - accuracy: 0.9111 - val_loss: 0.4102 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.4009 - accuracy: 0.9156 - val_loss: 0.4245 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4178 - accuracy: 0.9097 - val_loss: 0.4388 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3980 - accuracy: 0.9161 - val_loss: 0.4753 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4028 - accuracy: 0.9150 - val_loss: 0.4208 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4053 - accuracy: 0.9099 - val_loss: 0.4282 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3727 - accuracy: 0.9224 - val_loss: 0.3593 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3437 - accuracy: 0.9249 - val_loss: 0.3404 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3236 - accuracy: 0.9274 - val_loss: 0.3315 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3120 - accuracy: 0.9262 - val_loss: 0.3208 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3010 - accuracy: 0.9273 - val_loss: 0.3107 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2927 - accuracy: 0.9304 - val_loss: 0.3094 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2841 - accuracy: 0.9292 - val_loss: 0.2870 - val_accuracy: 0.9304 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2804 - accuracy: 0.9294 - val_loss: 0.2998 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2742 - accuracy: 0.9291 - val_loss: 0.3156 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2724 - accuracy: 0.9297 - val_loss: 0.2856 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2667 - accuracy: 0.9315 - val_loss: 0.2815 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2662 - accuracy: 0.9332 - val_loss: 0.2802 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2594 - accuracy: 0.9350 - val_loss: 0.2783 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2568 - accuracy: 0.9358 - val_loss: 0.2788 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2530 - accuracy: 0.9343 - val_loss: 0.2784 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2563 - accuracy: 0.9349 - val_loss: 0.2778 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2542 - accuracy: 0.9367 - val_loss: 0.2766 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2528 - accuracy: 0.9344 - val_loss: 0.2767 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2511 - accuracy: 0.9356 - val_loss: 0.2750 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2534 - accuracy: 0.9357 - val_loss: 0.2726 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2500 - accuracy: 0.9360 - val_loss: 0.2730 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2537 - accuracy: 0.9350 - val_loss: 0.2731 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2543 - accuracy: 0.9354 - val_loss: 0.2731 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2505 - accuracy: 0.9359 - val_loss: 0.2731 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2510 - accuracy: 0.9361 - val_loss: 0.2733 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2502 - accuracy: 0.9375 - val_loss: 0.2733 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2517 - accuracy: 0.9339 - val_loss: 0.2736 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2497 - accuracy: 0.9361 - val_loss: 0.2733 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.2512 - accuracy: 0.9340 - val_loss: 0.2736 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2871 - accuracy: 0.9248\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5749, TN:9922, FP:533, FN:742, loss0.28711339831352234, acc0.9247610055470318, sn0.8856878755199507, sp0.9490196078431372, f10.9001800673295233, auc0.9734090855004992\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 14s 32ms/step - loss: 7.8840 - accuracy: 0.6531 - val_loss: 6.6094 - val_accuracy: 0.6737 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 5.4102 - accuracy: 0.7772 - val_loss: 4.4116 - val_accuracy: 0.7597 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 3.5571 - accuracy: 0.8208 - val_loss: 2.8527 - val_accuracy: 0.8339 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 2.3373 - accuracy: 0.8326 - val_loss: 1.9143 - val_accuracy: 0.8215 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 1.5581 - accuracy: 0.8559 - val_loss: 1.3422 - val_accuracy: 0.8295 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 1.1076 - accuracy: 0.8675 - val_loss: 1.0160 - val_accuracy: 0.8402 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.8614 - accuracy: 0.8679 - val_loss: 0.7578 - val_accuracy: 0.8785 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.6993 - accuracy: 0.8801 - val_loss: 0.6724 - val_accuracy: 0.8783 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.6120 - accuracy: 0.8848 - val_loss: 0.5584 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.5577 - accuracy: 0.8917 - val_loss: 0.5840 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.5385 - accuracy: 0.8916 - val_loss: 0.4980 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.5210 - accuracy: 0.8916 - val_loss: 0.5438 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.5212 - accuracy: 0.8901 - val_loss: 0.4677 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4886 - accuracy: 0.8968 - val_loss: 0.6066 - val_accuracy: 0.8616 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4884 - accuracy: 0.8926 - val_loss: 0.4667 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4545 - accuracy: 0.8967 - val_loss: 0.5674 - val_accuracy: 0.8538 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4507 - accuracy: 0.8977 - val_loss: 0.4376 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4606 - accuracy: 0.8976 - val_loss: 0.4637 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4608 - accuracy: 0.8980 - val_loss: 0.4701 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4695 - accuracy: 0.8967 - val_loss: 0.5036 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4518 - accuracy: 0.8987 - val_loss: 0.4517 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4591 - accuracy: 0.9010 - val_loss: 0.4710 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4491 - accuracy: 0.9055 - val_loss: 0.6193 - val_accuracy: 0.8528 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4636 - accuracy: 0.8995 - val_loss: 0.4686 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4536 - accuracy: 0.9038 - val_loss: 0.4569 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4492 - accuracy: 0.9058 - val_loss: 0.4482 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4583 - accuracy: 0.9038 - val_loss: 0.4489 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4385 - accuracy: 0.9084 - val_loss: 0.4312 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4504 - accuracy: 0.9048 - val_loss: 0.4665 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4384 - accuracy: 0.9078 - val_loss: 0.4948 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4411 - accuracy: 0.9055 - val_loss: 0.4451 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4449 - accuracy: 0.9046 - val_loss: 0.4294 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4384 - accuracy: 0.9074 - val_loss: 0.4223 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4434 - accuracy: 0.9082 - val_loss: 0.5158 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4335 - accuracy: 0.9090 - val_loss: 0.4645 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4273 - accuracy: 0.9099 - val_loss: 0.5009 - val_accuracy: 0.8726 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4261 - accuracy: 0.9113 - val_loss: 0.5334 - val_accuracy: 0.8632 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4259 - accuracy: 0.9130 - val_loss: 0.4800 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4417 - accuracy: 0.9046 - val_loss: 0.5670 - val_accuracy: 0.8506 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4265 - accuracy: 0.9114 - val_loss: 0.5565 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4274 - accuracy: 0.9077 - val_loss: 0.4601 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4254 - accuracy: 0.9054 - val_loss: 0.4787 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4357 - accuracy: 0.9076 - val_loss: 0.4529 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4435 - accuracy: 0.9141 - val_loss: 0.4666 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4313 - accuracy: 0.9099 - val_loss: 0.4305 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4343 - accuracy: 0.9122 - val_loss: 0.4681 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4285 - accuracy: 0.9104 - val_loss: 0.4615 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4311 - accuracy: 0.9111 - val_loss: 0.4820 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4247 - accuracy: 0.9104 - val_loss: 0.5521 - val_accuracy: 0.8767 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4462 - accuracy: 0.9088 - val_loss: 0.5091 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4292 - accuracy: 0.9122 - val_loss: 0.4232 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4332 - accuracy: 0.9107 - val_loss: 0.5023 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4269 - accuracy: 0.9120 - val_loss: 0.4961 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4549 - accuracy: 0.9116 - val_loss: 0.5350 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4504 - accuracy: 0.9132 - val_loss: 0.4595 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4458 - accuracy: 0.9092 - val_loss: 0.4400 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4348 - accuracy: 0.9134 - val_loss: 0.4150 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4377 - accuracy: 0.9124 - val_loss: 0.5102 - val_accuracy: 0.8765 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4457 - accuracy: 0.9115 - val_loss: 0.4852 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4380 - accuracy: 0.9091 - val_loss: 0.5237 - val_accuracy: 0.8744 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4423 - accuracy: 0.9086 - val_loss: 0.5194 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4227 - accuracy: 0.9114 - val_loss: 0.4819 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4176 - accuracy: 0.9144 - val_loss: 0.4702 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4211 - accuracy: 0.9119 - val_loss: 0.5131 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4236 - accuracy: 0.9160 - val_loss: 0.5016 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4308 - accuracy: 0.9108 - val_loss: 0.4005 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4171 - accuracy: 0.9127 - val_loss: 0.4422 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4169 - accuracy: 0.9125 - val_loss: 0.7484 - val_accuracy: 0.7491 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4336 - accuracy: 0.9138 - val_loss: 0.5941 - val_accuracy: 0.8510 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4210 - accuracy: 0.9104 - val_loss: 0.3884 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4190 - accuracy: 0.9117 - val_loss: 0.4260 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3766 - accuracy: 0.9243 - val_loss: 0.4027 - val_accuracy: 0.9087 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3550 - accuracy: 0.9268 - val_loss: 0.3657 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3364 - accuracy: 0.9291 - val_loss: 0.3323 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3250 - accuracy: 0.9259 - val_loss: 0.3213 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3149 - accuracy: 0.9286 - val_loss: 0.3142 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3061 - accuracy: 0.9277 - val_loss: 0.3041 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2961 - accuracy: 0.9304 - val_loss: 0.3087 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2929 - accuracy: 0.9301 - val_loss: 0.2921 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2879 - accuracy: 0.9288 - val_loss: 0.2903 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2840 - accuracy: 0.9268 - val_loss: 0.2920 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2704 - accuracy: 0.9320 - val_loss: 0.2804 - val_accuracy: 0.9286 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2738 - accuracy: 0.9339 - val_loss: 0.2782 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2734 - accuracy: 0.9318 - val_loss: 0.2776 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2653 - accuracy: 0.9365 - val_loss: 0.2753 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2675 - accuracy: 0.9352 - val_loss: 0.2749 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2652 - accuracy: 0.9356 - val_loss: 0.2772 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2655 - accuracy: 0.9320 - val_loss: 0.2742 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2620 - accuracy: 0.9373 - val_loss: 0.2744 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2641 - accuracy: 0.9344 - val_loss: 0.2736 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2611 - accuracy: 0.9364 - val_loss: 0.2715 - val_accuracy: 0.9334 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2621 - accuracy: 0.9343 - val_loss: 0.2716 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2656 - accuracy: 0.9317 - val_loss: 0.2715 - val_accuracy: 0.9328 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2584 - accuracy: 0.9356 - val_loss: 0.2716 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2631 - accuracy: 0.9346 - val_loss: 0.2715 - val_accuracy: 0.9332 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2627 - accuracy: 0.9346 - val_loss: 0.2717 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2605 - accuracy: 0.9361 - val_loss: 0.2715 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2615 - accuracy: 0.9343 - val_loss: 0.2715 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2608 - accuracy: 0.9356 - val_loss: 0.2715 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.2586 - accuracy: 0.9352 - val_loss: 0.2715 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 6ms/step - loss: 0.2880 - accuracy: 0.9261\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5765, TN:9928, FP:527, FN:726, loss0.28801843523979187, acc0.9260592470199457, sn0.8881528269912186, sp0.9495934959349593, f10.9019791911131971, auc0.9740533576233023\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 14s 32ms/step - loss: 7.9161 - accuracy: 0.6619 - val_loss: 6.7533 - val_accuracy: 0.6264 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 5.4814 - accuracy: 0.7784 - val_loss: 4.4931 - val_accuracy: 0.7663 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 3.6051 - accuracy: 0.8252 - val_loss: 2.9147 - val_accuracy: 0.8109 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 2.3546 - accuracy: 0.8374 - val_loss: 1.9914 - val_accuracy: 0.7914 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 1.5820 - accuracy: 0.8483 - val_loss: 1.3000 - val_accuracy: 0.8640 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 1.1218 - accuracy: 0.8670 - val_loss: 0.9660 - val_accuracy: 0.8698 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.8712 - accuracy: 0.8682 - val_loss: 0.7539 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.7114 - accuracy: 0.8799 - val_loss: 0.6954 - val_accuracy: 0.8686 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.6297 - accuracy: 0.8832 - val_loss: 0.6026 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.5762 - accuracy: 0.8836 - val_loss: 0.6061 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.5395 - accuracy: 0.8867 - val_loss: 0.5258 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.5179 - accuracy: 0.8885 - val_loss: 0.5072 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4985 - accuracy: 0.8911 - val_loss: 0.4795 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4852 - accuracy: 0.8943 - val_loss: 0.4842 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4846 - accuracy: 0.8961 - val_loss: 0.4721 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.4746 - accuracy: 0.8917 - val_loss: 0.4445 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.4645 - accuracy: 0.8976 - val_loss: 0.4768 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.4641 - accuracy: 0.9004 - val_loss: 0.4725 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4805 - accuracy: 0.8946 - val_loss: 0.4857 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4574 - accuracy: 0.8985 - val_loss: 0.4460 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4591 - accuracy: 0.9000 - val_loss: 0.4584 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4657 - accuracy: 0.9020 - val_loss: 0.5240 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4495 - accuracy: 0.9027 - val_loss: 0.5081 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4473 - accuracy: 0.9026 - val_loss: 0.4623 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4606 - accuracy: 0.9025 - val_loss: 0.4382 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4493 - accuracy: 0.9053 - val_loss: 0.4485 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4423 - accuracy: 0.9006 - val_loss: 0.6274 - val_accuracy: 0.8289 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4449 - accuracy: 0.9032 - val_loss: 0.4412 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4348 - accuracy: 0.9054 - val_loss: 0.5072 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4463 - accuracy: 0.9056 - val_loss: 0.4793 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4592 - accuracy: 0.9079 - val_loss: 0.4603 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4511 - accuracy: 0.9074 - val_loss: 0.4844 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4511 - accuracy: 0.9067 - val_loss: 0.4253 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4315 - accuracy: 0.9099 - val_loss: 0.4133 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4238 - accuracy: 0.9109 - val_loss: 0.5504 - val_accuracy: 0.8606 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4422 - accuracy: 0.9050 - val_loss: 0.4308 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4351 - accuracy: 0.9079 - val_loss: 0.5349 - val_accuracy: 0.8670 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4238 - accuracy: 0.9067 - val_loss: 0.4559 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4310 - accuracy: 0.9079 - val_loss: 0.4200 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4288 - accuracy: 0.9118 - val_loss: 0.5103 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4363 - accuracy: 0.9068 - val_loss: 0.4313 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4418 - accuracy: 0.9098 - val_loss: 0.4525 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4394 - accuracy: 0.9100 - val_loss: 0.4927 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4340 - accuracy: 0.9116 - val_loss: 0.4623 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4324 - accuracy: 0.9085 - val_loss: 0.5016 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4381 - accuracy: 0.9045 - val_loss: 0.4868 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4410 - accuracy: 0.9076 - val_loss: 0.6513 - val_accuracy: 0.8426 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4340 - accuracy: 0.9102 - val_loss: 0.5464 - val_accuracy: 0.8726 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4318 - accuracy: 0.9108 - val_loss: 0.4127 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4387 - accuracy: 0.9107 - val_loss: 0.4620 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4341 - accuracy: 0.9127 - val_loss: 0.4608 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4408 - accuracy: 0.9112 - val_loss: 0.4494 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4326 - accuracy: 0.9091 - val_loss: 0.5047 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4248 - accuracy: 0.9113 - val_loss: 0.4135 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4267 - accuracy: 0.9125 - val_loss: 0.4650 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4345 - accuracy: 0.9128 - val_loss: 0.5175 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4351 - accuracy: 0.9131 - val_loss: 0.4327 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4343 - accuracy: 0.9101 - val_loss: 0.6252 - val_accuracy: 0.8484 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4307 - accuracy: 0.9152 - val_loss: 0.4391 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4268 - accuracy: 0.9138 - val_loss: 0.5792 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4387 - accuracy: 0.9134 - val_loss: 0.5491 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4514 - accuracy: 0.9119 - val_loss: 0.4460 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4483 - accuracy: 0.9115 - val_loss: 0.4548 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4359 - accuracy: 0.9112 - val_loss: 0.4715 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4299 - accuracy: 0.9129 - val_loss: 0.4340 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4484 - accuracy: 0.9083 - val_loss: 0.4404 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4280 - accuracy: 0.9155 - val_loss: 0.4980 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4299 - accuracy: 0.9117 - val_loss: 0.4468 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4316 - accuracy: 0.9126 - val_loss: 0.5243 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4417 - accuracy: 0.9116 - val_loss: 0.4852 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4300 - accuracy: 0.9145 - val_loss: 0.4857 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3956 - accuracy: 0.9220 - val_loss: 0.3802 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3706 - accuracy: 0.9244 - val_loss: 0.3607 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3477 - accuracy: 0.9279 - val_loss: 0.3500 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3367 - accuracy: 0.9269 - val_loss: 0.3288 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3236 - accuracy: 0.9272 - val_loss: 0.3302 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3100 - accuracy: 0.9294 - val_loss: 0.3197 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3025 - accuracy: 0.9318 - val_loss: 0.3023 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2969 - accuracy: 0.9298 - val_loss: 0.3178 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2963 - accuracy: 0.9294 - val_loss: 0.2951 - val_accuracy: 0.9300 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2837 - accuracy: 0.9314 - val_loss: 0.2961 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2806 - accuracy: 0.9322 - val_loss: 0.2898 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2748 - accuracy: 0.9340 - val_loss: 0.2866 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2717 - accuracy: 0.9350 - val_loss: 0.2873 - val_accuracy: 0.9326 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2752 - accuracy: 0.9319 - val_loss: 0.2858 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2730 - accuracy: 0.9332 - val_loss: 0.2853 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2752 - accuracy: 0.9350 - val_loss: 0.2843 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2735 - accuracy: 0.9328 - val_loss: 0.2825 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2694 - accuracy: 0.9335 - val_loss: 0.2829 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2725 - accuracy: 0.9338 - val_loss: 0.2823 - val_accuracy: 0.9324 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2660 - accuracy: 0.9335 - val_loss: 0.2815 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2669 - accuracy: 0.9336 - val_loss: 0.2820 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2714 - accuracy: 0.9326 - val_loss: 0.2823 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2675 - accuracy: 0.9343 - val_loss: 0.2820 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2735 - accuracy: 0.9291 - val_loss: 0.2822 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2652 - accuracy: 0.9353 - val_loss: 0.2820 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2638 - accuracy: 0.9367 - val_loss: 0.2818 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2654 - accuracy: 0.9361 - val_loss: 0.2818 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2683 - accuracy: 0.9318 - val_loss: 0.2813 - val_accuracy: 0.9326 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2692 - accuracy: 0.9343 - val_loss: 0.2814 - val_accuracy: 0.9324 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2957 - accuracy: 0.9248\n",
      "17/17 [==============================] - 1s 33ms/step\n",
      "TP:5816, TN:9855, FP:600, FN:675, loss0.29571518301963806, acc0.9247610055470318, sn0.8960098598058851, sp0.9426111908177905, f10.9012163942046952, auc0.974187052653783\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 44ms/step - loss: 7.8853 - accuracy: 0.6607 - val_loss: 6.6602 - val_accuracy: 0.6538 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 5.4433 - accuracy: 0.7891 - val_loss: 4.4141 - val_accuracy: 0.7898 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 3.5662 - accuracy: 0.8229 - val_loss: 2.8984 - val_accuracy: 0.7922 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.3111 - accuracy: 0.8438 - val_loss: 1.9388 - val_accuracy: 0.8010 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.5445 - accuracy: 0.8528 - val_loss: 1.3314 - val_accuracy: 0.8279 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.0851 - accuracy: 0.8677 - val_loss: 0.9817 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.8310 - accuracy: 0.8734 - val_loss: 0.7554 - val_accuracy: 0.8716 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6860 - accuracy: 0.8779 - val_loss: 0.6783 - val_accuracy: 0.8644 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5982 - accuracy: 0.8826 - val_loss: 0.6993 - val_accuracy: 0.8470 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5556 - accuracy: 0.8851 - val_loss: 0.5371 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5299 - accuracy: 0.8891 - val_loss: 0.5091 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5043 - accuracy: 0.8955 - val_loss: 0.5605 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5025 - accuracy: 0.8953 - val_loss: 0.5169 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4975 - accuracy: 0.8883 - val_loss: 0.4734 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4739 - accuracy: 0.8966 - val_loss: 0.5023 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4681 - accuracy: 0.8985 - val_loss: 0.5114 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4688 - accuracy: 0.8975 - val_loss: 0.4693 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4598 - accuracy: 0.8973 - val_loss: 0.4440 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.4623 - accuracy: 0.8973 - val_loss: 0.5084 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4498 - accuracy: 0.9022 - val_loss: 0.5045 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4567 - accuracy: 0.8993 - val_loss: 0.7387 - val_accuracy: 0.7750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4543 - accuracy: 0.9014 - val_loss: 0.4656 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4544 - accuracy: 0.9031 - val_loss: 0.5039 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4570 - accuracy: 0.9010 - val_loss: 0.5152 - val_accuracy: 0.8751 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4495 - accuracy: 0.9045 - val_loss: 0.4704 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4469 - accuracy: 0.9061 - val_loss: 0.4404 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4585 - accuracy: 0.9024 - val_loss: 0.4353 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4544 - accuracy: 0.9055 - val_loss: 0.4694 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4619 - accuracy: 0.9072 - val_loss: 0.4668 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4381 - accuracy: 0.9058 - val_loss: 0.4746 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4664 - accuracy: 0.9033 - val_loss: 0.4629 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4623 - accuracy: 0.9076 - val_loss: 0.4444 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4403 - accuracy: 0.9084 - val_loss: 0.4910 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4465 - accuracy: 0.9061 - val_loss: 0.4877 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4372 - accuracy: 0.9091 - val_loss: 0.4524 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4462 - accuracy: 0.9059 - val_loss: 0.5148 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4432 - accuracy: 0.9110 - val_loss: 0.4223 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4416 - accuracy: 0.9067 - val_loss: 0.4260 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4302 - accuracy: 0.9109 - val_loss: 0.4105 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4269 - accuracy: 0.9113 - val_loss: 0.4354 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4499 - accuracy: 0.9053 - val_loss: 0.5109 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4381 - accuracy: 0.9098 - val_loss: 0.4424 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4382 - accuracy: 0.9111 - val_loss: 0.4229 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4390 - accuracy: 0.9086 - val_loss: 0.5938 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4395 - accuracy: 0.9075 - val_loss: 0.4386 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4361 - accuracy: 0.9079 - val_loss: 0.4019 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4226 - accuracy: 0.9135 - val_loss: 0.4750 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4391 - accuracy: 0.9089 - val_loss: 0.4499 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4383 - accuracy: 0.9120 - val_loss: 0.5793 - val_accuracy: 0.8500 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4390 - accuracy: 0.9113 - val_loss: 0.4184 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4373 - accuracy: 0.9127 - val_loss: 0.4201 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4219 - accuracy: 0.9125 - val_loss: 0.5178 - val_accuracy: 0.8736 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4255 - accuracy: 0.9124 - val_loss: 0.4637 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4142 - accuracy: 0.9126 - val_loss: 0.5809 - val_accuracy: 0.8464 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4360 - accuracy: 0.9132 - val_loss: 0.4151 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4371 - accuracy: 0.9114 - val_loss: 0.4186 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4361 - accuracy: 0.9115 - val_loss: 0.4409 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4225 - accuracy: 0.9152 - val_loss: 0.5286 - val_accuracy: 0.8622 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4330 - accuracy: 0.9109 - val_loss: 0.4440 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4328 - accuracy: 0.9128 - val_loss: 0.4343 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4287 - accuracy: 0.9132 - val_loss: 0.4889 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4346 - accuracy: 0.9121 - val_loss: 0.4318 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4253 - accuracy: 0.9158 - val_loss: 0.7551 - val_accuracy: 0.7982 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4444 - accuracy: 0.9134 - val_loss: 0.4373 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4399 - accuracy: 0.9142 - val_loss: 0.4247 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4339 - accuracy: 0.9137 - val_loss: 0.4755 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4355 - accuracy: 0.9078 - val_loss: 0.4668 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4089 - accuracy: 0.9140 - val_loss: 0.4352 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4264 - accuracy: 0.9150 - val_loss: 0.4320 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4166 - accuracy: 0.9152 - val_loss: 0.4185 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4301 - accuracy: 0.9113 - val_loss: 0.5387 - val_accuracy: 0.8674 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4180 - accuracy: 0.9184 - val_loss: 0.3913 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3803 - accuracy: 0.9226 - val_loss: 0.3708 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3504 - accuracy: 0.9304 - val_loss: 0.3589 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3364 - accuracy: 0.9287 - val_loss: 0.3324 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3306 - accuracy: 0.9214 - val_loss: 0.3266 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3142 - accuracy: 0.9289 - val_loss: 0.3196 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3054 - accuracy: 0.9311 - val_loss: 0.3106 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3010 - accuracy: 0.9307 - val_loss: 0.3203 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2838 - accuracy: 0.9352 - val_loss: 0.3007 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2859 - accuracy: 0.9300 - val_loss: 0.2892 - val_accuracy: 0.9312 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2787 - accuracy: 0.9316 - val_loss: 0.2856 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2744 - accuracy: 0.9359 - val_loss: 0.2847 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2730 - accuracy: 0.9337 - val_loss: 0.2849 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2699 - accuracy: 0.9348 - val_loss: 0.2840 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2715 - accuracy: 0.9341 - val_loss: 0.2831 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2697 - accuracy: 0.9344 - val_loss: 0.2828 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2689 - accuracy: 0.9341 - val_loss: 0.2809 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2707 - accuracy: 0.9342 - val_loss: 0.2804 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2712 - accuracy: 0.9325 - val_loss: 0.2788 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2666 - accuracy: 0.9339 - val_loss: 0.2785 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2666 - accuracy: 0.9345 - val_loss: 0.2795 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2702 - accuracy: 0.9337 - val_loss: 0.2797 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2687 - accuracy: 0.9314 - val_loss: 0.2794 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2647 - accuracy: 0.9350 - val_loss: 0.2795 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2669 - accuracy: 0.9336 - val_loss: 0.2793 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2648 - accuracy: 0.9376 - val_loss: 0.2790 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2648 - accuracy: 0.9355 - val_loss: 0.2790 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2619 - accuracy: 0.9355 - val_loss: 0.2789 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2703 - accuracy: 0.9330 - val_loss: 0.2787 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2946 - accuracy: 0.9263\n",
      "17/17 [==============================] - 1s 32ms/step\n",
      "TP:5823, TN:9874, FP:581, FN:668, loss0.2946053743362427, acc0.9262952909241119, sn0.8970882760745648, sp0.9444285031085605, f10.9031407522295464, auc0.9737289117750575\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 45ms/step - loss: 7.9786 - accuracy: 0.6368 - val_loss: 6.7654 - val_accuracy: 0.6380 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 5.5803 - accuracy: 0.7644 - val_loss: 4.5622 - val_accuracy: 0.7631 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 3.7081 - accuracy: 0.8101 - val_loss: 3.1312 - val_accuracy: 0.7391 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.4291 - accuracy: 0.8409 - val_loss: 2.0411 - val_accuracy: 0.7996 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.6304 - accuracy: 0.8588 - val_loss: 1.4888 - val_accuracy: 0.7990 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.1549 - accuracy: 0.8685 - val_loss: 1.0484 - val_accuracy: 0.8464 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.8733 - accuracy: 0.8797 - val_loss: 0.8678 - val_accuracy: 0.8604 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7185 - accuracy: 0.8840 - val_loss: 0.6617 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6284 - accuracy: 0.8824 - val_loss: 0.6642 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5749 - accuracy: 0.8834 - val_loss: 0.5836 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5345 - accuracy: 0.8880 - val_loss: 0.5080 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5067 - accuracy: 0.8884 - val_loss: 0.5140 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4868 - accuracy: 0.8913 - val_loss: 0.4777 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4801 - accuracy: 0.8928 - val_loss: 0.4470 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4676 - accuracy: 0.8949 - val_loss: 0.5096 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4598 - accuracy: 0.8978 - val_loss: 0.4546 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.4608 - accuracy: 0.8982 - val_loss: 0.6288 - val_accuracy: 0.8424 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.4565 - accuracy: 0.8997 - val_loss: 0.4660 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4683 - accuracy: 0.8987 - val_loss: 0.4706 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4515 - accuracy: 0.9011 - val_loss: 0.4816 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4550 - accuracy: 0.9019 - val_loss: 0.4534 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4459 - accuracy: 0.9013 - val_loss: 0.5057 - val_accuracy: 0.8751 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4560 - accuracy: 0.8984 - val_loss: 0.4514 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4452 - accuracy: 0.9038 - val_loss: 0.4365 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4397 - accuracy: 0.9030 - val_loss: 0.4522 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4469 - accuracy: 0.8989 - val_loss: 0.4314 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4296 - accuracy: 0.9061 - val_loss: 0.5516 - val_accuracy: 0.8454 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4205 - accuracy: 0.9061 - val_loss: 0.4126 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4204 - accuracy: 0.9044 - val_loss: 0.4204 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4409 - accuracy: 0.9051 - val_loss: 0.4276 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4217 - accuracy: 0.9049 - val_loss: 0.5270 - val_accuracy: 0.8614 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4307 - accuracy: 0.9070 - val_loss: 0.4334 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4282 - accuracy: 0.9043 - val_loss: 0.4216 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4277 - accuracy: 0.9062 - val_loss: 0.4168 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4259 - accuracy: 0.9064 - val_loss: 0.4654 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4317 - accuracy: 0.9073 - val_loss: 0.5114 - val_accuracy: 0.8648 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4263 - accuracy: 0.9080 - val_loss: 0.4419 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4343 - accuracy: 0.9032 - val_loss: 0.4441 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4311 - accuracy: 0.9087 - val_loss: 0.6217 - val_accuracy: 0.8478 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4392 - accuracy: 0.9045 - val_loss: 0.5804 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4513 - accuracy: 0.9054 - val_loss: 0.5512 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4416 - accuracy: 0.9065 - val_loss: 0.4521 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4239 - accuracy: 0.9116 - val_loss: 0.5097 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4461 - accuracy: 0.9040 - val_loss: 0.5281 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4362 - accuracy: 0.9115 - val_loss: 0.4387 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4317 - accuracy: 0.9118 - val_loss: 0.4367 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4423 - accuracy: 0.9074 - val_loss: 0.4168 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4397 - accuracy: 0.9130 - val_loss: 0.5644 - val_accuracy: 0.8746 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4668 - accuracy: 0.9061 - val_loss: 0.5307 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4526 - accuracy: 0.9099 - val_loss: 0.4626 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4422 - accuracy: 0.9072 - val_loss: 0.4366 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4414 - accuracy: 0.9093 - val_loss: 0.4480 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4381 - accuracy: 0.9099 - val_loss: 0.4603 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4414 - accuracy: 0.9087 - val_loss: 0.4478 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4427 - accuracy: 0.9101 - val_loss: 0.4412 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4530 - accuracy: 0.9097 - val_loss: 0.4661 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4479 - accuracy: 0.9068 - val_loss: 0.5001 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4305 - accuracy: 0.9098 - val_loss: 0.5093 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4359 - accuracy: 0.9081 - val_loss: 0.4239 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4276 - accuracy: 0.9098 - val_loss: 0.5034 - val_accuracy: 0.8692 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4311 - accuracy: 0.9074 - val_loss: 0.5005 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4324 - accuracy: 0.9091 - val_loss: 0.4208 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4284 - accuracy: 0.9115 - val_loss: 0.5328 - val_accuracy: 0.8783 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4453 - accuracy: 0.9090 - val_loss: 0.4286 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4407 - accuracy: 0.9107 - val_loss: 0.4584 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4512 - accuracy: 0.9101 - val_loss: 0.4516 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4417 - accuracy: 0.9079 - val_loss: 0.5799 - val_accuracy: 0.8550 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4268 - accuracy: 0.9140 - val_loss: 0.4501 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4299 - accuracy: 0.9089 - val_loss: 0.4003 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4137 - accuracy: 0.9122 - val_loss: 0.4097 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4246 - accuracy: 0.9123 - val_loss: 0.5390 - val_accuracy: 0.8777 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3935 - accuracy: 0.9162 - val_loss: 0.3777 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3641 - accuracy: 0.9226 - val_loss: 0.3545 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3463 - accuracy: 0.9237 - val_loss: 0.3400 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3290 - accuracy: 0.9256 - val_loss: 0.3251 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3182 - accuracy: 0.9263 - val_loss: 0.3195 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3138 - accuracy: 0.9221 - val_loss: 0.3222 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3054 - accuracy: 0.9256 - val_loss: 0.3039 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2960 - accuracy: 0.9280 - val_loss: 0.2980 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2871 - accuracy: 0.9297 - val_loss: 0.2966 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2843 - accuracy: 0.9284 - val_loss: 0.2868 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2781 - accuracy: 0.9315 - val_loss: 0.2865 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2758 - accuracy: 0.9297 - val_loss: 0.2864 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2771 - accuracy: 0.9288 - val_loss: 0.2886 - val_accuracy: 0.9286 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2752 - accuracy: 0.9319 - val_loss: 0.2840 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2779 - accuracy: 0.9291 - val_loss: 0.2830 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2710 - accuracy: 0.9326 - val_loss: 0.2866 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2690 - accuracy: 0.9333 - val_loss: 0.2850 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2734 - accuracy: 0.9323 - val_loss: 0.2836 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2689 - accuracy: 0.9331 - val_loss: 0.2838 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2637 - accuracy: 0.9350 - val_loss: 0.2805 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2650 - accuracy: 0.9332 - val_loss: 0.2809 - val_accuracy: 0.9290 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2661 - accuracy: 0.9342 - val_loss: 0.2815 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2654 - accuracy: 0.9350 - val_loss: 0.2818 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2640 - accuracy: 0.9345 - val_loss: 0.2820 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2691 - accuracy: 0.9322 - val_loss: 0.2821 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2650 - accuracy: 0.9333 - val_loss: 0.2818 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2624 - accuracy: 0.9329 - val_loss: 0.2810 - val_accuracy: 0.9294 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2641 - accuracy: 0.9344 - val_loss: 0.2812 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2623 - accuracy: 0.9342 - val_loss: 0.2815 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2914 - accuracy: 0.9255\n",
      "17/17 [==============================] - 1s 32ms/step\n",
      "TP:5859, TN:9824, FP:631, FN:632, loss0.29139575362205505, acc0.9254691372595303, sn0.9026344168849175, sp0.9396461023433764, f10.9027039519297434, auc0.9740380106185358\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 16s 44ms/step - loss: 7.9157 - accuracy: 0.6488 - val_loss: 6.8445 - val_accuracy: 0.5293 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 5.4580 - accuracy: 0.7808 - val_loss: 4.5834 - val_accuracy: 0.6713 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 3.5896 - accuracy: 0.8214 - val_loss: 2.9204 - val_accuracy: 0.7854 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.3517 - accuracy: 0.8369 - val_loss: 1.9879 - val_accuracy: 0.7812 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.5731 - accuracy: 0.8555 - val_loss: 1.5140 - val_accuracy: 0.7772 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.1349 - accuracy: 0.8646 - val_loss: 1.1516 - val_accuracy: 0.7922 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.8738 - accuracy: 0.8721 - val_loss: 0.9441 - val_accuracy: 0.8275 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7233 - accuracy: 0.8792 - val_loss: 0.6476 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6328 - accuracy: 0.8824 - val_loss: 0.6214 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5820 - accuracy: 0.8846 - val_loss: 0.6294 - val_accuracy: 0.8714 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5458 - accuracy: 0.8901 - val_loss: 0.4925 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.5161 - accuracy: 0.8920 - val_loss: 0.5542 - val_accuracy: 0.8783 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5010 - accuracy: 0.8938 - val_loss: 0.5663 - val_accuracy: 0.8732 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4955 - accuracy: 0.8984 - val_loss: 0.5414 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.5033 - accuracy: 0.8955 - val_loss: 0.5361 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4771 - accuracy: 0.8946 - val_loss: 0.4714 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4740 - accuracy: 0.8924 - val_loss: 0.5385 - val_accuracy: 0.8722 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4640 - accuracy: 0.8989 - val_loss: 0.4761 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.4515 - accuracy: 0.8987 - val_loss: 0.6642 - val_accuracy: 0.8024 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4609 - accuracy: 0.8979 - val_loss: 0.4636 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4523 - accuracy: 0.9045 - val_loss: 0.4724 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4536 - accuracy: 0.9039 - val_loss: 0.4557 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4577 - accuracy: 0.9012 - val_loss: 0.4482 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4404 - accuracy: 0.9053 - val_loss: 0.4642 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4453 - accuracy: 0.9038 - val_loss: 0.4396 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4527 - accuracy: 0.9079 - val_loss: 0.5923 - val_accuracy: 0.8664 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4508 - accuracy: 0.9044 - val_loss: 0.4630 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4423 - accuracy: 0.9053 - val_loss: 0.4393 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4369 - accuracy: 0.9054 - val_loss: 0.4134 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4420 - accuracy: 0.9048 - val_loss: 0.5737 - val_accuracy: 0.8546 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4519 - accuracy: 0.9046 - val_loss: 0.4319 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4404 - accuracy: 0.9048 - val_loss: 0.4996 - val_accuracy: 0.8767 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4374 - accuracy: 0.9100 - val_loss: 0.4758 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4377 - accuracy: 0.9066 - val_loss: 0.4093 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4269 - accuracy: 0.9119 - val_loss: 0.4289 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4255 - accuracy: 0.9117 - val_loss: 0.4749 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4375 - accuracy: 0.9077 - val_loss: 0.4711 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4526 - accuracy: 0.9101 - val_loss: 0.6052 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4520 - accuracy: 0.9049 - val_loss: 0.4721 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4344 - accuracy: 0.9099 - val_loss: 0.4225 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4238 - accuracy: 0.9080 - val_loss: 0.4683 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4356 - accuracy: 0.9095 - val_loss: 0.4564 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4368 - accuracy: 0.9082 - val_loss: 0.5096 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4273 - accuracy: 0.9121 - val_loss: 0.4289 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4433 - accuracy: 0.9104 - val_loss: 0.4073 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4332 - accuracy: 0.9091 - val_loss: 0.4410 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4237 - accuracy: 0.9138 - val_loss: 0.4444 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4431 - accuracy: 0.9066 - val_loss: 0.6366 - val_accuracy: 0.8287 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4444 - accuracy: 0.9104 - val_loss: 0.5890 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4432 - accuracy: 0.9126 - val_loss: 0.4428 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4391 - accuracy: 0.9106 - val_loss: 0.4704 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4348 - accuracy: 0.9138 - val_loss: 0.4308 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4355 - accuracy: 0.9125 - val_loss: 0.4876 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4313 - accuracy: 0.9155 - val_loss: 0.4530 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4319 - accuracy: 0.9099 - val_loss: 0.4400 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4325 - accuracy: 0.9114 - val_loss: 0.4963 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4383 - accuracy: 0.9116 - val_loss: 0.3961 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4153 - accuracy: 0.9136 - val_loss: 0.4626 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4351 - accuracy: 0.9102 - val_loss: 0.4738 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4232 - accuracy: 0.9154 - val_loss: 0.5298 - val_accuracy: 0.8724 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4257 - accuracy: 0.9133 - val_loss: 0.4165 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4197 - accuracy: 0.9143 - val_loss: 0.4233 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4252 - accuracy: 0.9095 - val_loss: 0.4108 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4329 - accuracy: 0.9140 - val_loss: 0.4535 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4021 - accuracy: 0.9163 - val_loss: 0.4075 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4149 - accuracy: 0.9132 - val_loss: 0.4217 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4171 - accuracy: 0.9120 - val_loss: 0.4849 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4124 - accuracy: 0.9144 - val_loss: 0.3963 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4037 - accuracy: 0.9129 - val_loss: 0.4265 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4225 - accuracy: 0.9116 - val_loss: 0.5915 - val_accuracy: 0.8664 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4131 - accuracy: 0.9140 - val_loss: 0.6643 - val_accuracy: 0.8333 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3893 - accuracy: 0.9232 - val_loss: 0.3788 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3624 - accuracy: 0.9238 - val_loss: 0.3549 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3362 - accuracy: 0.9269 - val_loss: 0.3365 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3238 - accuracy: 0.9277 - val_loss: 0.3334 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3148 - accuracy: 0.9288 - val_loss: 0.3394 - val_accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2970 - accuracy: 0.9319 - val_loss: 0.3068 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2965 - accuracy: 0.9308 - val_loss: 0.3050 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2946 - accuracy: 0.9302 - val_loss: 0.2965 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2819 - accuracy: 0.9311 - val_loss: 0.2972 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2784 - accuracy: 0.9343 - val_loss: 0.2894 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2720 - accuracy: 0.9329 - val_loss: 0.2832 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2735 - accuracy: 0.9331 - val_loss: 0.2836 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2728 - accuracy: 0.9312 - val_loss: 0.2842 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2722 - accuracy: 0.9332 - val_loss: 0.2858 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2652 - accuracy: 0.9327 - val_loss: 0.2851 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2690 - accuracy: 0.9348 - val_loss: 0.2822 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2640 - accuracy: 0.9351 - val_loss: 0.2816 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2649 - accuracy: 0.9375 - val_loss: 0.2825 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2660 - accuracy: 0.9337 - val_loss: 0.2795 - val_accuracy: 0.9322 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2618 - accuracy: 0.9350 - val_loss: 0.2812 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2673 - accuracy: 0.9345 - val_loss: 0.2806 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2590 - accuracy: 0.9342 - val_loss: 0.2806 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2643 - accuracy: 0.9339 - val_loss: 0.2800 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2611 - accuracy: 0.9366 - val_loss: 0.2803 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2611 - accuracy: 0.9356 - val_loss: 0.2800 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2629 - accuracy: 0.9349 - val_loss: 0.2799 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2630 - accuracy: 0.9333 - val_loss: 0.2804 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2642 - accuracy: 0.9355 - val_loss: 0.2800 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2640 - accuracy: 0.9331 - val_loss: 0.2797 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2924 - accuracy: 0.9257\n",
      "17/17 [==============================] - 1s 33ms/step\n",
      "TP:5839, TN:9848, FP:607, FN:652, loss0.2923808991909027, acc0.9257051811636965, sn0.8995532275458327, sp0.9419416547106647, f10.902682229264899, auc0.9742894568876996\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 45ms/step - loss: 7.8049 - accuracy: 0.6613 - val_loss: 6.5479 - val_accuracy: 0.6207 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 5.2340 - accuracy: 0.7774 - val_loss: 4.2393 - val_accuracy: 0.7393 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 3.3292 - accuracy: 0.8281 - val_loss: 2.6624 - val_accuracy: 0.8057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.1345 - accuracy: 0.8468 - val_loss: 1.7103 - val_accuracy: 0.8410 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.4088 - accuracy: 0.8621 - val_loss: 1.2028 - val_accuracy: 0.8440 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.0075 - accuracy: 0.8709 - val_loss: 0.9585 - val_accuracy: 0.8406 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7788 - accuracy: 0.8803 - val_loss: 0.8214 - val_accuracy: 0.8456 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6590 - accuracy: 0.8810 - val_loss: 0.6222 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5882 - accuracy: 0.8839 - val_loss: 0.6430 - val_accuracy: 0.8646 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5488 - accuracy: 0.8852 - val_loss: 0.5108 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.5305 - accuracy: 0.8856 - val_loss: 0.5281 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.5115 - accuracy: 0.8912 - val_loss: 0.5839 - val_accuracy: 0.8594 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4911 - accuracy: 0.8923 - val_loss: 0.5974 - val_accuracy: 0.8534 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.4869 - accuracy: 0.8943 - val_loss: 0.4863 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4798 - accuracy: 0.8934 - val_loss: 0.5084 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4781 - accuracy: 0.8965 - val_loss: 0.4742 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4661 - accuracy: 0.8995 - val_loss: 0.4742 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.4684 - accuracy: 0.8972 - val_loss: 0.5716 - val_accuracy: 0.8767 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4539 - accuracy: 0.9002 - val_loss: 0.5891 - val_accuracy: 0.8634 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4677 - accuracy: 0.8980 - val_loss: 0.4667 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4578 - accuracy: 0.9029 - val_loss: 0.4742 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4585 - accuracy: 0.9012 - val_loss: 0.4595 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4518 - accuracy: 0.9058 - val_loss: 0.4442 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4677 - accuracy: 0.9003 - val_loss: 0.4304 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4346 - accuracy: 0.9044 - val_loss: 0.5194 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4411 - accuracy: 0.9046 - val_loss: 0.4430 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4407 - accuracy: 0.9074 - val_loss: 0.4614 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4436 - accuracy: 0.9066 - val_loss: 0.4414 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4439 - accuracy: 0.9030 - val_loss: 0.4244 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4309 - accuracy: 0.9067 - val_loss: 0.4649 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4393 - accuracy: 0.9028 - val_loss: 0.4482 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4388 - accuracy: 0.9086 - val_loss: 0.4703 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4380 - accuracy: 0.9093 - val_loss: 0.5989 - val_accuracy: 0.8570 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4511 - accuracy: 0.9075 - val_loss: 0.5052 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4482 - accuracy: 0.9064 - val_loss: 0.4728 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4291 - accuracy: 0.9106 - val_loss: 0.4537 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4330 - accuracy: 0.9061 - val_loss: 0.4185 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4323 - accuracy: 0.9073 - val_loss: 0.4529 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4494 - accuracy: 0.9048 - val_loss: 0.6151 - val_accuracy: 0.8391 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4412 - accuracy: 0.9096 - val_loss: 0.4184 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4175 - accuracy: 0.9134 - val_loss: 0.4729 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4332 - accuracy: 0.9117 - val_loss: 0.5793 - val_accuracy: 0.8588 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4344 - accuracy: 0.9085 - val_loss: 0.4629 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4368 - accuracy: 0.9073 - val_loss: 0.4368 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4232 - accuracy: 0.9121 - val_loss: 0.4373 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4262 - accuracy: 0.9118 - val_loss: 0.4772 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4379 - accuracy: 0.9114 - val_loss: 0.4455 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4290 - accuracy: 0.9105 - val_loss: 0.5680 - val_accuracy: 0.8708 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4475 - accuracy: 0.9122 - val_loss: 0.4287 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4267 - accuracy: 0.9136 - val_loss: 0.4172 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4231 - accuracy: 0.9132 - val_loss: 0.4596 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4287 - accuracy: 0.9143 - val_loss: 0.4738 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4281 - accuracy: 0.9107 - val_loss: 0.4126 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4242 - accuracy: 0.9105 - val_loss: 0.4350 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4322 - accuracy: 0.9114 - val_loss: 0.5443 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4256 - accuracy: 0.9118 - val_loss: 0.7033 - val_accuracy: 0.8295 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4240 - accuracy: 0.9155 - val_loss: 0.4425 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4315 - accuracy: 0.9124 - val_loss: 0.4614 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4413 - accuracy: 0.9120 - val_loss: 0.4889 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4211 - accuracy: 0.9124 - val_loss: 0.4152 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4297 - accuracy: 0.9113 - val_loss: 0.4432 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4239 - accuracy: 0.9149 - val_loss: 0.4142 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4175 - accuracy: 0.9149 - val_loss: 0.4625 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4256 - accuracy: 0.9138 - val_loss: 0.4036 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4124 - accuracy: 0.9125 - val_loss: 0.3966 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4037 - accuracy: 0.9161 - val_loss: 0.4516 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4238 - accuracy: 0.9097 - val_loss: 0.4396 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4285 - accuracy: 0.9115 - val_loss: 0.3916 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4096 - accuracy: 0.9129 - val_loss: 0.4040 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4173 - accuracy: 0.9153 - val_loss: 0.4017 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4128 - accuracy: 0.9181 - val_loss: 0.3929 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3734 - accuracy: 0.9216 - val_loss: 0.3792 - val_accuracy: 0.9146 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3483 - accuracy: 0.9272 - val_loss: 0.3418 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3302 - accuracy: 0.9252 - val_loss: 0.3351 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3163 - accuracy: 0.9291 - val_loss: 0.3170 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3102 - accuracy: 0.9283 - val_loss: 0.3084 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3047 - accuracy: 0.9266 - val_loss: 0.3058 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2952 - accuracy: 0.9289 - val_loss: 0.2960 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2898 - accuracy: 0.9291 - val_loss: 0.2958 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2795 - accuracy: 0.9313 - val_loss: 0.2962 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2761 - accuracy: 0.9317 - val_loss: 0.2857 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2686 - accuracy: 0.9344 - val_loss: 0.2819 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2658 - accuracy: 0.9348 - val_loss: 0.2797 - val_accuracy: 0.9276 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2656 - accuracy: 0.9355 - val_loss: 0.2783 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2673 - accuracy: 0.9341 - val_loss: 0.2761 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2620 - accuracy: 0.9346 - val_loss: 0.2776 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2582 - accuracy: 0.9354 - val_loss: 0.2766 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2634 - accuracy: 0.9336 - val_loss: 0.2765 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2586 - accuracy: 0.9353 - val_loss: 0.2749 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2595 - accuracy: 0.9360 - val_loss: 0.2745 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2603 - accuracy: 0.9347 - val_loss: 0.2741 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2576 - accuracy: 0.9346 - val_loss: 0.2734 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2596 - accuracy: 0.9352 - val_loss: 0.2733 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2603 - accuracy: 0.9340 - val_loss: 0.2734 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2574 - accuracy: 0.9361 - val_loss: 0.2733 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2612 - accuracy: 0.9340 - val_loss: 0.2732 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2597 - accuracy: 0.9359 - val_loss: 0.2728 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2567 - accuracy: 0.9367 - val_loss: 0.2728 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2568 - accuracy: 0.9364 - val_loss: 0.2725 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2567 - accuracy: 0.9367 - val_loss: 0.2727 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2868 - accuracy: 0.9256\n",
      "17/17 [==============================] - 1s 33ms/step\n",
      "TP:5773, TN:9912, FP:543, FN:718, loss0.28682395815849304, acc0.9255871592116134, sn0.8893853027268526, sp0.9480631276901005, f10.9015382212852346, auc0.9743083698791714\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 44ms/step - loss: 7.8991 - accuracy: 0.6487 - val_loss: 6.7906 - val_accuracy: 0.4960 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 5.4146 - accuracy: 0.7763 - val_loss: 4.4863 - val_accuracy: 0.6968 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 3.5380 - accuracy: 0.8141 - val_loss: 2.8673 - val_accuracy: 0.7976 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.3023 - accuracy: 0.8366 - val_loss: 1.8662 - val_accuracy: 0.8363 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.5442 - accuracy: 0.8581 - val_loss: 1.3164 - val_accuracy: 0.8381 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.0965 - accuracy: 0.8647 - val_loss: 0.9266 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.8400 - accuracy: 0.8783 - val_loss: 0.8154 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7065 - accuracy: 0.8790 - val_loss: 0.7513 - val_accuracy: 0.8514 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6127 - accuracy: 0.8836 - val_loss: 0.5893 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.5714 - accuracy: 0.8836 - val_loss: 0.5219 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.5312 - accuracy: 0.8898 - val_loss: 0.5309 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5130 - accuracy: 0.8896 - val_loss: 0.5288 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.5060 - accuracy: 0.8872 - val_loss: 0.5413 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4790 - accuracy: 0.8915 - val_loss: 0.4520 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4829 - accuracy: 0.8908 - val_loss: 0.4831 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4781 - accuracy: 0.8946 - val_loss: 0.5263 - val_accuracy: 0.8712 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4624 - accuracy: 0.8974 - val_loss: 0.4819 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4578 - accuracy: 0.8965 - val_loss: 0.5427 - val_accuracy: 0.8765 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4610 - accuracy: 0.8954 - val_loss: 0.4609 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4619 - accuracy: 0.8963 - val_loss: 0.4315 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4432 - accuracy: 0.8998 - val_loss: 0.5314 - val_accuracy: 0.8656 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4431 - accuracy: 0.8978 - val_loss: 0.4538 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4327 - accuracy: 0.9020 - val_loss: 0.4284 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4404 - accuracy: 0.9010 - val_loss: 0.4347 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4393 - accuracy: 0.9013 - val_loss: 0.5246 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4418 - accuracy: 0.9032 - val_loss: 0.4259 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4500 - accuracy: 0.9026 - val_loss: 0.4369 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4472 - accuracy: 0.9048 - val_loss: 0.4524 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4384 - accuracy: 0.9037 - val_loss: 0.4367 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4373 - accuracy: 0.9041 - val_loss: 0.4094 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4250 - accuracy: 0.9000 - val_loss: 0.4616 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4193 - accuracy: 0.9055 - val_loss: 0.7122 - val_accuracy: 0.7694 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4206 - accuracy: 0.9062 - val_loss: 0.4724 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4184 - accuracy: 0.9071 - val_loss: 0.4049 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4309 - accuracy: 0.9044 - val_loss: 0.4135 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4272 - accuracy: 0.9055 - val_loss: 0.4260 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4233 - accuracy: 0.9085 - val_loss: 0.4536 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4339 - accuracy: 0.9067 - val_loss: 0.4180 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4254 - accuracy: 0.9072 - val_loss: 0.4037 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4305 - accuracy: 0.9067 - val_loss: 0.7358 - val_accuracy: 0.8239 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4406 - accuracy: 0.9079 - val_loss: 0.4311 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4379 - accuracy: 0.9107 - val_loss: 0.4215 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4435 - accuracy: 0.9106 - val_loss: 0.5346 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4344 - accuracy: 0.9074 - val_loss: 0.4478 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4352 - accuracy: 0.9098 - val_loss: 0.4654 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4296 - accuracy: 0.9124 - val_loss: 0.4709 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4619 - accuracy: 0.9085 - val_loss: 0.5060 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4403 - accuracy: 0.9092 - val_loss: 0.4237 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4472 - accuracy: 0.9055 - val_loss: 0.4459 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4350 - accuracy: 0.9057 - val_loss: 0.5679 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4249 - accuracy: 0.9120 - val_loss: 0.4359 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4147 - accuracy: 0.9128 - val_loss: 0.3925 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4332 - accuracy: 0.9042 - val_loss: 0.4444 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4219 - accuracy: 0.9089 - val_loss: 0.4516 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4177 - accuracy: 0.9104 - val_loss: 0.4149 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4299 - accuracy: 0.9102 - val_loss: 0.4158 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4088 - accuracy: 0.9125 - val_loss: 0.4403 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4282 - accuracy: 0.9055 - val_loss: 0.4393 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4059 - accuracy: 0.9113 - val_loss: 0.5794 - val_accuracy: 0.8458 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4156 - accuracy: 0.9115 - val_loss: 0.4125 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4132 - accuracy: 0.9142 - val_loss: 0.5850 - val_accuracy: 0.8444 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4225 - accuracy: 0.9138 - val_loss: 0.4440 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4193 - accuracy: 0.9122 - val_loss: 0.5051 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4257 - accuracy: 0.9077 - val_loss: 0.4160 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4207 - accuracy: 0.9110 - val_loss: 0.4708 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4282 - accuracy: 0.9108 - val_loss: 0.4234 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4171 - accuracy: 0.9095 - val_loss: 0.4861 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4068 - accuracy: 0.9121 - val_loss: 0.4448 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4211 - accuracy: 0.9137 - val_loss: 0.5229 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4203 - accuracy: 0.9085 - val_loss: 0.4283 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4051 - accuracy: 0.9144 - val_loss: 0.4164 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3830 - accuracy: 0.9220 - val_loss: 0.3676 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3508 - accuracy: 0.9244 - val_loss: 0.3631 - val_accuracy: 0.9158 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3347 - accuracy: 0.9255 - val_loss: 0.3465 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3163 - accuracy: 0.9275 - val_loss: 0.3433 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3131 - accuracy: 0.9275 - val_loss: 0.3440 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2985 - accuracy: 0.9286 - val_loss: 0.3281 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2945 - accuracy: 0.9268 - val_loss: 0.2990 - val_accuracy: 0.9294 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2854 - accuracy: 0.9278 - val_loss: 0.3313 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2765 - accuracy: 0.9313 - val_loss: 0.3317 - val_accuracy: 0.9150 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2713 - accuracy: 0.9339 - val_loss: 0.2981 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2679 - accuracy: 0.9354 - val_loss: 0.2920 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2706 - accuracy: 0.9320 - val_loss: 0.2876 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2689 - accuracy: 0.9308 - val_loss: 0.2853 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2652 - accuracy: 0.9330 - val_loss: 0.2840 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2627 - accuracy: 0.9354 - val_loss: 0.2825 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2610 - accuracy: 0.9329 - val_loss: 0.2829 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2566 - accuracy: 0.9362 - val_loss: 0.2831 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2649 - accuracy: 0.9334 - val_loss: 0.2817 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2608 - accuracy: 0.9325 - val_loss: 0.2814 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2582 - accuracy: 0.9344 - val_loss: 0.2795 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2603 - accuracy: 0.9314 - val_loss: 0.2801 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2584 - accuracy: 0.9348 - val_loss: 0.2801 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2566 - accuracy: 0.9340 - val_loss: 0.2805 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2543 - accuracy: 0.9373 - val_loss: 0.2804 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2560 - accuracy: 0.9334 - val_loss: 0.2803 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2607 - accuracy: 0.9314 - val_loss: 0.2803 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2582 - accuracy: 0.9338 - val_loss: 0.2801 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2564 - accuracy: 0.9342 - val_loss: 0.2802 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2549 - accuracy: 0.9359 - val_loss: 0.2802 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2934 - accuracy: 0.9238\n",
      "17/17 [==============================] - 1s 27ms/step\n",
      "TP:5723, TN:9932, FP:523, FN:768, loss0.2934466004371643, acc0.9238168299303671, sn0.8816823293791404, sp0.9499760879961741, f10.8986417523749706, auc0.9732154391604725\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 17s 44ms/step - loss: 7.9124 - accuracy: 0.6536 - val_loss: 6.6024 - val_accuracy: 0.7300 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 5.4564 - accuracy: 0.7687 - val_loss: 4.4005 - val_accuracy: 0.7904 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 3.5819 - accuracy: 0.8216 - val_loss: 2.9105 - val_accuracy: 0.7872 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.3409 - accuracy: 0.8384 - val_loss: 2.0302 - val_accuracy: 0.7730 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.5889 - accuracy: 0.8508 - val_loss: 1.3438 - val_accuracy: 0.8416 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.1504 - accuracy: 0.8606 - val_loss: 1.1501 - val_accuracy: 0.8121 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.9039 - accuracy: 0.8651 - val_loss: 0.9313 - val_accuracy: 0.8127 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7515 - accuracy: 0.8667 - val_loss: 0.7304 - val_accuracy: 0.8628 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6646 - accuracy: 0.8686 - val_loss: 0.6627 - val_accuracy: 0.8658 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.6034 - accuracy: 0.8786 - val_loss: 0.5574 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.5843 - accuracy: 0.8752 - val_loss: 0.6496 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.5809 - accuracy: 0.8755 - val_loss: 0.5375 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5448 - accuracy: 0.8804 - val_loss: 0.5441 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5439 - accuracy: 0.8783 - val_loss: 0.5150 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.5399 - accuracy: 0.8780 - val_loss: 0.5253 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.5256 - accuracy: 0.8846 - val_loss: 0.5028 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5132 - accuracy: 0.8845 - val_loss: 0.5414 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5175 - accuracy: 0.8886 - val_loss: 0.4848 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5030 - accuracy: 0.8926 - val_loss: 0.4753 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4988 - accuracy: 0.8924 - val_loss: 0.4727 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4895 - accuracy: 0.8960 - val_loss: 0.5294 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4952 - accuracy: 0.8905 - val_loss: 0.5078 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4982 - accuracy: 0.8903 - val_loss: 0.6156 - val_accuracy: 0.8464 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4896 - accuracy: 0.8941 - val_loss: 0.6129 - val_accuracy: 0.8363 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4811 - accuracy: 0.8985 - val_loss: 0.4505 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4782 - accuracy: 0.8993 - val_loss: 0.6067 - val_accuracy: 0.8666 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4780 - accuracy: 0.8967 - val_loss: 0.6667 - val_accuracy: 0.8331 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4695 - accuracy: 0.9026 - val_loss: 0.6671 - val_accuracy: 0.8006 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4580 - accuracy: 0.8990 - val_loss: 0.5118 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4616 - accuracy: 0.9003 - val_loss: 0.4911 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4596 - accuracy: 0.8981 - val_loss: 0.6495 - val_accuracy: 0.8239 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4465 - accuracy: 0.9032 - val_loss: 0.5960 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4457 - accuracy: 0.9018 - val_loss: 0.5042 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4591 - accuracy: 0.9044 - val_loss: 0.4196 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4363 - accuracy: 0.9065 - val_loss: 0.4518 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4493 - accuracy: 0.9037 - val_loss: 0.4118 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4359 - accuracy: 0.9055 - val_loss: 0.4307 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4407 - accuracy: 0.9065 - val_loss: 0.5077 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4409 - accuracy: 0.9047 - val_loss: 0.4256 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4442 - accuracy: 0.9069 - val_loss: 0.6853 - val_accuracy: 0.8121 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4327 - accuracy: 0.9051 - val_loss: 0.4379 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4285 - accuracy: 0.9090 - val_loss: 0.4023 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4171 - accuracy: 0.9087 - val_loss: 0.4454 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4238 - accuracy: 0.9054 - val_loss: 0.5783 - val_accuracy: 0.8267 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4068 - accuracy: 0.9111 - val_loss: 0.4336 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4146 - accuracy: 0.9069 - val_loss: 0.3929 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4082 - accuracy: 0.9085 - val_loss: 0.3998 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3977 - accuracy: 0.9089 - val_loss: 0.4104 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3997 - accuracy: 0.9107 - val_loss: 0.6615 - val_accuracy: 0.8006 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4001 - accuracy: 0.9132 - val_loss: 0.3879 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4080 - accuracy: 0.9117 - val_loss: 0.6764 - val_accuracy: 0.7892 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3995 - accuracy: 0.9128 - val_loss: 0.3972 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4030 - accuracy: 0.9099 - val_loss: 0.3886 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4098 - accuracy: 0.9104 - val_loss: 0.3918 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4030 - accuracy: 0.9093 - val_loss: 0.5452 - val_accuracy: 0.8654 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4160 - accuracy: 0.9073 - val_loss: 0.4899 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4108 - accuracy: 0.9121 - val_loss: 0.3904 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3984 - accuracy: 0.9115 - val_loss: 0.4331 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3987 - accuracy: 0.9124 - val_loss: 0.4165 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4018 - accuracy: 0.9097 - val_loss: 0.4079 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3986 - accuracy: 0.9114 - val_loss: 0.4133 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4073 - accuracy: 0.9084 - val_loss: 0.4218 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4008 - accuracy: 0.9115 - val_loss: 0.4334 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3942 - accuracy: 0.9126 - val_loss: 0.3844 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4055 - accuracy: 0.9155 - val_loss: 0.4135 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4079 - accuracy: 0.9148 - val_loss: 0.4048 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4073 - accuracy: 0.9138 - val_loss: 0.3826 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4015 - accuracy: 0.9127 - val_loss: 0.3769 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4031 - accuracy: 0.9120 - val_loss: 0.4532 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4181 - accuracy: 0.9139 - val_loss: 0.4344 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4155 - accuracy: 0.9109 - val_loss: 0.4274 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3747 - accuracy: 0.9261 - val_loss: 0.3859 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3526 - accuracy: 0.9250 - val_loss: 0.3542 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3328 - accuracy: 0.9254 - val_loss: 0.3604 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3153 - accuracy: 0.9263 - val_loss: 0.3561 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3109 - accuracy: 0.9279 - val_loss: 0.3240 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2992 - accuracy: 0.9283 - val_loss: 0.3056 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2910 - accuracy: 0.9265 - val_loss: 0.3028 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2871 - accuracy: 0.9279 - val_loss: 0.2873 - val_accuracy: 0.9296 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2784 - accuracy: 0.9304 - val_loss: 0.2806 - val_accuracy: 0.9310 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2734 - accuracy: 0.9303 - val_loss: 0.2943 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2691 - accuracy: 0.9329 - val_loss: 0.2819 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2633 - accuracy: 0.9348 - val_loss: 0.2806 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2656 - accuracy: 0.9304 - val_loss: 0.2789 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2620 - accuracy: 0.9344 - val_loss: 0.2767 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2608 - accuracy: 0.9338 - val_loss: 0.2761 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2617 - accuracy: 0.9338 - val_loss: 0.2762 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2599 - accuracy: 0.9336 - val_loss: 0.2782 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2608 - accuracy: 0.9326 - val_loss: 0.2737 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2529 - accuracy: 0.9361 - val_loss: 0.2765 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2567 - accuracy: 0.9332 - val_loss: 0.2790 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2524 - accuracy: 0.9353 - val_loss: 0.2770 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2556 - accuracy: 0.9359 - val_loss: 0.2757 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2520 - accuracy: 0.9356 - val_loss: 0.2750 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2555 - accuracy: 0.9340 - val_loss: 0.2752 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2584 - accuracy: 0.9331 - val_loss: 0.2750 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2531 - accuracy: 0.9345 - val_loss: 0.2749 - val_accuracy: 0.9288 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2531 - accuracy: 0.9356 - val_loss: 0.2745 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2559 - accuracy: 0.9345 - val_loss: 0.2746 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2545 - accuracy: 0.9336 - val_loss: 0.2747 - val_accuracy: 0.9290 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2903 - accuracy: 0.9246\n",
      "17/17 [==============================] - 1s 32ms/step\n",
      "TP:5766, TN:9903, FP:552, FN:725, loss0.2903013229370117, acc0.9246429835949487, sn0.8883068864581729, sp0.9472022955523672, f10.9003044734171286, auc0.9731804350223806\n",
      "Average Test loss:  0.2908779442310333\n",
      "Average Accuracy:  0.9251917856721349\n",
      "Average Sensitivity:  0.8911569865968264\n",
      "Average Specificity:  0.9463223338115734\n",
      "Average F1 Score:  0.9012371332096967\n",
      "Average AUC Score:  0.9738171360544025\n",
      "AUC for ROC curve 1: 0.9738\n",
      "AUC for ROC curve 2: 0.9738\n",
      "AUC for ROC curve 3: 0.9734\n",
      "AUC for ROC curve 4: 0.9734\n",
      "AUC for ROC curve 5: 0.9741\n",
      "AUC for ROC curve 6: 0.9741\n",
      "AUC for ROC curve 7: 0.9742\n",
      "AUC for ROC curve 8: 0.9742\n",
      "AUC for ROC curve 9: 0.9737\n",
      "AUC for ROC curve 10: 0.9737\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f9963-cfa7-47f5-8701-2a24e58e20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lxx",
   "language": "python",
   "name": "lxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
