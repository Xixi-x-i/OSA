{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 14:56:52.352226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = BatchNormalization()(input1)\n",
    "    x1 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    \n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3)(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3)(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=5)(x1)    \n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "    x2 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    \n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    \n",
    "\n",
    "    # CNN-1\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "    x3 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    \n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)   \n",
    "    x3 = MaxPooling1D(pool_size=1)(x3)    \n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "\n",
    "    attention1 = ResidualAttentionBlock(256, 256)\n",
    "    x1 = attention1(x1)\n",
    "    x2 = attention1(x2)\n",
    "    x3 = attention1(x3)\n",
    "    \n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1,input2,input3], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  # Convert to two categories\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 16s 36ms/step - loss: 3.4541 - accuracy: 0.5824 - val_loss: 3.0993 - val_accuracy: 0.4146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 2.6516 - accuracy: 0.6323 - val_loss: 2.4520 - val_accuracy: 0.4089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 2.0994 - accuracy: 0.6666 - val_loss: 1.9667 - val_accuracy: 0.5886 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 1.6828 - accuracy: 0.6976 - val_loss: 1.6594 - val_accuracy: 0.6133 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 1.3765 - accuracy: 0.7266 - val_loss: 1.2920 - val_accuracy: 0.7122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 1.1550 - accuracy: 0.7406 - val_loss: 1.0964 - val_accuracy: 0.7252 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.9759 - accuracy: 0.7617 - val_loss: 0.9818 - val_accuracy: 0.7104 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.8554 - accuracy: 0.7782 - val_loss: 0.8638 - val_accuracy: 0.7349 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.7625 - accuracy: 0.7940 - val_loss: 0.8602 - val_accuracy: 0.7000 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.7113 - accuracy: 0.7945 - val_loss: 0.7018 - val_accuracy: 0.7904 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.6459 - accuracy: 0.8149 - val_loss: 0.6004 - val_accuracy: 0.8329 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.6017 - accuracy: 0.8236 - val_loss: 0.6133 - val_accuracy: 0.8065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.5657 - accuracy: 0.8291 - val_loss: 0.5189 - val_accuracy: 0.8528 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.5375 - accuracy: 0.8374 - val_loss: 0.4830 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.5064 - accuracy: 0.8524 - val_loss: 0.4721 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4926 - accuracy: 0.8519 - val_loss: 0.4790 - val_accuracy: 0.8554 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4702 - accuracy: 0.8599 - val_loss: 0.4526 - val_accuracy: 0.8652 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4660 - accuracy: 0.8567 - val_loss: 0.4249 - val_accuracy: 0.8746 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4555 - accuracy: 0.8606 - val_loss: 0.4257 - val_accuracy: 0.8718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4439 - accuracy: 0.8638 - val_loss: 0.4294 - val_accuracy: 0.8706 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4357 - accuracy: 0.8642 - val_loss: 0.3754 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4288 - accuracy: 0.8676 - val_loss: 0.4193 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4270 - accuracy: 0.8689 - val_loss: 0.3728 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4101 - accuracy: 0.8745 - val_loss: 0.3845 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4140 - accuracy: 0.8694 - val_loss: 0.3659 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4109 - accuracy: 0.8717 - val_loss: 0.3905 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4057 - accuracy: 0.8755 - val_loss: 0.3486 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3994 - accuracy: 0.8743 - val_loss: 0.3722 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3934 - accuracy: 0.8769 - val_loss: 0.3587 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3942 - accuracy: 0.8766 - val_loss: 0.3364 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.4001 - accuracy: 0.8755 - val_loss: 0.3482 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3896 - accuracy: 0.8790 - val_loss: 0.3397 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3924 - accuracy: 0.8752 - val_loss: 0.3524 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3879 - accuracy: 0.8780 - val_loss: 0.3432 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3791 - accuracy: 0.8796 - val_loss: 0.3256 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3865 - accuracy: 0.8801 - val_loss: 0.3368 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3830 - accuracy: 0.8776 - val_loss: 0.3410 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3862 - accuracy: 0.8795 - val_loss: 0.3291 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3811 - accuracy: 0.8810 - val_loss: 0.3386 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3820 - accuracy: 0.8808 - val_loss: 0.3370 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3771 - accuracy: 0.8815 - val_loss: 0.3527 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3780 - accuracy: 0.8816 - val_loss: 0.3205 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3739 - accuracy: 0.8836 - val_loss: 0.3283 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3751 - accuracy: 0.8834 - val_loss: 0.3455 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3763 - accuracy: 0.8815 - val_loss: 0.3193 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3691 - accuracy: 0.8883 - val_loss: 0.3288 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3711 - accuracy: 0.8838 - val_loss: 0.3212 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3706 - accuracy: 0.8810 - val_loss: 0.3326 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3769 - accuracy: 0.8826 - val_loss: 0.3226 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3713 - accuracy: 0.8808 - val_loss: 0.3119 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3685 - accuracy: 0.8839 - val_loss: 0.3250 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3687 - accuracy: 0.8891 - val_loss: 0.3244 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3708 - accuracy: 0.8866 - val_loss: 0.3303 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3667 - accuracy: 0.8855 - val_loss: 0.3486 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3667 - accuracy: 0.8843 - val_loss: 0.3554 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3675 - accuracy: 0.8875 - val_loss: 0.3168 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3656 - accuracy: 0.8882 - val_loss: 0.3330 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3653 - accuracy: 0.8864 - val_loss: 0.3222 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3674 - accuracy: 0.8873 - val_loss: 0.3246 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3673 - accuracy: 0.8862 - val_loss: 0.3255 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3619 - accuracy: 0.8861 - val_loss: 0.3143 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3589 - accuracy: 0.8889 - val_loss: 0.3200 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3642 - accuracy: 0.8835 - val_loss: 0.3049 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3551 - accuracy: 0.8881 - val_loss: 0.3278 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3628 - accuracy: 0.8871 - val_loss: 0.3119 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3561 - accuracy: 0.8892 - val_loss: 0.3110 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3611 - accuracy: 0.8855 - val_loss: 0.3262 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3622 - accuracy: 0.8881 - val_loss: 0.3195 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3587 - accuracy: 0.8873 - val_loss: 0.3221 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3582 - accuracy: 0.8876 - val_loss: 0.3107 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3526 - accuracy: 0.8880 - val_loss: 0.3340 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3407 - accuracy: 0.8939 - val_loss: 0.3042 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3391 - accuracy: 0.8930 - val_loss: 0.3000 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3334 - accuracy: 0.8926 - val_loss: 0.2945 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3298 - accuracy: 0.8992 - val_loss: 0.2924 - val_accuracy: 0.9108 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3257 - accuracy: 0.8957 - val_loss: 0.2906 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3198 - accuracy: 0.8995 - val_loss: 0.2864 - val_accuracy: 0.9132 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3245 - accuracy: 0.8949 - val_loss: 0.2823 - val_accuracy: 0.9146 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3203 - accuracy: 0.9005 - val_loss: 0.2848 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3177 - accuracy: 0.9008 - val_loss: 0.2832 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3091 - accuracy: 0.8983 - val_loss: 0.2795 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3095 - accuracy: 0.8995 - val_loss: 0.2814 - val_accuracy: 0.9126 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3160 - accuracy: 0.8971 - val_loss: 0.2814 - val_accuracy: 0.9128 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3147 - accuracy: 0.8974 - val_loss: 0.2820 - val_accuracy: 0.9134 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3121 - accuracy: 0.9009 - val_loss: 0.2822 - val_accuracy: 0.9132 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3126 - accuracy: 0.9020 - val_loss: 0.2815 - val_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3089 - accuracy: 0.9016 - val_loss: 0.2811 - val_accuracy: 0.9130 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3103 - accuracy: 0.8986 - val_loss: 0.2815 - val_accuracy: 0.9128 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3109 - accuracy: 0.9008 - val_loss: 0.2808 - val_accuracy: 0.9132 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3107 - accuracy: 0.9013 - val_loss: 0.2812 - val_accuracy: 0.9122 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3078 - accuracy: 0.8984 - val_loss: 0.2805 - val_accuracy: 0.9138 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3078 - accuracy: 0.8996 - val_loss: 0.2807 - val_accuracy: 0.9134 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3101 - accuracy: 0.8989 - val_loss: 0.2809 - val_accuracy: 0.9126 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3092 - accuracy: 0.8969 - val_loss: 0.2808 - val_accuracy: 0.9132 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3110 - accuracy: 0.8994 - val_loss: 0.2811 - val_accuracy: 0.9126 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3069 - accuracy: 0.8997 - val_loss: 0.2809 - val_accuracy: 0.9128 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3105 - accuracy: 0.9015 - val_loss: 0.2807 - val_accuracy: 0.9134 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3043 - accuracy: 0.9032 - val_loss: 0.2808 - val_accuracy: 0.9132 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3167 - accuracy: 0.8990 - val_loss: 0.2808 - val_accuracy: 0.9130 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3077 - accuracy: 0.8996 - val_loss: 0.2806 - val_accuracy: 0.9134 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 6s 10ms/step - loss: 0.2784 - accuracy: 0.9159\n",
      "17/17 [==============================] - 2s 37ms/step\n",
      "TP:5771, TN:9749, FP:706, FN:720, loss0.2783592641353607, acc0.9158503481647586, sn0.8890771837929441, sp0.9324725011956002, f10.8900370141887722, auc0.9681897334800693\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 14s 36ms/step - loss: 3.4368 - accuracy: 0.5852 - val_loss: 3.1456 - val_accuracy: 0.4049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 2.6257 - accuracy: 0.6306 - val_loss: 2.7435 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 2.0644 - accuracy: 0.6686 - val_loss: 2.1664 - val_accuracy: 0.4011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.6628 - accuracy: 0.6842 - val_loss: 1.7385 - val_accuracy: 0.4174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.3620 - accuracy: 0.7206 - val_loss: 1.5221 - val_accuracy: 0.4715 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 1.1201 - accuracy: 0.7602 - val_loss: 1.4286 - val_accuracy: 0.4920 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.9438 - accuracy: 0.7932 - val_loss: 0.9311 - val_accuracy: 0.7764 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.8233 - accuracy: 0.8086 - val_loss: 0.7663 - val_accuracy: 0.8171 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.7372 - accuracy: 0.8205 - val_loss: 0.6779 - val_accuracy: 0.8291 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6667 - accuracy: 0.8344 - val_loss: 0.6141 - val_accuracy: 0.8460 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.6154 - accuracy: 0.8364 - val_loss: 0.5933 - val_accuracy: 0.8418 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5718 - accuracy: 0.8482 - val_loss: 0.5393 - val_accuracy: 0.8522 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5432 - accuracy: 0.8513 - val_loss: 0.5119 - val_accuracy: 0.8584 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.5194 - accuracy: 0.8537 - val_loss: 0.4672 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4917 - accuracy: 0.8599 - val_loss: 0.4986 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4792 - accuracy: 0.8619 - val_loss: 0.4246 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4699 - accuracy: 0.8593 - val_loss: 0.4370 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4582 - accuracy: 0.8640 - val_loss: 0.4329 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4409 - accuracy: 0.8696 - val_loss: 0.4119 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4351 - accuracy: 0.8656 - val_loss: 0.3868 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4329 - accuracy: 0.8674 - val_loss: 0.3875 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4243 - accuracy: 0.8702 - val_loss: 0.3637 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4231 - accuracy: 0.8708 - val_loss: 0.3569 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4073 - accuracy: 0.8721 - val_loss: 0.4095 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4053 - accuracy: 0.8714 - val_loss: 0.3486 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3943 - accuracy: 0.8776 - val_loss: 0.3754 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3976 - accuracy: 0.8773 - val_loss: 0.3737 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3976 - accuracy: 0.8763 - val_loss: 0.3684 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3962 - accuracy: 0.8765 - val_loss: 0.3673 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3920 - accuracy: 0.8800 - val_loss: 0.3366 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.4039 - accuracy: 0.8733 - val_loss: 0.3537 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3902 - accuracy: 0.8785 - val_loss: 0.3414 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3893 - accuracy: 0.8767 - val_loss: 0.3404 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3823 - accuracy: 0.8817 - val_loss: 0.3366 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3831 - accuracy: 0.8798 - val_loss: 0.3292 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3798 - accuracy: 0.8815 - val_loss: 0.3337 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3744 - accuracy: 0.8825 - val_loss: 0.3289 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3755 - accuracy: 0.8798 - val_loss: 0.3258 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3690 - accuracy: 0.8860 - val_loss: 0.3242 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3754 - accuracy: 0.8840 - val_loss: 0.3298 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3710 - accuracy: 0.8848 - val_loss: 0.3140 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3721 - accuracy: 0.8822 - val_loss: 0.3238 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3663 - accuracy: 0.8852 - val_loss: 0.3262 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3732 - accuracy: 0.8821 - val_loss: 0.3162 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3696 - accuracy: 0.8830 - val_loss: 0.3367 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3694 - accuracy: 0.8834 - val_loss: 0.3180 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3732 - accuracy: 0.8784 - val_loss: 0.3212 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3625 - accuracy: 0.8856 - val_loss: 0.3626 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3684 - accuracy: 0.8811 - val_loss: 0.3196 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3620 - accuracy: 0.8877 - val_loss: 0.3336 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3661 - accuracy: 0.8860 - val_loss: 0.3402 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3601 - accuracy: 0.8867 - val_loss: 0.3156 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3646 - accuracy: 0.8864 - val_loss: 0.3134 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3594 - accuracy: 0.8885 - val_loss: 0.3123 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3653 - accuracy: 0.8890 - val_loss: 0.3117 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3583 - accuracy: 0.8869 - val_loss: 0.3024 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3587 - accuracy: 0.8905 - val_loss: 0.3157 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3582 - accuracy: 0.8898 - val_loss: 0.3234 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3550 - accuracy: 0.8930 - val_loss: 0.3134 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 27ms/step - loss: 0.3641 - accuracy: 0.8855 - val_loss: 0.3084 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3556 - accuracy: 0.8899 - val_loss: 0.3140 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3605 - accuracy: 0.8865 - val_loss: 0.3098 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3556 - accuracy: 0.8896 - val_loss: 0.3043 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3506 - accuracy: 0.8906 - val_loss: 0.3088 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 27ms/step - loss: 0.3581 - accuracy: 0.8894 - val_loss: 0.3359 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3556 - accuracy: 0.8881 - val_loss: 0.3147 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3514 - accuracy: 0.8926 - val_loss: 0.3107 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3557 - accuracy: 0.8896 - val_loss: 0.3119 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3518 - accuracy: 0.8929 - val_loss: 0.3377 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3555 - accuracy: 0.8894 - val_loss: 0.3220 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3501 - accuracy: 0.8877 - val_loss: 0.3083 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3450 - accuracy: 0.8947 - val_loss: 0.3015 - val_accuracy: 0.9079 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3291 - accuracy: 0.8955 - val_loss: 0.2953 - val_accuracy: 0.9107 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3224 - accuracy: 0.9016 - val_loss: 0.2947 - val_accuracy: 0.9093 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3255 - accuracy: 0.8982 - val_loss: 0.2884 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3249 - accuracy: 0.8950 - val_loss: 0.2881 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3229 - accuracy: 0.8973 - val_loss: 0.2828 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3196 - accuracy: 0.8976 - val_loss: 0.2806 - val_accuracy: 0.9132 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3222 - accuracy: 0.8951 - val_loss: 0.2815 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3157 - accuracy: 0.8975 - val_loss: 0.2807 - val_accuracy: 0.9108 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3081 - accuracy: 0.9033 - val_loss: 0.2788 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3101 - accuracy: 0.8995 - val_loss: 0.2793 - val_accuracy: 0.9114 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3143 - accuracy: 0.8991 - val_loss: 0.2804 - val_accuracy: 0.9114 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3096 - accuracy: 0.8990 - val_loss: 0.2804 - val_accuracy: 0.9116 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3106 - accuracy: 0.8981 - val_loss: 0.2802 - val_accuracy: 0.9120 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3078 - accuracy: 0.9015 - val_loss: 0.2798 - val_accuracy: 0.9122 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3073 - accuracy: 0.9046 - val_loss: 0.2797 - val_accuracy: 0.9116 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3051 - accuracy: 0.9009 - val_loss: 0.2799 - val_accuracy: 0.9120 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3076 - accuracy: 0.9005 - val_loss: 0.2799 - val_accuracy: 0.9118 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3051 - accuracy: 0.9017 - val_loss: 0.2804 - val_accuracy: 0.9114 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3123 - accuracy: 0.9003 - val_loss: 0.2795 - val_accuracy: 0.9118 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3088 - accuracy: 0.8977 - val_loss: 0.2797 - val_accuracy: 0.9118 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3051 - accuracy: 0.9026 - val_loss: 0.2795 - val_accuracy: 0.9122 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3100 - accuracy: 0.8996 - val_loss: 0.2800 - val_accuracy: 0.9120 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3083 - accuracy: 0.8998 - val_loss: 0.2801 - val_accuracy: 0.9120 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3068 - accuracy: 0.8986 - val_loss: 0.2800 - val_accuracy: 0.9120 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3075 - accuracy: 0.9001 - val_loss: 0.2799 - val_accuracy: 0.9118 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3134 - accuracy: 0.8973 - val_loss: 0.2802 - val_accuracy: 0.9120 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3104 - accuracy: 0.8962 - val_loss: 0.2796 - val_accuracy: 0.9124 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3113 - accuracy: 0.8985 - val_loss: 0.2795 - val_accuracy: 0.9122 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2779 - accuracy: 0.9160\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5786, TN:9736, FP:719, FN:705, loss0.2778785824775696, acc0.9159683701168417, sn0.8913880757972578, sp0.9312290769966524, f10.8904278239458295, auc0.9682846078825545\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 28ms/step - loss: 3.3863 - accuracy: 0.5840 - val_loss: 3.0871 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 2.5556 - accuracy: 0.6344 - val_loss: 2.4396 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.0042 - accuracy: 0.6535 - val_loss: 1.8796 - val_accuracy: 0.5273 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.6027 - accuracy: 0.6744 - val_loss: 1.5207 - val_accuracy: 0.5957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.3175 - accuracy: 0.6963 - val_loss: 1.2634 - val_accuracy: 0.6522 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.0898 - accuracy: 0.7397 - val_loss: 0.9800 - val_accuracy: 0.7627 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.9221 - accuracy: 0.7672 - val_loss: 0.8420 - val_accuracy: 0.7916 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7921 - accuracy: 0.7966 - val_loss: 0.7612 - val_accuracy: 0.7918 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7036 - accuracy: 0.8121 - val_loss: 0.6499 - val_accuracy: 0.8293 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.6386 - accuracy: 0.8228 - val_loss: 0.5796 - val_accuracy: 0.8494 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5800 - accuracy: 0.8372 - val_loss: 0.5512 - val_accuracy: 0.8534 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5486 - accuracy: 0.8489 - val_loss: 0.4915 - val_accuracy: 0.8702 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4971 - accuracy: 0.8610 - val_loss: 0.4692 - val_accuracy: 0.8716 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4699 - accuracy: 0.8679 - val_loss: 0.4176 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4532 - accuracy: 0.8715 - val_loss: 0.4141 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4392 - accuracy: 0.8773 - val_loss: 0.4033 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4267 - accuracy: 0.8793 - val_loss: 0.3706 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4252 - accuracy: 0.8761 - val_loss: 0.3763 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4149 - accuracy: 0.8779 - val_loss: 0.3741 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4003 - accuracy: 0.8820 - val_loss: 0.3547 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3931 - accuracy: 0.8796 - val_loss: 0.3452 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3884 - accuracy: 0.8865 - val_loss: 0.3902 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3861 - accuracy: 0.8826 - val_loss: 0.3333 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3859 - accuracy: 0.8814 - val_loss: 0.3433 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3826 - accuracy: 0.8861 - val_loss: 0.3459 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3764 - accuracy: 0.8871 - val_loss: 0.3311 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3745 - accuracy: 0.8874 - val_loss: 0.3296 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3707 - accuracy: 0.8899 - val_loss: 0.3286 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3712 - accuracy: 0.8867 - val_loss: 0.3242 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3669 - accuracy: 0.8902 - val_loss: 0.3352 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3615 - accuracy: 0.8940 - val_loss: 0.3143 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3570 - accuracy: 0.8912 - val_loss: 0.3145 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3621 - accuracy: 0.8869 - val_loss: 0.3176 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3540 - accuracy: 0.8942 - val_loss: 0.3215 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3629 - accuracy: 0.8885 - val_loss: 0.3229 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3659 - accuracy: 0.8903 - val_loss: 0.3190 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3610 - accuracy: 0.8892 - val_loss: 0.3301 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3563 - accuracy: 0.8908 - val_loss: 0.3159 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3527 - accuracy: 0.8915 - val_loss: 0.3182 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3575 - accuracy: 0.8907 - val_loss: 0.3170 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3610 - accuracy: 0.8893 - val_loss: 0.3114 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3538 - accuracy: 0.8942 - val_loss: 0.3171 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3517 - accuracy: 0.8910 - val_loss: 0.3056 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3476 - accuracy: 0.8946 - val_loss: 0.3073 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3473 - accuracy: 0.8935 - val_loss: 0.3018 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3484 - accuracy: 0.8904 - val_loss: 0.3064 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3418 - accuracy: 0.8941 - val_loss: 0.3064 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3420 - accuracy: 0.8942 - val_loss: 0.3047 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3387 - accuracy: 0.8959 - val_loss: 0.3041 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3445 - accuracy: 0.8942 - val_loss: 0.3066 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3403 - accuracy: 0.8938 - val_loss: 0.3128 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3497 - accuracy: 0.8924 - val_loss: 0.3239 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3433 - accuracy: 0.8929 - val_loss: 0.3161 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3470 - accuracy: 0.8976 - val_loss: 0.2983 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3443 - accuracy: 0.8941 - val_loss: 0.3077 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3397 - accuracy: 0.8961 - val_loss: 0.3147 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3352 - accuracy: 0.8972 - val_loss: 0.3223 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3398 - accuracy: 0.8973 - val_loss: 0.3215 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3383 - accuracy: 0.8996 - val_loss: 0.3079 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3413 - accuracy: 0.8921 - val_loss: 0.3022 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3362 - accuracy: 0.8964 - val_loss: 0.3064 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3393 - accuracy: 0.8968 - val_loss: 0.3058 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3301 - accuracy: 0.8992 - val_loss: 0.3060 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3298 - accuracy: 0.8975 - val_loss: 0.3167 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3389 - accuracy: 0.8959 - val_loss: 0.2977 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3370 - accuracy: 0.8967 - val_loss: 0.2952 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3340 - accuracy: 0.9008 - val_loss: 0.2929 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3406 - accuracy: 0.8940 - val_loss: 0.3106 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3351 - accuracy: 0.8985 - val_loss: 0.2952 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3356 - accuracy: 0.8979 - val_loss: 0.2984 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3315 - accuracy: 0.8979 - val_loss: 0.2963 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3186 - accuracy: 0.9038 - val_loss: 0.2850 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3171 - accuracy: 0.9042 - val_loss: 0.2826 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3124 - accuracy: 0.9011 - val_loss: 0.2790 - val_accuracy: 0.9158 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3112 - accuracy: 0.9019 - val_loss: 0.2760 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3044 - accuracy: 0.9039 - val_loss: 0.2754 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2983 - accuracy: 0.9089 - val_loss: 0.2710 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3016 - accuracy: 0.9056 - val_loss: 0.2704 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2925 - accuracy: 0.9075 - val_loss: 0.2702 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2959 - accuracy: 0.9073 - val_loss: 0.2670 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2912 - accuracy: 0.9065 - val_loss: 0.2659 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2899 - accuracy: 0.9077 - val_loss: 0.2651 - val_accuracy: 0.9176 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2871 - accuracy: 0.9071 - val_loss: 0.2648 - val_accuracy: 0.9188 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2926 - accuracy: 0.9059 - val_loss: 0.2643 - val_accuracy: 0.9194 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2955 - accuracy: 0.9058 - val_loss: 0.2639 - val_accuracy: 0.9178 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2904 - accuracy: 0.9069 - val_loss: 0.2634 - val_accuracy: 0.9180 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2907 - accuracy: 0.9068 - val_loss: 0.2634 - val_accuracy: 0.9182 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2823 - accuracy: 0.9102 - val_loss: 0.2629 - val_accuracy: 0.9178 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2937 - accuracy: 0.9060 - val_loss: 0.2630 - val_accuracy: 0.9178 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2903 - accuracy: 0.9089 - val_loss: 0.2626 - val_accuracy: 0.9176 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2883 - accuracy: 0.9075 - val_loss: 0.2625 - val_accuracy: 0.9182 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2904 - accuracy: 0.9071 - val_loss: 0.2625 - val_accuracy: 0.9178 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2870 - accuracy: 0.9087 - val_loss: 0.2625 - val_accuracy: 0.9178 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2871 - accuracy: 0.9078 - val_loss: 0.2625 - val_accuracy: 0.9182 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2928 - accuracy: 0.9045 - val_loss: 0.2624 - val_accuracy: 0.9180 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2918 - accuracy: 0.9054 - val_loss: 0.2624 - val_accuracy: 0.9180 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2922 - accuracy: 0.9043 - val_loss: 0.2623 - val_accuracy: 0.9182 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2881 - accuracy: 0.9066 - val_loss: 0.2624 - val_accuracy: 0.9180 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2859 - accuracy: 0.9065 - val_loss: 0.2625 - val_accuracy: 0.9176 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2866 - accuracy: 0.9091 - val_loss: 0.2625 - val_accuracy: 0.9180 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2557 - accuracy: 0.9205\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5689, TN:9910, FP:545, FN:802, loss0.2556840479373932, acc0.9205122152720406, sn0.8764443075026961, sp0.9478718316594931, f10.8941453831041258, auc0.9703397140181811\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 33ms/step - loss: 3.4087 - accuracy: 0.5903 - val_loss: 3.1958 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 2.6061 - accuracy: 0.6422 - val_loss: 2.7772 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 2.0685 - accuracy: 0.6646 - val_loss: 2.1287 - val_accuracy: 0.4087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 1.6758 - accuracy: 0.6807 - val_loss: 1.6757 - val_accuracy: 0.4757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 1.3840 - accuracy: 0.6925 - val_loss: 1.4717 - val_accuracy: 0.5088 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.1658 - accuracy: 0.7218 - val_loss: 1.4016 - val_accuracy: 0.5018 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.0047 - accuracy: 0.7438 - val_loss: 1.2276 - val_accuracy: 0.5533 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.8831 - accuracy: 0.7626 - val_loss: 1.0562 - val_accuracy: 0.6111 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7730 - accuracy: 0.7855 - val_loss: 0.7828 - val_accuracy: 0.7669 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7003 - accuracy: 0.8024 - val_loss: 0.9173 - val_accuracy: 0.6627 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.6472 - accuracy: 0.8147 - val_loss: 0.7285 - val_accuracy: 0.7543 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5991 - accuracy: 0.8291 - val_loss: 0.5403 - val_accuracy: 0.8566 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.5700 - accuracy: 0.8360 - val_loss: 0.5195 - val_accuracy: 0.8614 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5349 - accuracy: 0.8437 - val_loss: 0.5720 - val_accuracy: 0.8317 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5076 - accuracy: 0.8486 - val_loss: 0.4540 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4795 - accuracy: 0.8599 - val_loss: 0.4475 - val_accuracy: 0.8732 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4653 - accuracy: 0.8642 - val_loss: 0.4050 - val_accuracy: 0.8853 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4513 - accuracy: 0.8682 - val_loss: 0.4168 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4435 - accuracy: 0.8662 - val_loss: 0.4069 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4327 - accuracy: 0.8704 - val_loss: 0.4173 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4259 - accuracy: 0.8709 - val_loss: 0.3773 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4175 - accuracy: 0.8750 - val_loss: 0.3670 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4133 - accuracy: 0.8740 - val_loss: 0.3644 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4081 - accuracy: 0.8745 - val_loss: 0.3524 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4113 - accuracy: 0.8749 - val_loss: 0.3610 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3998 - accuracy: 0.8778 - val_loss: 0.3470 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3929 - accuracy: 0.8783 - val_loss: 0.3670 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4012 - accuracy: 0.8745 - val_loss: 0.3731 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3833 - accuracy: 0.8823 - val_loss: 0.3520 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3854 - accuracy: 0.8802 - val_loss: 0.3519 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3871 - accuracy: 0.8817 - val_loss: 0.3590 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3800 - accuracy: 0.8839 - val_loss: 0.3738 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3812 - accuracy: 0.8812 - val_loss: 0.3477 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3777 - accuracy: 0.8822 - val_loss: 0.3386 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3839 - accuracy: 0.8790 - val_loss: 0.3305 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3660 - accuracy: 0.8856 - val_loss: 0.3677 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3784 - accuracy: 0.8821 - val_loss: 0.3422 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3654 - accuracy: 0.8878 - val_loss: 0.3566 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3734 - accuracy: 0.8828 - val_loss: 0.3358 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3742 - accuracy: 0.8829 - val_loss: 0.3310 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3624 - accuracy: 0.8879 - val_loss: 0.3342 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3665 - accuracy: 0.8852 - val_loss: 0.3239 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3578 - accuracy: 0.8909 - val_loss: 0.3274 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3614 - accuracy: 0.8855 - val_loss: 0.3232 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3636 - accuracy: 0.8844 - val_loss: 0.3515 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3577 - accuracy: 0.8899 - val_loss: 0.3168 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3598 - accuracy: 0.8891 - val_loss: 0.3158 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3576 - accuracy: 0.8873 - val_loss: 0.3370 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3649 - accuracy: 0.8879 - val_loss: 0.3117 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3566 - accuracy: 0.8885 - val_loss: 0.3089 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3555 - accuracy: 0.8862 - val_loss: 0.3173 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3554 - accuracy: 0.8865 - val_loss: 0.3108 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3625 - accuracy: 0.8820 - val_loss: 0.3170 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3600 - accuracy: 0.8891 - val_loss: 0.3355 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3511 - accuracy: 0.8901 - val_loss: 0.3205 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3572 - accuracy: 0.8905 - val_loss: 0.3032 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3453 - accuracy: 0.8914 - val_loss: 0.3137 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3558 - accuracy: 0.8888 - val_loss: 0.3035 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3479 - accuracy: 0.8906 - val_loss: 0.3134 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3524 - accuracy: 0.8891 - val_loss: 0.3353 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3471 - accuracy: 0.8951 - val_loss: 0.3097 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3538 - accuracy: 0.8925 - val_loss: 0.3184 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3476 - accuracy: 0.8917 - val_loss: 0.3080 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3464 - accuracy: 0.8923 - val_loss: 0.3058 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3431 - accuracy: 0.8943 - val_loss: 0.3248 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3478 - accuracy: 0.8899 - val_loss: 0.2956 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3405 - accuracy: 0.8945 - val_loss: 0.3120 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3478 - accuracy: 0.8936 - val_loss: 0.3242 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3479 - accuracy: 0.8916 - val_loss: 0.3018 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3441 - accuracy: 0.8934 - val_loss: 0.3060 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3443 - accuracy: 0.8920 - val_loss: 0.3196 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3340 - accuracy: 0.8963 - val_loss: 0.2950 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3271 - accuracy: 0.9024 - val_loss: 0.2928 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3288 - accuracy: 0.8981 - val_loss: 0.2886 - val_accuracy: 0.9132 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3184 - accuracy: 0.9014 - val_loss: 0.2875 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3211 - accuracy: 0.9006 - val_loss: 0.2849 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3165 - accuracy: 0.8991 - val_loss: 0.2831 - val_accuracy: 0.9140 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3099 - accuracy: 0.9030 - val_loss: 0.2799 - val_accuracy: 0.9140 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3060 - accuracy: 0.9021 - val_loss: 0.2793 - val_accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3023 - accuracy: 0.9059 - val_loss: 0.2751 - val_accuracy: 0.9150 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3015 - accuracy: 0.9048 - val_loss: 0.2720 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3029 - accuracy: 0.9004 - val_loss: 0.2744 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3022 - accuracy: 0.9040 - val_loss: 0.2750 - val_accuracy: 0.9142 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3010 - accuracy: 0.9044 - val_loss: 0.2756 - val_accuracy: 0.9136 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3029 - accuracy: 0.9049 - val_loss: 0.2759 - val_accuracy: 0.9132 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.2985 - accuracy: 0.9018 - val_loss: 0.2754 - val_accuracy: 0.9140 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2967 - accuracy: 0.9019 - val_loss: 0.2756 - val_accuracy: 0.9136 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3020 - accuracy: 0.9030 - val_loss: 0.2753 - val_accuracy: 0.9138 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2983 - accuracy: 0.9054 - val_loss: 0.2748 - val_accuracy: 0.9138 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2956 - accuracy: 0.9050 - val_loss: 0.2747 - val_accuracy: 0.9138 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.2991 - accuracy: 0.9015 - val_loss: 0.2737 - val_accuracy: 0.9144 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2930 - accuracy: 0.9074 - val_loss: 0.2741 - val_accuracy: 0.9142 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2922 - accuracy: 0.9055 - val_loss: 0.2745 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.2947 - accuracy: 0.9048 - val_loss: 0.2746 - val_accuracy: 0.9138 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.2972 - accuracy: 0.9053 - val_loss: 0.2748 - val_accuracy: 0.9134 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 0.3006 - accuracy: 0.9049 - val_loss: 0.2742 - val_accuracy: 0.9146 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3010 - accuracy: 0.9054 - val_loss: 0.2742 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2996 - accuracy: 0.9034 - val_loss: 0.2745 - val_accuracy: 0.9138 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2999 - accuracy: 0.9038 - val_loss: 0.2748 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2952 - accuracy: 0.9033 - val_loss: 0.2745 - val_accuracy: 0.9144 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2672 - accuracy: 0.9197\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5860, TN:9726, FP:729, FN:631, loss0.26718074083328247, acc0.9197450725835006, sn0.9027884763518719, sp0.9302725968436155, f10.8960244648318043, auc0.9713130220919508\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 29ms/step - loss: 3.4251 - accuracy: 0.5838 - val_loss: 3.0579 - val_accuracy: 0.4017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.6175 - accuracy: 0.6348 - val_loss: 2.4941 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.0755 - accuracy: 0.6527 - val_loss: 1.9779 - val_accuracy: 0.4547 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.6722 - accuracy: 0.6770 - val_loss: 1.6200 - val_accuracy: 0.5535 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.3833 - accuracy: 0.6935 - val_loss: 1.4064 - val_accuracy: 0.4886 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.1363 - accuracy: 0.7361 - val_loss: 1.3118 - val_accuracy: 0.5245 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.9277 - accuracy: 0.7902 - val_loss: 0.9632 - val_accuracy: 0.7170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.8016 - accuracy: 0.8095 - val_loss: 0.7847 - val_accuracy: 0.7922 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7142 - accuracy: 0.8214 - val_loss: 0.7166 - val_accuracy: 0.8057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.6486 - accuracy: 0.8365 - val_loss: 0.5785 - val_accuracy: 0.8592 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5849 - accuracy: 0.8478 - val_loss: 0.5748 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5428 - accuracy: 0.8546 - val_loss: 0.5162 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5116 - accuracy: 0.8602 - val_loss: 0.4852 - val_accuracy: 0.8680 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4927 - accuracy: 0.8610 - val_loss: 0.4627 - val_accuracy: 0.8718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4694 - accuracy: 0.8676 - val_loss: 0.4657 - val_accuracy: 0.8634 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4564 - accuracy: 0.8690 - val_loss: 0.4292 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4376 - accuracy: 0.8744 - val_loss: 0.3932 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4329 - accuracy: 0.8744 - val_loss: 0.4244 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4210 - accuracy: 0.8743 - val_loss: 0.3726 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4124 - accuracy: 0.8804 - val_loss: 0.3906 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3997 - accuracy: 0.8791 - val_loss: 0.3717 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3993 - accuracy: 0.8780 - val_loss: 0.4607 - val_accuracy: 0.8536 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3973 - accuracy: 0.8796 - val_loss: 0.3490 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3939 - accuracy: 0.8796 - val_loss: 0.3570 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3857 - accuracy: 0.8856 - val_loss: 0.4101 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3896 - accuracy: 0.8833 - val_loss: 0.3388 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3878 - accuracy: 0.8804 - val_loss: 0.3347 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3791 - accuracy: 0.8837 - val_loss: 0.3334 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3742 - accuracy: 0.8855 - val_loss: 0.3192 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3767 - accuracy: 0.8839 - val_loss: 0.3307 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3783 - accuracy: 0.8830 - val_loss: 0.3425 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3752 - accuracy: 0.8855 - val_loss: 0.3305 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3760 - accuracy: 0.8829 - val_loss: 0.3313 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3670 - accuracy: 0.8848 - val_loss: 0.3258 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3592 - accuracy: 0.8905 - val_loss: 0.3216 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3714 - accuracy: 0.8868 - val_loss: 0.3371 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3720 - accuracy: 0.8861 - val_loss: 0.3230 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3589 - accuracy: 0.8915 - val_loss: 0.3223 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3553 - accuracy: 0.8926 - val_loss: 0.3147 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3627 - accuracy: 0.8882 - val_loss: 0.3163 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3575 - accuracy: 0.8919 - val_loss: 0.3308 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3610 - accuracy: 0.8879 - val_loss: 0.3149 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3586 - accuracy: 0.8899 - val_loss: 0.3185 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3524 - accuracy: 0.8920 - val_loss: 0.3226 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 30ms/step - loss: 0.3580 - accuracy: 0.8903 - val_loss: 0.3181 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3636 - accuracy: 0.8902 - val_loss: 0.3280 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3586 - accuracy: 0.8919 - val_loss: 0.3172 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3533 - accuracy: 0.8922 - val_loss: 0.3118 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3502 - accuracy: 0.8943 - val_loss: 0.3132 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3520 - accuracy: 0.8916 - val_loss: 0.3180 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3562 - accuracy: 0.8897 - val_loss: 0.3303 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3513 - accuracy: 0.8967 - val_loss: 0.3174 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3515 - accuracy: 0.8926 - val_loss: 0.3216 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3601 - accuracy: 0.8922 - val_loss: 0.3180 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3566 - accuracy: 0.8897 - val_loss: 0.3224 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3502 - accuracy: 0.8902 - val_loss: 0.3132 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3480 - accuracy: 0.8946 - val_loss: 0.3089 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3533 - accuracy: 0.8911 - val_loss: 0.3232 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3450 - accuracy: 0.8943 - val_loss: 0.3019 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3514 - accuracy: 0.8938 - val_loss: 0.3052 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3463 - accuracy: 0.8926 - val_loss: 0.3063 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3509 - accuracy: 0.8902 - val_loss: 0.3018 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3455 - accuracy: 0.8909 - val_loss: 0.3134 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3439 - accuracy: 0.8947 - val_loss: 0.3017 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3393 - accuracy: 0.8958 - val_loss: 0.3118 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3434 - accuracy: 0.8940 - val_loss: 0.3280 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3415 - accuracy: 0.8958 - val_loss: 0.3103 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3445 - accuracy: 0.8950 - val_loss: 0.3272 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3459 - accuracy: 0.8935 - val_loss: 0.3366 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3391 - accuracy: 0.8990 - val_loss: 0.3163 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3381 - accuracy: 0.8955 - val_loss: 0.3056 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3279 - accuracy: 0.8996 - val_loss: 0.2949 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3220 - accuracy: 0.8996 - val_loss: 0.2887 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3175 - accuracy: 0.9034 - val_loss: 0.2885 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3151 - accuracy: 0.9024 - val_loss: 0.2832 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3169 - accuracy: 0.8973 - val_loss: 0.2802 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3083 - accuracy: 0.9056 - val_loss: 0.2773 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3115 - accuracy: 0.9045 - val_loss: 0.2753 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2990 - accuracy: 0.9037 - val_loss: 0.2722 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2983 - accuracy: 0.9061 - val_loss: 0.2708 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3020 - accuracy: 0.9009 - val_loss: 0.2702 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3015 - accuracy: 0.9042 - val_loss: 0.2698 - val_accuracy: 0.9166 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2983 - accuracy: 0.9038 - val_loss: 0.2701 - val_accuracy: 0.9178 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2960 - accuracy: 0.9067 - val_loss: 0.2700 - val_accuracy: 0.9172 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3016 - accuracy: 0.9027 - val_loss: 0.2700 - val_accuracy: 0.9176 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2938 - accuracy: 0.9078 - val_loss: 0.2698 - val_accuracy: 0.9172 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2976 - accuracy: 0.9044 - val_loss: 0.2694 - val_accuracy: 0.9166 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2937 - accuracy: 0.9061 - val_loss: 0.2695 - val_accuracy: 0.9164 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2944 - accuracy: 0.9070 - val_loss: 0.2692 - val_accuracy: 0.9168 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2962 - accuracy: 0.9049 - val_loss: 0.2691 - val_accuracy: 0.9164 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2970 - accuracy: 0.9048 - val_loss: 0.2685 - val_accuracy: 0.9168 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2999 - accuracy: 0.9049 - val_loss: 0.2685 - val_accuracy: 0.9166 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2960 - accuracy: 0.9044 - val_loss: 0.2685 - val_accuracy: 0.9162 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3024 - accuracy: 0.9026 - val_loss: 0.2686 - val_accuracy: 0.9168 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2989 - accuracy: 0.9024 - val_loss: 0.2685 - val_accuracy: 0.9166 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3019 - accuracy: 0.9017 - val_loss: 0.2684 - val_accuracy: 0.9166 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3010 - accuracy: 0.9014 - val_loss: 0.2683 - val_accuracy: 0.9166 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2949 - accuracy: 0.9020 - val_loss: 0.2684 - val_accuracy: 0.9170 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2931 - accuracy: 0.9030 - val_loss: 0.2685 - val_accuracy: 0.9164 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2922 - accuracy: 0.9055 - val_loss: 0.2685 - val_accuracy: 0.9164 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2579 - accuracy: 0.9205\n",
      "17/17 [==============================] - 1s 9ms/step\n",
      "TP:5763, TN:9835, FP:620, FN:728, loss0.25790560245513916, acc0.9204532042959991, sn0.8878447080573101, sp0.9406982305117169, f10.8952928382787012, auc0.9704222253510565\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 29ms/step - loss: 3.4252 - accuracy: 0.5929 - val_loss: 3.1022 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.6268 - accuracy: 0.6404 - val_loss: 2.4469 - val_accuracy: 0.4414 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.0752 - accuracy: 0.6684 - val_loss: 1.9665 - val_accuracy: 0.5170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.6662 - accuracy: 0.6898 - val_loss: 1.6057 - val_accuracy: 0.5780 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.3618 - accuracy: 0.7160 - val_loss: 1.2850 - val_accuracy: 0.7234 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.1226 - accuracy: 0.7514 - val_loss: 1.0388 - val_accuracy: 0.7593 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.9517 - accuracy: 0.7719 - val_loss: 0.9273 - val_accuracy: 0.7633 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.8266 - accuracy: 0.7908 - val_loss: 0.7489 - val_accuracy: 0.8205 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7400 - accuracy: 0.8063 - val_loss: 0.6876 - val_accuracy: 0.8267 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.6794 - accuracy: 0.8145 - val_loss: 0.6373 - val_accuracy: 0.8323 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.6251 - accuracy: 0.8308 - val_loss: 0.5727 - val_accuracy: 0.8440 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5766 - accuracy: 0.8375 - val_loss: 0.5242 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5401 - accuracy: 0.8472 - val_loss: 0.6028 - val_accuracy: 0.8363 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5210 - accuracy: 0.8445 - val_loss: 0.4475 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4975 - accuracy: 0.8545 - val_loss: 0.4401 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4753 - accuracy: 0.8595 - val_loss: 0.4197 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4677 - accuracy: 0.8590 - val_loss: 0.3993 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4500 - accuracy: 0.8645 - val_loss: 0.3900 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4424 - accuracy: 0.8654 - val_loss: 0.4212 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4320 - accuracy: 0.8690 - val_loss: 0.3817 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4269 - accuracy: 0.8695 - val_loss: 0.3652 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4194 - accuracy: 0.8717 - val_loss: 0.3548 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4224 - accuracy: 0.8692 - val_loss: 0.3663 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4069 - accuracy: 0.8727 - val_loss: 0.3486 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4073 - accuracy: 0.8720 - val_loss: 0.3482 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4077 - accuracy: 0.8736 - val_loss: 0.3557 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3924 - accuracy: 0.8791 - val_loss: 0.3560 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3888 - accuracy: 0.8761 - val_loss: 0.3381 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3960 - accuracy: 0.8755 - val_loss: 0.3797 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3980 - accuracy: 0.8736 - val_loss: 0.3817 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3881 - accuracy: 0.8767 - val_loss: 0.3452 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3825 - accuracy: 0.8822 - val_loss: 0.3430 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3782 - accuracy: 0.8832 - val_loss: 0.3278 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3899 - accuracy: 0.8765 - val_loss: 0.3281 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3769 - accuracy: 0.8842 - val_loss: 0.3254 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3807 - accuracy: 0.8808 - val_loss: 0.3355 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3896 - accuracy: 0.8755 - val_loss: 0.3304 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3794 - accuracy: 0.8830 - val_loss: 0.3287 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3727 - accuracy: 0.8843 - val_loss: 0.3219 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3722 - accuracy: 0.8850 - val_loss: 0.3390 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3754 - accuracy: 0.8865 - val_loss: 0.3304 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3750 - accuracy: 0.8832 - val_loss: 0.3417 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3710 - accuracy: 0.8859 - val_loss: 0.3503 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3747 - accuracy: 0.8847 - val_loss: 0.3239 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3697 - accuracy: 0.8858 - val_loss: 0.3195 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3710 - accuracy: 0.8858 - val_loss: 0.3209 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3726 - accuracy: 0.8836 - val_loss: 0.3139 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3628 - accuracy: 0.8860 - val_loss: 0.3393 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3690 - accuracy: 0.8873 - val_loss: 0.3285 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3668 - accuracy: 0.8851 - val_loss: 0.3357 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3679 - accuracy: 0.8889 - val_loss: 0.3214 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3712 - accuracy: 0.8847 - val_loss: 0.3305 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3623 - accuracy: 0.8891 - val_loss: 0.3382 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3680 - accuracy: 0.8844 - val_loss: 0.3153 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3651 - accuracy: 0.8876 - val_loss: 0.3234 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3691 - accuracy: 0.8869 - val_loss: 0.3315 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3618 - accuracy: 0.8896 - val_loss: 0.3216 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3688 - accuracy: 0.8890 - val_loss: 0.3274 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3646 - accuracy: 0.8886 - val_loss: 0.3656 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3583 - accuracy: 0.8888 - val_loss: 0.3413 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3574 - accuracy: 0.8900 - val_loss: 0.3131 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3637 - accuracy: 0.8876 - val_loss: 0.3239 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3549 - accuracy: 0.8912 - val_loss: 0.3391 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3581 - accuracy: 0.8903 - val_loss: 0.3347 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3606 - accuracy: 0.8887 - val_loss: 0.3131 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3567 - accuracy: 0.8904 - val_loss: 0.3397 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3542 - accuracy: 0.8908 - val_loss: 0.3171 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3534 - accuracy: 0.8883 - val_loss: 0.3282 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3485 - accuracy: 0.8919 - val_loss: 0.3110 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3496 - accuracy: 0.8958 - val_loss: 0.3338 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3514 - accuracy: 0.8898 - val_loss: 0.3240 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3398 - accuracy: 0.8961 - val_loss: 0.3056 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3325 - accuracy: 0.8958 - val_loss: 0.3011 - val_accuracy: 0.9122 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3317 - accuracy: 0.8972 - val_loss: 0.2951 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3231 - accuracy: 0.8973 - val_loss: 0.2934 - val_accuracy: 0.9128 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3238 - accuracy: 0.8971 - val_loss: 0.2939 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3200 - accuracy: 0.8978 - val_loss: 0.2851 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3132 - accuracy: 0.9008 - val_loss: 0.2873 - val_accuracy: 0.9114 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3156 - accuracy: 0.9016 - val_loss: 0.2834 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3119 - accuracy: 0.8988 - val_loss: 0.2833 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3081 - accuracy: 0.9015 - val_loss: 0.2804 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3070 - accuracy: 0.9021 - val_loss: 0.2801 - val_accuracy: 0.9142 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3090 - accuracy: 0.9016 - val_loss: 0.2809 - val_accuracy: 0.9140 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3074 - accuracy: 0.8991 - val_loss: 0.2804 - val_accuracy: 0.9142 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3034 - accuracy: 0.9032 - val_loss: 0.2802 - val_accuracy: 0.9140 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3105 - accuracy: 0.8986 - val_loss: 0.2802 - val_accuracy: 0.9142 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3082 - accuracy: 0.9018 - val_loss: 0.2799 - val_accuracy: 0.9138 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3091 - accuracy: 0.9004 - val_loss: 0.2793 - val_accuracy: 0.9138 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3070 - accuracy: 0.9039 - val_loss: 0.2784 - val_accuracy: 0.9150 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3022 - accuracy: 0.9044 - val_loss: 0.2792 - val_accuracy: 0.9142 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3031 - accuracy: 0.9050 - val_loss: 0.2784 - val_accuracy: 0.9148 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3028 - accuracy: 0.9002 - val_loss: 0.2787 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3026 - accuracy: 0.9018 - val_loss: 0.2789 - val_accuracy: 0.9138 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3056 - accuracy: 0.9014 - val_loss: 0.2789 - val_accuracy: 0.9136 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3007 - accuracy: 0.9030 - val_loss: 0.2788 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3007 - accuracy: 0.9019 - val_loss: 0.2787 - val_accuracy: 0.9138 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3028 - accuracy: 0.9036 - val_loss: 0.2786 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2994 - accuracy: 0.9029 - val_loss: 0.2789 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3049 - accuracy: 0.9018 - val_loss: 0.2787 - val_accuracy: 0.9144 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2991 - accuracy: 0.9049 - val_loss: 0.2786 - val_accuracy: 0.9140 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2684 - accuracy: 0.9164\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5745, TN:9785, FP:670, FN:746, loss0.26843881607055664, acc0.916440457925174, sn0.8850716376521337, sp0.9359158297465328, f10.8902835890283589, auc0.9696582053317249\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 29ms/step - loss: 3.4220 - accuracy: 0.5996 - val_loss: 3.0810 - val_accuracy: 0.4394 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 2.6368 - accuracy: 0.6451 - val_loss: 2.4820 - val_accuracy: 0.4236 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.0816 - accuracy: 0.6943 - val_loss: 1.8789 - val_accuracy: 0.7351 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.6496 - accuracy: 0.7372 - val_loss: 1.4769 - val_accuracy: 0.7575 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.3358 - accuracy: 0.7631 - val_loss: 1.2041 - val_accuracy: 0.7824 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.1095 - accuracy: 0.7756 - val_loss: 1.0112 - val_accuracy: 0.7892 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.9431 - accuracy: 0.7893 - val_loss: 0.8788 - val_accuracy: 0.7978 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.8255 - accuracy: 0.7991 - val_loss: 0.7741 - val_accuracy: 0.8147 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7357 - accuracy: 0.8125 - val_loss: 0.7024 - val_accuracy: 0.8121 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.6742 - accuracy: 0.8155 - val_loss: 0.6096 - val_accuracy: 0.8406 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.6214 - accuracy: 0.8290 - val_loss: 0.5590 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5864 - accuracy: 0.8319 - val_loss: 0.5317 - val_accuracy: 0.8536 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5422 - accuracy: 0.8486 - val_loss: 0.5096 - val_accuracy: 0.8550 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5233 - accuracy: 0.8471 - val_loss: 0.4966 - val_accuracy: 0.8598 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5005 - accuracy: 0.8490 - val_loss: 0.4483 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4858 - accuracy: 0.8516 - val_loss: 0.4392 - val_accuracy: 0.8680 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4772 - accuracy: 0.8563 - val_loss: 0.4571 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4604 - accuracy: 0.8611 - val_loss: 0.4511 - val_accuracy: 0.8678 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4533 - accuracy: 0.8584 - val_loss: 0.4051 - val_accuracy: 0.8805 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4377 - accuracy: 0.8662 - val_loss: 0.4003 - val_accuracy: 0.8805 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4461 - accuracy: 0.8600 - val_loss: 0.3910 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4313 - accuracy: 0.8632 - val_loss: 0.3696 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4232 - accuracy: 0.8673 - val_loss: 0.3717 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4199 - accuracy: 0.8677 - val_loss: 0.3625 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4170 - accuracy: 0.8690 - val_loss: 0.3553 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4145 - accuracy: 0.8684 - val_loss: 0.3672 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4122 - accuracy: 0.8693 - val_loss: 0.3541 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4041 - accuracy: 0.8731 - val_loss: 0.3400 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4057 - accuracy: 0.8740 - val_loss: 0.3372 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3997 - accuracy: 0.8738 - val_loss: 0.3445 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3982 - accuracy: 0.8727 - val_loss: 0.3457 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3899 - accuracy: 0.8756 - val_loss: 0.3276 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3912 - accuracy: 0.8763 - val_loss: 0.3384 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3922 - accuracy: 0.8725 - val_loss: 0.3596 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3886 - accuracy: 0.8784 - val_loss: 0.3428 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3957 - accuracy: 0.8744 - val_loss: 0.3240 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3843 - accuracy: 0.8768 - val_loss: 0.3361 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3822 - accuracy: 0.8779 - val_loss: 0.3364 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3854 - accuracy: 0.8765 - val_loss: 0.3204 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3760 - accuracy: 0.8815 - val_loss: 0.3272 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3788 - accuracy: 0.8810 - val_loss: 0.3229 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3892 - accuracy: 0.8770 - val_loss: 0.3286 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3718 - accuracy: 0.8857 - val_loss: 0.3281 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3797 - accuracy: 0.8790 - val_loss: 0.3227 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3735 - accuracy: 0.8840 - val_loss: 0.3293 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3719 - accuracy: 0.8794 - val_loss: 0.3371 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3768 - accuracy: 0.8790 - val_loss: 0.3110 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3688 - accuracy: 0.8820 - val_loss: 0.3193 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3664 - accuracy: 0.8814 - val_loss: 0.3223 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3629 - accuracy: 0.8854 - val_loss: 0.3208 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3631 - accuracy: 0.8845 - val_loss: 0.3148 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3682 - accuracy: 0.8787 - val_loss: 0.3197 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3676 - accuracy: 0.8812 - val_loss: 0.3291 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3654 - accuracy: 0.8820 - val_loss: 0.3287 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3631 - accuracy: 0.8849 - val_loss: 0.3147 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3672 - accuracy: 0.8807 - val_loss: 0.3373 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3696 - accuracy: 0.8826 - val_loss: 0.3207 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3650 - accuracy: 0.8879 - val_loss: 0.3198 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3572 - accuracy: 0.8885 - val_loss: 0.3237 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3667 - accuracy: 0.8820 - val_loss: 0.3073 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3643 - accuracy: 0.8873 - val_loss: 0.3128 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3582 - accuracy: 0.8847 - val_loss: 0.3059 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3677 - accuracy: 0.8813 - val_loss: 0.3256 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3662 - accuracy: 0.8831 - val_loss: 0.3321 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3641 - accuracy: 0.8843 - val_loss: 0.3161 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3695 - accuracy: 0.8838 - val_loss: 0.3165 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3616 - accuracy: 0.8867 - val_loss: 0.3087 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3659 - accuracy: 0.8843 - val_loss: 0.3158 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3670 - accuracy: 0.8856 - val_loss: 0.3130 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3627 - accuracy: 0.8879 - val_loss: 0.3227 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3669 - accuracy: 0.8881 - val_loss: 0.3127 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3456 - accuracy: 0.8956 - val_loss: 0.3060 - val_accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3429 - accuracy: 0.8953 - val_loss: 0.3043 - val_accuracy: 0.9073 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3337 - accuracy: 0.8966 - val_loss: 0.3019 - val_accuracy: 0.9099 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3317 - accuracy: 0.8998 - val_loss: 0.2958 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3346 - accuracy: 0.8949 - val_loss: 0.2962 - val_accuracy: 0.9087 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3294 - accuracy: 0.8972 - val_loss: 0.2898 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3223 - accuracy: 0.8978 - val_loss: 0.2881 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3236 - accuracy: 0.8979 - val_loss: 0.2875 - val_accuracy: 0.9099 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3218 - accuracy: 0.8965 - val_loss: 0.2822 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3234 - accuracy: 0.8966 - val_loss: 0.2830 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3183 - accuracy: 0.8979 - val_loss: 0.2858 - val_accuracy: 0.9105 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3137 - accuracy: 0.8983 - val_loss: 0.2858 - val_accuracy: 0.9103 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3225 - accuracy: 0.8943 - val_loss: 0.2861 - val_accuracy: 0.9107 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3201 - accuracy: 0.8978 - val_loss: 0.2866 - val_accuracy: 0.9105 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3162 - accuracy: 0.8968 - val_loss: 0.2856 - val_accuracy: 0.9097 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3128 - accuracy: 0.9008 - val_loss: 0.2860 - val_accuracy: 0.9110 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3132 - accuracy: 0.9002 - val_loss: 0.2855 - val_accuracy: 0.9112 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3167 - accuracy: 0.8981 - val_loss: 0.2868 - val_accuracy: 0.9118 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3149 - accuracy: 0.8961 - val_loss: 0.2853 - val_accuracy: 0.9114 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3140 - accuracy: 0.8983 - val_loss: 0.2857 - val_accuracy: 0.9114 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3130 - accuracy: 0.8993 - val_loss: 0.2862 - val_accuracy: 0.9116 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3183 - accuracy: 0.8943 - val_loss: 0.2861 - val_accuracy: 0.9116 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3118 - accuracy: 0.8992 - val_loss: 0.2855 - val_accuracy: 0.9114 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3140 - accuracy: 0.8977 - val_loss: 0.2860 - val_accuracy: 0.9112 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3131 - accuracy: 0.8996 - val_loss: 0.2863 - val_accuracy: 0.9112 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3097 - accuracy: 0.8999 - val_loss: 0.2859 - val_accuracy: 0.9110 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3142 - accuracy: 0.8997 - val_loss: 0.2863 - val_accuracy: 0.9114 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3160 - accuracy: 0.8967 - val_loss: 0.2865 - val_accuracy: 0.9116 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3118 - accuracy: 0.8986 - val_loss: 0.2861 - val_accuracy: 0.9116 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2876 - accuracy: 0.9110\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5796, TN:9641, FP:814, FN:695, loss0.28764015436172485, acc0.9109524371533105, sn0.8929286704668001, sp0.9221425155428025, f10.8848179528280284, auc0.9662569023761775\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 29ms/step - loss: 3.4845 - accuracy: 0.5854 - val_loss: 3.1682 - val_accuracy: 0.4057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.6886 - accuracy: 0.6295 - val_loss: 2.6445 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.1582 - accuracy: 0.6549 - val_loss: 2.0931 - val_accuracy: 0.4248 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.7721 - accuracy: 0.6767 - val_loss: 1.6921 - val_accuracy: 0.5788 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.4743 - accuracy: 0.6916 - val_loss: 1.4286 - val_accuracy: 0.6145 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.2482 - accuracy: 0.7124 - val_loss: 1.2211 - val_accuracy: 0.6548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.0838 - accuracy: 0.7286 - val_loss: 1.0449 - val_accuracy: 0.7082 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.9534 - accuracy: 0.7471 - val_loss: 0.9278 - val_accuracy: 0.7333 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.8507 - accuracy: 0.7614 - val_loss: 0.8687 - val_accuracy: 0.7339 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7703 - accuracy: 0.7729 - val_loss: 0.7307 - val_accuracy: 0.7784 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7041 - accuracy: 0.7844 - val_loss: 0.7983 - val_accuracy: 0.7176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6529 - accuracy: 0.8017 - val_loss: 0.6163 - val_accuracy: 0.8153 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5935 - accuracy: 0.8180 - val_loss: 0.5672 - val_accuracy: 0.8357 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5716 - accuracy: 0.8259 - val_loss: 0.5657 - val_accuracy: 0.8263 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5377 - accuracy: 0.8379 - val_loss: 0.5435 - val_accuracy: 0.8448 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5066 - accuracy: 0.8504 - val_loss: 0.4302 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4800 - accuracy: 0.8559 - val_loss: 0.4374 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4621 - accuracy: 0.8619 - val_loss: 0.3991 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4429 - accuracy: 0.8719 - val_loss: 0.4024 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4261 - accuracy: 0.8737 - val_loss: 0.3642 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4295 - accuracy: 0.8673 - val_loss: 0.3628 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4074 - accuracy: 0.8772 - val_loss: 0.3583 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4038 - accuracy: 0.8789 - val_loss: 0.3543 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4003 - accuracy: 0.8763 - val_loss: 0.3687 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3947 - accuracy: 0.8820 - val_loss: 0.3849 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3923 - accuracy: 0.8797 - val_loss: 0.3633 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3878 - accuracy: 0.8797 - val_loss: 0.3370 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3777 - accuracy: 0.8830 - val_loss: 0.3572 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3803 - accuracy: 0.8842 - val_loss: 0.3323 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3814 - accuracy: 0.8803 - val_loss: 0.3324 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3710 - accuracy: 0.8826 - val_loss: 0.3753 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3740 - accuracy: 0.8852 - val_loss: 0.3228 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3825 - accuracy: 0.8832 - val_loss: 0.3264 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3725 - accuracy: 0.8861 - val_loss: 0.3283 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3686 - accuracy: 0.8863 - val_loss: 0.3332 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3674 - accuracy: 0.8863 - val_loss: 0.3293 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3660 - accuracy: 0.8840 - val_loss: 0.3254 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3677 - accuracy: 0.8852 - val_loss: 0.3245 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3590 - accuracy: 0.8888 - val_loss: 0.3286 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3652 - accuracy: 0.8884 - val_loss: 0.3356 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3615 - accuracy: 0.8908 - val_loss: 0.3257 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3637 - accuracy: 0.8873 - val_loss: 0.3147 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3685 - accuracy: 0.8875 - val_loss: 0.3212 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3667 - accuracy: 0.8846 - val_loss: 0.3229 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3603 - accuracy: 0.8913 - val_loss: 0.3220 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3621 - accuracy: 0.8887 - val_loss: 0.3317 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3632 - accuracy: 0.8865 - val_loss: 0.3271 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3608 - accuracy: 0.8886 - val_loss: 0.3227 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3557 - accuracy: 0.8918 - val_loss: 0.3334 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3541 - accuracy: 0.8920 - val_loss: 0.3136 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3477 - accuracy: 0.8933 - val_loss: 0.3054 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3511 - accuracy: 0.8897 - val_loss: 0.3067 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3490 - accuracy: 0.8942 - val_loss: 0.3164 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3597 - accuracy: 0.8883 - val_loss: 0.3486 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3476 - accuracy: 0.8956 - val_loss: 0.3204 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3538 - accuracy: 0.8920 - val_loss: 0.3199 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3553 - accuracy: 0.8908 - val_loss: 0.3139 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3498 - accuracy: 0.8879 - val_loss: 0.3511 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3572 - accuracy: 0.8915 - val_loss: 0.3351 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3517 - accuracy: 0.8900 - val_loss: 0.3150 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3495 - accuracy: 0.8919 - val_loss: 0.3273 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3374 - accuracy: 0.8950 - val_loss: 0.2940 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3462 - accuracy: 0.8944 - val_loss: 0.3460 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3433 - accuracy: 0.8925 - val_loss: 0.3193 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3446 - accuracy: 0.8940 - val_loss: 0.3141 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3429 - accuracy: 0.8936 - val_loss: 0.3298 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3428 - accuracy: 0.8931 - val_loss: 0.3022 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3340 - accuracy: 0.8976 - val_loss: 0.3093 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3374 - accuracy: 0.8944 - val_loss: 0.3183 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3438 - accuracy: 0.8955 - val_loss: 0.3011 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3383 - accuracy: 0.8957 - val_loss: 0.3216 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3351 - accuracy: 0.8954 - val_loss: 0.3019 - val_accuracy: 0.9097 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3251 - accuracy: 0.9029 - val_loss: 0.3000 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3261 - accuracy: 0.8981 - val_loss: 0.2892 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3207 - accuracy: 0.9008 - val_loss: 0.2877 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3142 - accuracy: 0.9035 - val_loss: 0.2854 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3124 - accuracy: 0.9021 - val_loss: 0.2844 - val_accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3102 - accuracy: 0.9009 - val_loss: 0.2807 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3046 - accuracy: 0.9041 - val_loss: 0.2816 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3044 - accuracy: 0.9018 - val_loss: 0.2762 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3046 - accuracy: 0.9012 - val_loss: 0.2730 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3029 - accuracy: 0.9068 - val_loss: 0.2755 - val_accuracy: 0.9166 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2993 - accuracy: 0.9073 - val_loss: 0.2772 - val_accuracy: 0.9154 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2982 - accuracy: 0.9067 - val_loss: 0.2759 - val_accuracy: 0.9164 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3027 - accuracy: 0.9071 - val_loss: 0.2762 - val_accuracy: 0.9162 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2983 - accuracy: 0.9046 - val_loss: 0.2759 - val_accuracy: 0.9160 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2969 - accuracy: 0.9047 - val_loss: 0.2762 - val_accuracy: 0.9156 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3008 - accuracy: 0.9032 - val_loss: 0.2755 - val_accuracy: 0.9158 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3003 - accuracy: 0.9049 - val_loss: 0.2751 - val_accuracy: 0.9162 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2977 - accuracy: 0.9049 - val_loss: 0.2745 - val_accuracy: 0.9160 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2949 - accuracy: 0.9088 - val_loss: 0.2741 - val_accuracy: 0.9168 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2974 - accuracy: 0.9051 - val_loss: 0.2745 - val_accuracy: 0.9168 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3009 - accuracy: 0.9030 - val_loss: 0.2752 - val_accuracy: 0.9168 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2979 - accuracy: 0.9057 - val_loss: 0.2751 - val_accuracy: 0.9170 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2967 - accuracy: 0.9053 - val_loss: 0.2753 - val_accuracy: 0.9164 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2955 - accuracy: 0.9067 - val_loss: 0.2753 - val_accuracy: 0.9164 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2956 - accuracy: 0.9051 - val_loss: 0.2751 - val_accuracy: 0.9164 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2998 - accuracy: 0.9031 - val_loss: 0.2750 - val_accuracy: 0.9168 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2952 - accuracy: 0.9087 - val_loss: 0.2749 - val_accuracy: 0.9164 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2953 - accuracy: 0.9061 - val_loss: 0.2750 - val_accuracy: 0.9168 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2636 - accuracy: 0.9195\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5828, TN:9754, FP:701, FN:663, loss0.26355668902397156, acc0.9195090286793344, sn0.8978585734093361, sp0.9329507412721186, f10.8952380952380953, auc0.9720875779810931\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 29ms/step - loss: 3.4511 - accuracy: 0.5811 - val_loss: 3.1528 - val_accuracy: 0.4009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 21ms/step - loss: 2.6461 - accuracy: 0.6278 - val_loss: 2.4597 - val_accuracy: 0.4521 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.0931 - accuracy: 0.6626 - val_loss: 1.9804 - val_accuracy: 0.4932 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.6773 - accuracy: 0.6970 - val_loss: 1.6051 - val_accuracy: 0.6302 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.3499 - accuracy: 0.7413 - val_loss: 1.2998 - val_accuracy: 0.6611 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.1134 - accuracy: 0.7726 - val_loss: 1.0047 - val_accuracy: 0.7900 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.9404 - accuracy: 0.7903 - val_loss: 0.8563 - val_accuracy: 0.8151 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.8250 - accuracy: 0.8025 - val_loss: 0.7323 - val_accuracy: 0.8335 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7257 - accuracy: 0.8189 - val_loss: 0.6498 - val_accuracy: 0.8464 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.6433 - accuracy: 0.8343 - val_loss: 0.5875 - val_accuracy: 0.8568 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5955 - accuracy: 0.8418 - val_loss: 0.5723 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5517 - accuracy: 0.8551 - val_loss: 0.5052 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5150 - accuracy: 0.8618 - val_loss: 0.5178 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4848 - accuracy: 0.8664 - val_loss: 0.4542 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4699 - accuracy: 0.8679 - val_loss: 0.4325 - val_accuracy: 0.8819 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4465 - accuracy: 0.8752 - val_loss: 0.4207 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4397 - accuracy: 0.8745 - val_loss: 0.4047 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4262 - accuracy: 0.8791 - val_loss: 0.3825 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4108 - accuracy: 0.8784 - val_loss: 0.3589 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4106 - accuracy: 0.8779 - val_loss: 0.3826 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4004 - accuracy: 0.8808 - val_loss: 0.3734 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3956 - accuracy: 0.8816 - val_loss: 0.3460 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3909 - accuracy: 0.8843 - val_loss: 0.3409 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3818 - accuracy: 0.8823 - val_loss: 0.3381 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3851 - accuracy: 0.8837 - val_loss: 0.3335 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3748 - accuracy: 0.8879 - val_loss: 0.3244 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3746 - accuracy: 0.8860 - val_loss: 0.3250 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3692 - accuracy: 0.8887 - val_loss: 0.3248 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3660 - accuracy: 0.8888 - val_loss: 0.3341 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3685 - accuracy: 0.8875 - val_loss: 0.3231 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3680 - accuracy: 0.8857 - val_loss: 0.3158 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3517 - accuracy: 0.8908 - val_loss: 0.3374 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3621 - accuracy: 0.8884 - val_loss: 0.3249 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3592 - accuracy: 0.8941 - val_loss: 0.3382 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3565 - accuracy: 0.8928 - val_loss: 0.3291 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3705 - accuracy: 0.8843 - val_loss: 0.3277 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3567 - accuracy: 0.8936 - val_loss: 0.3139 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3562 - accuracy: 0.8908 - val_loss: 0.3281 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3632 - accuracy: 0.8889 - val_loss: 0.3150 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3522 - accuracy: 0.8926 - val_loss: 0.3412 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3600 - accuracy: 0.8906 - val_loss: 0.3197 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3540 - accuracy: 0.8914 - val_loss: 0.3097 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3519 - accuracy: 0.8923 - val_loss: 0.3123 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3456 - accuracy: 0.8986 - val_loss: 0.3209 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3546 - accuracy: 0.8910 - val_loss: 0.3170 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3477 - accuracy: 0.8946 - val_loss: 0.3053 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3534 - accuracy: 0.8945 - val_loss: 0.3090 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3526 - accuracy: 0.8932 - val_loss: 0.3143 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3459 - accuracy: 0.8932 - val_loss: 0.3161 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3484 - accuracy: 0.8911 - val_loss: 0.3154 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 25ms/step - loss: 0.3453 - accuracy: 0.8974 - val_loss: 0.3286 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3458 - accuracy: 0.8935 - val_loss: 0.3267 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3390 - accuracy: 0.8976 - val_loss: 0.3033 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3412 - accuracy: 0.8958 - val_loss: 0.3003 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3389 - accuracy: 0.8974 - val_loss: 0.3050 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 26ms/step - loss: 0.3416 - accuracy: 0.8960 - val_loss: 0.3184 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3352 - accuracy: 0.8968 - val_loss: 0.3161 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3420 - accuracy: 0.8961 - val_loss: 0.2998 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 6s 61ms/step - loss: 0.3353 - accuracy: 0.8980 - val_loss: 0.2944 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 0.3429 - accuracy: 0.8919 - val_loss: 0.3246 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 60ms/step - loss: 0.3361 - accuracy: 0.8956 - val_loss: 0.2997 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 6s 62ms/step - loss: 0.3290 - accuracy: 0.8985 - val_loss: 0.3088 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 6s 61ms/step - loss: 0.3330 - accuracy: 0.8947 - val_loss: 0.2941 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3313 - accuracy: 0.8963 - val_loss: 0.3359 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3318 - accuracy: 0.8970 - val_loss: 0.2976 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 6s 67ms/step - loss: 0.3304 - accuracy: 0.8942 - val_loss: 0.2906 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 6s 63ms/step - loss: 0.3363 - accuracy: 0.8940 - val_loss: 0.2969 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 6s 62ms/step - loss: 0.3385 - accuracy: 0.8955 - val_loss: 0.2925 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 0.3342 - accuracy: 0.8987 - val_loss: 0.2946 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 0.3303 - accuracy: 0.8979 - val_loss: 0.2981 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 28ms/step - loss: 0.3306 - accuracy: 0.8973 - val_loss: 0.3000 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3249 - accuracy: 0.9014 - val_loss: 0.2826 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3123 - accuracy: 0.9063 - val_loss: 0.2801 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3098 - accuracy: 0.9059 - val_loss: 0.2768 - val_accuracy: 0.9162 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3124 - accuracy: 0.9012 - val_loss: 0.2739 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3024 - accuracy: 0.9026 - val_loss: 0.2733 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.2966 - accuracy: 0.9073 - val_loss: 0.2708 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2979 - accuracy: 0.9066 - val_loss: 0.2692 - val_accuracy: 0.9182 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2941 - accuracy: 0.9042 - val_loss: 0.2679 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2921 - accuracy: 0.9033 - val_loss: 0.2664 - val_accuracy: 0.9162 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2929 - accuracy: 0.9069 - val_loss: 0.2636 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2879 - accuracy: 0.9061 - val_loss: 0.2630 - val_accuracy: 0.9176 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2891 - accuracy: 0.9093 - val_loss: 0.2631 - val_accuracy: 0.9178 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.2885 - accuracy: 0.9075 - val_loss: 0.2627 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2903 - accuracy: 0.9055 - val_loss: 0.2625 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2889 - accuracy: 0.9082 - val_loss: 0.2627 - val_accuracy: 0.9166 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2931 - accuracy: 0.9041 - val_loss: 0.2625 - val_accuracy: 0.9170 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2913 - accuracy: 0.9067 - val_loss: 0.2618 - val_accuracy: 0.9162 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2888 - accuracy: 0.9067 - val_loss: 0.2618 - val_accuracy: 0.9166 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2865 - accuracy: 0.9103 - val_loss: 0.2618 - val_accuracy: 0.9160 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2850 - accuracy: 0.9091 - val_loss: 0.2615 - val_accuracy: 0.9174 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2836 - accuracy: 0.9073 - val_loss: 0.2617 - val_accuracy: 0.9174 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2814 - accuracy: 0.9100 - val_loss: 0.2619 - val_accuracy: 0.9170 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2910 - accuracy: 0.9071 - val_loss: 0.2619 - val_accuracy: 0.9172 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2873 - accuracy: 0.9071 - val_loss: 0.2615 - val_accuracy: 0.9176 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2808 - accuracy: 0.9071 - val_loss: 0.2616 - val_accuracy: 0.9172 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.2868 - accuracy: 0.9094 - val_loss: 0.2617 - val_accuracy: 0.9174 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2842 - accuracy: 0.9115 - val_loss: 0.2618 - val_accuracy: 0.9168 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2886 - accuracy: 0.9056 - val_loss: 0.2616 - val_accuracy: 0.9174 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.2875 - accuracy: 0.9085 - val_loss: 0.2616 - val_accuracy: 0.9170 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2502 - accuracy: 0.9220\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5755, TN:9871, FP:584, FN:736, loss0.25022315979003906, acc0.9221055116251623, sn0.8866122323216762, sp0.9441415590626494, f10.897116134060795, auc0.9718822316681576\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 13s 29ms/step - loss: 3.3641 - accuracy: 0.5918 - val_loss: 3.0321 - val_accuracy: 0.4130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 2.5603 - accuracy: 0.6374 - val_loss: 2.4266 - val_accuracy: 0.4011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.9890 - accuracy: 0.6762 - val_loss: 1.8735 - val_accuracy: 0.4587 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 1.5562 - accuracy: 0.7241 - val_loss: 1.4246 - val_accuracy: 0.7335 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 1.2355 - accuracy: 0.7660 - val_loss: 1.1604 - val_accuracy: 0.7481 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 1.0248 - accuracy: 0.7801 - val_loss: 0.9275 - val_accuracy: 0.7876 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.8796 - accuracy: 0.7925 - val_loss: 0.8028 - val_accuracy: 0.8036 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.7778 - accuracy: 0.7993 - val_loss: 0.6962 - val_accuracy: 0.8313 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.7039 - accuracy: 0.8105 - val_loss: 0.6364 - val_accuracy: 0.8339 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.6476 - accuracy: 0.8225 - val_loss: 0.5897 - val_accuracy: 0.8436 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5985 - accuracy: 0.8342 - val_loss: 0.5868 - val_accuracy: 0.8297 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5598 - accuracy: 0.8423 - val_loss: 0.5242 - val_accuracy: 0.8512 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.5326 - accuracy: 0.8456 - val_loss: 0.4703 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.5047 - accuracy: 0.8542 - val_loss: 0.4643 - val_accuracy: 0.8714 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4941 - accuracy: 0.8504 - val_loss: 0.4383 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4715 - accuracy: 0.8614 - val_loss: 0.4123 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4648 - accuracy: 0.8587 - val_loss: 0.4034 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4569 - accuracy: 0.8584 - val_loss: 0.4653 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4435 - accuracy: 0.8632 - val_loss: 0.4414 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4291 - accuracy: 0.8661 - val_loss: 0.3857 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4295 - accuracy: 0.8676 - val_loss: 0.4135 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4193 - accuracy: 0.8707 - val_loss: 0.4093 - val_accuracy: 0.8726 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4227 - accuracy: 0.8673 - val_loss: 0.3872 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4126 - accuracy: 0.8702 - val_loss: 0.4376 - val_accuracy: 0.8606 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.4151 - accuracy: 0.8707 - val_loss: 0.3669 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4047 - accuracy: 0.8714 - val_loss: 0.3539 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4047 - accuracy: 0.8752 - val_loss: 0.3608 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.4046 - accuracy: 0.8722 - val_loss: 0.3635 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3936 - accuracy: 0.8776 - val_loss: 0.3446 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3948 - accuracy: 0.8733 - val_loss: 0.3755 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3923 - accuracy: 0.8780 - val_loss: 0.3693 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3919 - accuracy: 0.8755 - val_loss: 0.3491 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3959 - accuracy: 0.8752 - val_loss: 0.3426 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3888 - accuracy: 0.8803 - val_loss: 0.3415 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3843 - accuracy: 0.8790 - val_loss: 0.3355 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3897 - accuracy: 0.8786 - val_loss: 0.3660 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3879 - accuracy: 0.8788 - val_loss: 0.3294 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3801 - accuracy: 0.8832 - val_loss: 0.3486 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3807 - accuracy: 0.8807 - val_loss: 0.3434 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3797 - accuracy: 0.8805 - val_loss: 0.3193 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3787 - accuracy: 0.8804 - val_loss: 0.3255 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3769 - accuracy: 0.8860 - val_loss: 0.3153 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3778 - accuracy: 0.8775 - val_loss: 0.3427 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3682 - accuracy: 0.8843 - val_loss: 0.3222 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3671 - accuracy: 0.8861 - val_loss: 0.3343 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3704 - accuracy: 0.8857 - val_loss: 0.3261 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3642 - accuracy: 0.8869 - val_loss: 0.3364 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3795 - accuracy: 0.8825 - val_loss: 0.3271 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3717 - accuracy: 0.8849 - val_loss: 0.3199 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3616 - accuracy: 0.8901 - val_loss: 0.3216 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3655 - accuracy: 0.8843 - val_loss: 0.3381 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3697 - accuracy: 0.8823 - val_loss: 0.3413 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3651 - accuracy: 0.8861 - val_loss: 0.3303 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3595 - accuracy: 0.8853 - val_loss: 0.3124 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3629 - accuracy: 0.8869 - val_loss: 0.3298 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3629 - accuracy: 0.8843 - val_loss: 0.3176 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3668 - accuracy: 0.8877 - val_loss: 0.3226 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3642 - accuracy: 0.8895 - val_loss: 0.3504 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3618 - accuracy: 0.8891 - val_loss: 0.3199 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3631 - accuracy: 0.8907 - val_loss: 0.3194 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3548 - accuracy: 0.8934 - val_loss: 0.3431 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3512 - accuracy: 0.8932 - val_loss: 0.3325 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3558 - accuracy: 0.8891 - val_loss: 0.3236 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3588 - accuracy: 0.8872 - val_loss: 0.3172 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3570 - accuracy: 0.8907 - val_loss: 0.3249 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3559 - accuracy: 0.8902 - val_loss: 0.3151 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3603 - accuracy: 0.8915 - val_loss: 0.3181 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3579 - accuracy: 0.8927 - val_loss: 0.3184 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3510 - accuracy: 0.8905 - val_loss: 0.3110 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3533 - accuracy: 0.8938 - val_loss: 0.3373 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3590 - accuracy: 0.8897 - val_loss: 0.3150 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3416 - accuracy: 0.8984 - val_loss: 0.3132 - val_accuracy: 0.9093 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3318 - accuracy: 0.9005 - val_loss: 0.3093 - val_accuracy: 0.9091 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3317 - accuracy: 0.8984 - val_loss: 0.3034 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3239 - accuracy: 0.9024 - val_loss: 0.2994 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3220 - accuracy: 0.9021 - val_loss: 0.2997 - val_accuracy: 0.9128 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3208 - accuracy: 0.9005 - val_loss: 0.2959 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3184 - accuracy: 0.9002 - val_loss: 0.2945 - val_accuracy: 0.9122 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3119 - accuracy: 0.9038 - val_loss: 0.2942 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3136 - accuracy: 0.9022 - val_loss: 0.2879 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3106 - accuracy: 0.8988 - val_loss: 0.2845 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3116 - accuracy: 0.9029 - val_loss: 0.2875 - val_accuracy: 0.9146 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3085 - accuracy: 0.9044 - val_loss: 0.2880 - val_accuracy: 0.9150 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3048 - accuracy: 0.9053 - val_loss: 0.2884 - val_accuracy: 0.9146 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 2s 24ms/step - loss: 0.3047 - accuracy: 0.9030 - val_loss: 0.2894 - val_accuracy: 0.9140 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3068 - accuracy: 0.9016 - val_loss: 0.2881 - val_accuracy: 0.9148 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3097 - accuracy: 0.9046 - val_loss: 0.2883 - val_accuracy: 0.9144 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3056 - accuracy: 0.9045 - val_loss: 0.2873 - val_accuracy: 0.9148 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3104 - accuracy: 0.9013 - val_loss: 0.2880 - val_accuracy: 0.9148 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3121 - accuracy: 0.8992 - val_loss: 0.2876 - val_accuracy: 0.9142 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3041 - accuracy: 0.9020 - val_loss: 0.2869 - val_accuracy: 0.9152 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3053 - accuracy: 0.9067 - val_loss: 0.2871 - val_accuracy: 0.9148 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3056 - accuracy: 0.9018 - val_loss: 0.2872 - val_accuracy: 0.9148 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3097 - accuracy: 0.8993 - val_loss: 0.2875 - val_accuracy: 0.9146 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3079 - accuracy: 0.9009 - val_loss: 0.2874 - val_accuracy: 0.9146 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3075 - accuracy: 0.9008 - val_loss: 0.2873 - val_accuracy: 0.9148 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 2s 22ms/step - loss: 0.3129 - accuracy: 0.9001 - val_loss: 0.2871 - val_accuracy: 0.9148 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3022 - accuracy: 0.9031 - val_loss: 0.2877 - val_accuracy: 0.9146 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3048 - accuracy: 0.9029 - val_loss: 0.2875 - val_accuracy: 0.9148 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 2s 23ms/step - loss: 0.3064 - accuracy: 0.9016 - val_loss: 0.2876 - val_accuracy: 0.9146 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 3s 5ms/step - loss: 0.2747 - accuracy: 0.9185\n",
      "17/17 [==============================] - 1s 8ms/step\n",
      "TP:5884, TN:9681, FP:774, FN:607, loss0.27470797300338745, acc0.9185058420866281, sn0.9064859035587737, sp0.9259684361549498, f10.8949730017491824, auc0.9714075649460854\n",
      "Average Test loss:  0.26815750300884245\n",
      "Average Accuracy:  0.918004248790275\n",
      "Average Sensitivity:  0.8916499768910799\n",
      "Average Specificity:  0.9343663318986131\n",
      "Average F1 Score:  0.8928356297253692\n",
      "Average AUC Score:  0.9699841785127049\n",
      "AUC for ROC curve 1: 0.9682\n",
      "AUC for ROC curve 2: 0.9682\n",
      "AUC for ROC curve 3: 0.9683\n",
      "AUC for ROC curve 4: 0.9683\n",
      "AUC for ROC curve 5: 0.9703\n",
      "AUC for ROC curve 6: 0.9703\n",
      "AUC for ROC curve 7: 0.9713\n",
      "AUC for ROC curve 8: 0.9713\n",
      "AUC for ROC curve 9: 0.9704\n",
      "AUC for ROC curve 10: 0.9704\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f9963-cfa7-47f5-8701-2a24e58e20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lxx",
   "language": "python",
   "name": "lxx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
