{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b497ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5596fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data(path):\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, path), 'rb') as f:  # read preprocessing result\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        sample = o_train[i]  \n",
    "        rri_tm, rri_signal = sample[\"rri\"]          \n",
    "        ampl_tm, ampl_signal = sample[\"amplitude\"]   \n",
    "        hrv_features = sample[\"hrv\"]                 \n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)       \n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_signal), k=3), ext=1)\n",
    "\n",
    "        combined_5min = np.stack([                     \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_hf\"]),   \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_vlf\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_hfn\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_lfn\"]),  \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_lf\"]),    \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sd2\"]),        \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_lfhf\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_meannn\"]),  \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_tp\"]),    \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sdnn\"]),        \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sd1sd2\"]),      \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_mediannn\"]), \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_minnn\"]),    \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_cvi\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_csi\"]),      \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_pnn20\"]),  \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_pnn50\"]),\n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sdsd\"]) ,\n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_rmssd\"]),            \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sd1\"]) \n",
    "        ], axis=-1)  \n",
    "\n",
    "        x_train1.append(combined_5min)              \n",
    "        x_train2.append(combined_5min[180:720])     \n",
    "        x_train3.append(combined_5min[360:540])     \n",
    "        \n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num = [i for i in range(16709)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        sample = o_test[i]  \n",
    "        rri_tm, rri_signal = sample[\"rri\"]          \n",
    "        ampl_tm, ampl_signal = sample[\"amplitude\"]   \n",
    "        hrv_features = sample[\"hrv\"]                 \n",
    "\n",
    "        rri_interp_signal = splev(tm, splrep(rri_tm, scaler(rri_signal), k=3), ext=1)\n",
    "        ampl_interp_signal = splev(tm, splrep(ampl_tm, scaler(ampl_signal), k=3), ext=1)\n",
    "        \n",
    "        combined_5min = np.stack([                        \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_hf\"]),   \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_vlf\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_hfn\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_lfn\"]),  \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_lf\"]),    \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sd2\"]),        \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_lfhf\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_meannn\"]),  \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_tp\"]),    \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sdnn\"]),        \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sd1sd2\"]),      \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_mediannn\"]), \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_minnn\"]),    \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_cvi\"]),       \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_csi\"]),      \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_pnn20\"]),  \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_pnn50\"]),\n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sdsd\"]) ,\n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_rmssd\"]),            \n",
    "            np.full_like(rri_interp_signal, hrv_features[\"hrv_sd1\"]) \n",
    "        ], axis=-1)  \n",
    "\n",
    "        x_test1.append(combined_5min)              \n",
    "        x_test2.append(combined_5min[180:720])     \n",
    "        x_test3.append(combined_5min[360:540])    \n",
    "        \n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, \\\n",
    "           x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=1, keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=1, keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b2aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ecfbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)    \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "\n",
    "    x1 = BatchNormalization()(input1)    \n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)  \n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "\n",
    "    x2 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    \n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    # CNN-3\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "  \n",
    "    x3 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    attention1 = ResidualAttentionBlock(256, 256)\n",
    "    attention2 = ResidualAttentionBlock(64, 64)\n",
    "    x1 = attention1(x1)\n",
    "    x2 = attention1(x2)\n",
    "    x3 = attention2(x3)\n",
    " \n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c17ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee9087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c935cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "    path = \"apnea-ecg-hrv(all).pkl\"\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2,\\\n",
    "    x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test = load_data(path)\n",
    "    \n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  \n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea2f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1 = np.transpose(x_train1, (0, 2, 1))  \n",
    "x_train2 = np.transpose(x_train2, (0, 2, 1))  \n",
    "x_train3 = np.transpose(x_train3, (0, 2, 1))  \n",
    "\n",
    "x_val1 = np.transpose(x_val1, (0, 2, 1))\n",
    "x_val2 = np.transpose(x_val2, (0, 2, 1))\n",
    "x_val3 = np.transpose(x_val3, (0, 2, 1))\n",
    "\n",
    "x_test1 = np.transpose(x_test1, (0, 2, 1))\n",
    "x_test2 = np.transpose(x_test2, (0, 2, 1))\n",
    "x_test3 = np.transpose(x_test3, (0, 2, 1))\n",
    "print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "\n",
    "num_trainings = 10\n",
    "\n",
    "for _ in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN) \n",
    "    sp = TN / (TN + FP)  \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    auc_score = roc_auc_score(y_true, y_prob[:, 1])\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{},auc{}\".format(TP, TN, FP, FN,loss, acc, sn, sp, f1,auc_score))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(auc_score)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = 0.0\n",
    "for i in range(num_trainings):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
    "    plt.plot(fpr, tpr, alpha=0.3, label='ROC curve {}'.format(i+1))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = %0.2f)' % mean_auc, lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
