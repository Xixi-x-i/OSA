{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 13:18:31.241801: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = BatchNormalization()(input1)    \n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "    x1 = Conv1D(256, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "\n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "    x2 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    \n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    \n",
    "    \n",
    "\n",
    "    # CNN-3\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "    x3 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  \n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 31s 70ms/step - loss: 3.9581 - accuracy: 0.6395 - val_loss: 3.5438 - val_accuracy: 0.4069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 2.5334 - accuracy: 0.7923 - val_loss: 2.4946 - val_accuracy: 0.4631 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 1.6940 - accuracy: 0.8346 - val_loss: 1.5447 - val_accuracy: 0.7316 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.1976 - accuracy: 0.8577 - val_loss: 1.1043 - val_accuracy: 0.8379 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.9253 - accuracy: 0.8696 - val_loss: 0.8292 - val_accuracy: 0.8730 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.7609 - accuracy: 0.8810 - val_loss: 0.6761 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.6549 - accuracy: 0.8853 - val_loss: 0.6207 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.5970 - accuracy: 0.8878 - val_loss: 0.5775 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.5429 - accuracy: 0.8911 - val_loss: 0.4966 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.5121 - accuracy: 0.8973 - val_loss: 0.5173 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.4944 - accuracy: 0.9002 - val_loss: 0.5399 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4681 - accuracy: 0.8999 - val_loss: 0.4480 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4580 - accuracy: 0.8977 - val_loss: 0.4188 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4424 - accuracy: 0.9049 - val_loss: 0.4302 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.4354 - accuracy: 0.9020 - val_loss: 0.4315 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4309 - accuracy: 0.9039 - val_loss: 0.4872 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4209 - accuracy: 0.9065 - val_loss: 0.4220 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4138 - accuracy: 0.9067 - val_loss: 0.3961 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4227 - accuracy: 0.9056 - val_loss: 0.4065 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4168 - accuracy: 0.9059 - val_loss: 0.4079 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4082 - accuracy: 0.9046 - val_loss: 0.3980 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4230 - accuracy: 0.9006 - val_loss: 0.3835 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3956 - accuracy: 0.9091 - val_loss: 0.4106 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4031 - accuracy: 0.9055 - val_loss: 0.3856 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4003 - accuracy: 0.9070 - val_loss: 0.4365 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4045 - accuracy: 0.9062 - val_loss: 0.4084 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3998 - accuracy: 0.9102 - val_loss: 0.4006 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3929 - accuracy: 0.9093 - val_loss: 0.4094 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3958 - accuracy: 0.9080 - val_loss: 0.3761 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3924 - accuracy: 0.9091 - val_loss: 0.3828 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3968 - accuracy: 0.9086 - val_loss: 0.4137 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3951 - accuracy: 0.9082 - val_loss: 0.4044 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4017 - accuracy: 0.9076 - val_loss: 0.3909 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3922 - accuracy: 0.9079 - val_loss: 0.4096 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3855 - accuracy: 0.9096 - val_loss: 0.4111 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3867 - accuracy: 0.9098 - val_loss: 0.3800 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3766 - accuracy: 0.9107 - val_loss: 0.4061 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3921 - accuracy: 0.9086 - val_loss: 0.3692 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3855 - accuracy: 0.9087 - val_loss: 0.3858 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3807 - accuracy: 0.9115 - val_loss: 0.4520 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3828 - accuracy: 0.9065 - val_loss: 0.4314 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3776 - accuracy: 0.9132 - val_loss: 0.3676 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3891 - accuracy: 0.9086 - val_loss: 0.3757 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3848 - accuracy: 0.9097 - val_loss: 0.3857 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3798 - accuracy: 0.9116 - val_loss: 0.3761 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3786 - accuracy: 0.9113 - val_loss: 0.3745 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3808 - accuracy: 0.9134 - val_loss: 0.3794 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3838 - accuracy: 0.9120 - val_loss: 0.3657 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3822 - accuracy: 0.9106 - val_loss: 0.3786 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3751 - accuracy: 0.9118 - val_loss: 0.3515 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3666 - accuracy: 0.9146 - val_loss: 0.3804 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3853 - accuracy: 0.9094 - val_loss: 0.3698 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3703 - accuracy: 0.9139 - val_loss: 0.3841 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3794 - accuracy: 0.9068 - val_loss: 0.4332 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3694 - accuracy: 0.9111 - val_loss: 0.3546 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3746 - accuracy: 0.9099 - val_loss: 0.4032 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3605 - accuracy: 0.9111 - val_loss: 0.4379 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3728 - accuracy: 0.9072 - val_loss: 0.4031 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3656 - accuracy: 0.9131 - val_loss: 0.3809 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3752 - accuracy: 0.9106 - val_loss: 0.3843 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3722 - accuracy: 0.9092 - val_loss: 0.3526 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3624 - accuracy: 0.9177 - val_loss: 0.3582 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3674 - accuracy: 0.9084 - val_loss: 0.3701 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3693 - accuracy: 0.9101 - val_loss: 0.3658 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3635 - accuracy: 0.9141 - val_loss: 0.3695 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3724 - accuracy: 0.9125 - val_loss: 0.3681 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3731 - accuracy: 0.9112 - val_loss: 0.3678 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3712 - accuracy: 0.9131 - val_loss: 0.3603 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3711 - accuracy: 0.9115 - val_loss: 0.3640 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3707 - accuracy: 0.9097 - val_loss: 0.3471 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3631 - accuracy: 0.9135 - val_loss: 0.3608 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3379 - accuracy: 0.9207 - val_loss: 0.3492 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3161 - accuracy: 0.9255 - val_loss: 0.3464 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3124 - accuracy: 0.9252 - val_loss: 0.3226 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3024 - accuracy: 0.9256 - val_loss: 0.3222 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2936 - accuracy: 0.9270 - val_loss: 0.2972 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2900 - accuracy: 0.9263 - val_loss: 0.3006 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2808 - accuracy: 0.9266 - val_loss: 0.2873 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2765 - accuracy: 0.9279 - val_loss: 0.2840 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2751 - accuracy: 0.9258 - val_loss: 0.2817 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2667 - accuracy: 0.9291 - val_loss: 0.2761 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2564 - accuracy: 0.9292 - val_loss: 0.2761 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2597 - accuracy: 0.9321 - val_loss: 0.2758 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2559 - accuracy: 0.9316 - val_loss: 0.2761 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2543 - accuracy: 0.9342 - val_loss: 0.2765 - val_accuracy: 0.9256 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2579 - accuracy: 0.9310 - val_loss: 0.2755 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2560 - accuracy: 0.9323 - val_loss: 0.2743 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2552 - accuracy: 0.9309 - val_loss: 0.2739 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2539 - accuracy: 0.9322 - val_loss: 0.2735 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2501 - accuracy: 0.9346 - val_loss: 0.2732 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2523 - accuracy: 0.9337 - val_loss: 0.2729 - val_accuracy: 0.9256 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2490 - accuracy: 0.9357 - val_loss: 0.2738 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2534 - accuracy: 0.9323 - val_loss: 0.2738 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2532 - accuracy: 0.9338 - val_loss: 0.2743 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2470 - accuracy: 0.9360 - val_loss: 0.2743 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2487 - accuracy: 0.9324 - val_loss: 0.2743 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2499 - accuracy: 0.9332 - val_loss: 0.2744 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2495 - accuracy: 0.9347 - val_loss: 0.2743 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2493 - accuracy: 0.9342 - val_loss: 0.2739 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2510 - accuracy: 0.9340 - val_loss: 0.2737 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 15ms/step - loss: 0.2804 - accuracy: 0.9259\n",
      "17/17 [==============================] - 2s 41ms/step\n",
      "TP:5815, TN:9876, FP:579, FN:676, loss0.28044670820236206, acc0.9259412250678626, sn0.8958558003389309, sp0.9446197991391678, f10.9025999223903765, auc0.9734577125919338\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 29s 65ms/step - loss: 3.8284 - accuracy: 0.6876 - val_loss: 4.5520 - val_accuracy: 0.3895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 2.4637 - accuracy: 0.8207 - val_loss: 2.0002 - val_accuracy: 0.8315 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 1.6416 - accuracy: 0.8502 - val_loss: 1.4002 - val_accuracy: 0.8404 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 1.1710 - accuracy: 0.8667 - val_loss: 1.1266 - val_accuracy: 0.8165 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.8983 - accuracy: 0.8728 - val_loss: 0.8109 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.7277 - accuracy: 0.8860 - val_loss: 0.8523 - val_accuracy: 0.8135 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.6283 - accuracy: 0.8881 - val_loss: 0.5851 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5659 - accuracy: 0.8947 - val_loss: 0.6041 - val_accuracy: 0.8751 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.5212 - accuracy: 0.8988 - val_loss: 0.4809 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4951 - accuracy: 0.8997 - val_loss: 0.5015 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4695 - accuracy: 0.9035 - val_loss: 0.4788 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4750 - accuracy: 0.8966 - val_loss: 0.4860 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4531 - accuracy: 0.9002 - val_loss: 0.4157 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4313 - accuracy: 0.9032 - val_loss: 0.4131 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4237 - accuracy: 0.9020 - val_loss: 0.4185 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4158 - accuracy: 0.9046 - val_loss: 0.4033 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4213 - accuracy: 0.9026 - val_loss: 0.4364 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4186 - accuracy: 0.9039 - val_loss: 0.4464 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4020 - accuracy: 0.9062 - val_loss: 0.3873 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4018 - accuracy: 0.9068 - val_loss: 0.3968 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.4030 - accuracy: 0.9075 - val_loss: 0.4043 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4021 - accuracy: 0.9054 - val_loss: 0.3942 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.4030 - accuracy: 0.9076 - val_loss: 0.3876 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4083 - accuracy: 0.9043 - val_loss: 0.3753 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3945 - accuracy: 0.9091 - val_loss: 0.3785 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3920 - accuracy: 0.9075 - val_loss: 0.3861 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3990 - accuracy: 0.9062 - val_loss: 0.4860 - val_accuracy: 0.8648 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3971 - accuracy: 0.9084 - val_loss: 0.4881 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3979 - accuracy: 0.9111 - val_loss: 0.3970 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3930 - accuracy: 0.9084 - val_loss: 0.3766 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3880 - accuracy: 0.9102 - val_loss: 0.3720 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3908 - accuracy: 0.9097 - val_loss: 0.3799 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3904 - accuracy: 0.9080 - val_loss: 0.4184 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3916 - accuracy: 0.9079 - val_loss: 0.3888 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3976 - accuracy: 0.9059 - val_loss: 0.4109 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3819 - accuracy: 0.9089 - val_loss: 0.4716 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3883 - accuracy: 0.9058 - val_loss: 0.3656 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3881 - accuracy: 0.9072 - val_loss: 0.4006 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3868 - accuracy: 0.9080 - val_loss: 0.3772 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3874 - accuracy: 0.9077 - val_loss: 0.3908 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3822 - accuracy: 0.9093 - val_loss: 0.3940 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3883 - accuracy: 0.9077 - val_loss: 0.4146 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3812 - accuracy: 0.9123 - val_loss: 0.3778 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3861 - accuracy: 0.9094 - val_loss: 0.3788 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3942 - accuracy: 0.9066 - val_loss: 0.3682 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3766 - accuracy: 0.9120 - val_loss: 0.3828 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3801 - accuracy: 0.9115 - val_loss: 0.3734 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3813 - accuracy: 0.9125 - val_loss: 0.3970 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3746 - accuracy: 0.9108 - val_loss: 0.4367 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3773 - accuracy: 0.9127 - val_loss: 0.4393 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3775 - accuracy: 0.9100 - val_loss: 0.4077 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3808 - accuracy: 0.9085 - val_loss: 0.4097 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3711 - accuracy: 0.9108 - val_loss: 0.3882 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3790 - accuracy: 0.9088 - val_loss: 0.3841 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3733 - accuracy: 0.9091 - val_loss: 0.3852 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3720 - accuracy: 0.9129 - val_loss: 0.4035 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3724 - accuracy: 0.9132 - val_loss: 0.3584 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3725 - accuracy: 0.9125 - val_loss: 0.3863 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3866 - accuracy: 0.9105 - val_loss: 0.3624 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3755 - accuracy: 0.9100 - val_loss: 0.3605 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3716 - accuracy: 0.9094 - val_loss: 0.5810 - val_accuracy: 0.8387 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3757 - accuracy: 0.9098 - val_loss: 0.3569 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3720 - accuracy: 0.9116 - val_loss: 0.3667 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3750 - accuracy: 0.9100 - val_loss: 0.3914 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3596 - accuracy: 0.9152 - val_loss: 0.3850 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3716 - accuracy: 0.9109 - val_loss: 0.3840 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3694 - accuracy: 0.9124 - val_loss: 0.3496 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3610 - accuracy: 0.9108 - val_loss: 0.3705 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3622 - accuracy: 0.9145 - val_loss: 0.4288 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3651 - accuracy: 0.9132 - val_loss: 0.3758 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3698 - accuracy: 0.9126 - val_loss: 0.3629 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3437 - accuracy: 0.9204 - val_loss: 0.3508 - val_accuracy: 0.9158 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3282 - accuracy: 0.9216 - val_loss: 0.3524 - val_accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3139 - accuracy: 0.9250 - val_loss: 0.3237 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3006 - accuracy: 0.9262 - val_loss: 0.3319 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3003 - accuracy: 0.9241 - val_loss: 0.3214 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2870 - accuracy: 0.9296 - val_loss: 0.3104 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2875 - accuracy: 0.9260 - val_loss: 0.2898 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2800 - accuracy: 0.9285 - val_loss: 0.2886 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2778 - accuracy: 0.9282 - val_loss: 0.2954 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2702 - accuracy: 0.9308 - val_loss: 0.2890 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2601 - accuracy: 0.9328 - val_loss: 0.2799 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2573 - accuracy: 0.9344 - val_loss: 0.2790 - val_accuracy: 0.9266 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2570 - accuracy: 0.9328 - val_loss: 0.2797 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2601 - accuracy: 0.9326 - val_loss: 0.2777 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2590 - accuracy: 0.9329 - val_loss: 0.2782 - val_accuracy: 0.9268 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2583 - accuracy: 0.9317 - val_loss: 0.2792 - val_accuracy: 0.9272 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2571 - accuracy: 0.9359 - val_loss: 0.2784 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2578 - accuracy: 0.9311 - val_loss: 0.2763 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2574 - accuracy: 0.9316 - val_loss: 0.2767 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2586 - accuracy: 0.9318 - val_loss: 0.2779 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2532 - accuracy: 0.9341 - val_loss: 0.2770 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2539 - accuracy: 0.9326 - val_loss: 0.2769 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2544 - accuracy: 0.9341 - val_loss: 0.2766 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.2531 - accuracy: 0.9323 - val_loss: 0.2768 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2534 - accuracy: 0.9320 - val_loss: 0.2764 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2527 - accuracy: 0.9327 - val_loss: 0.2763 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2531 - accuracy: 0.9326 - val_loss: 0.2762 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2506 - accuracy: 0.9351 - val_loss: 0.2764 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2524 - accuracy: 0.9346 - val_loss: 0.2762 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2825 - accuracy: 0.9258\n",
      "17/17 [==============================] - 1s 25ms/step\n",
      "TP:5819, TN:9869, FP:586, FN:672, loss0.2824680209159851, acc0.925764192139738, sn0.8964720382067478, sp0.9439502630320421, f10.9024503722084367, auc0.9733347302570509\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 30s 68ms/step - loss: 3.9066 - accuracy: 0.6795 - val_loss: 4.3138 - val_accuracy: 0.3997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 2.5571 - accuracy: 0.8116 - val_loss: 2.2619 - val_accuracy: 0.6677 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 1.7242 - accuracy: 0.8444 - val_loss: 1.5329 - val_accuracy: 0.8081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 1.2214 - accuracy: 0.8649 - val_loss: 1.0426 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.9349 - accuracy: 0.8746 - val_loss: 0.8210 - val_accuracy: 0.8785 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.7499 - accuracy: 0.8886 - val_loss: 0.7303 - val_accuracy: 0.8694 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.6520 - accuracy: 0.8879 - val_loss: 0.7244 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.5771 - accuracy: 0.8930 - val_loss: 0.5654 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.5284 - accuracy: 0.8982 - val_loss: 0.4902 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4950 - accuracy: 0.8973 - val_loss: 0.4830 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4800 - accuracy: 0.8974 - val_loss: 0.4739 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4590 - accuracy: 0.8985 - val_loss: 0.4285 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.4404 - accuracy: 0.9031 - val_loss: 0.6271 - val_accuracy: 0.8297 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4373 - accuracy: 0.9006 - val_loss: 0.4085 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.4279 - accuracy: 0.9044 - val_loss: 0.4037 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4245 - accuracy: 0.9027 - val_loss: 0.4464 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4150 - accuracy: 0.9050 - val_loss: 0.3837 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4130 - accuracy: 0.9065 - val_loss: 0.4051 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4159 - accuracy: 0.9049 - val_loss: 0.4250 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4164 - accuracy: 0.9046 - val_loss: 0.4281 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4025 - accuracy: 0.9079 - val_loss: 0.6295 - val_accuracy: 0.8329 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4115 - accuracy: 0.9044 - val_loss: 0.3858 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3969 - accuracy: 0.9072 - val_loss: 0.4179 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3923 - accuracy: 0.9083 - val_loss: 0.3681 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3991 - accuracy: 0.9049 - val_loss: 0.3920 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3898 - accuracy: 0.9092 - val_loss: 0.3875 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3898 - accuracy: 0.9061 - val_loss: 0.4142 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4020 - accuracy: 0.9029 - val_loss: 0.3797 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3865 - accuracy: 0.9071 - val_loss: 0.3746 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3885 - accuracy: 0.9082 - val_loss: 0.4055 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3876 - accuracy: 0.9076 - val_loss: 0.4155 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3882 - accuracy: 0.9066 - val_loss: 0.4562 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3839 - accuracy: 0.9075 - val_loss: 0.3876 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3756 - accuracy: 0.9111 - val_loss: 0.3867 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3920 - accuracy: 0.9074 - val_loss: 0.3608 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3811 - accuracy: 0.9066 - val_loss: 0.3765 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3770 - accuracy: 0.9107 - val_loss: 0.3733 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3833 - accuracy: 0.9088 - val_loss: 0.4642 - val_accuracy: 0.8753 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3798 - accuracy: 0.9112 - val_loss: 0.3691 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3842 - accuracy: 0.9096 - val_loss: 0.3651 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3827 - accuracy: 0.9091 - val_loss: 0.4122 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3773 - accuracy: 0.9094 - val_loss: 0.3537 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3806 - accuracy: 0.9096 - val_loss: 0.3557 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3720 - accuracy: 0.9103 - val_loss: 0.3624 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3714 - accuracy: 0.9124 - val_loss: 0.3682 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3810 - accuracy: 0.9132 - val_loss: 0.4518 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3724 - accuracy: 0.9144 - val_loss: 0.3866 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3784 - accuracy: 0.9097 - val_loss: 0.3625 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3780 - accuracy: 0.9116 - val_loss: 0.3594 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3692 - accuracy: 0.9123 - val_loss: 0.3973 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3786 - accuracy: 0.9099 - val_loss: 0.3808 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3823 - accuracy: 0.9087 - val_loss: 0.4137 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3784 - accuracy: 0.9095 - val_loss: 0.3715 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3820 - accuracy: 0.9114 - val_loss: 0.4085 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3845 - accuracy: 0.9098 - val_loss: 0.3798 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3807 - accuracy: 0.9073 - val_loss: 0.3900 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3706 - accuracy: 0.9102 - val_loss: 0.3779 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3699 - accuracy: 0.9102 - val_loss: 0.3715 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3711 - accuracy: 0.9122 - val_loss: 0.3533 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3733 - accuracy: 0.9112 - val_loss: 0.3740 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3699 - accuracy: 0.9106 - val_loss: 0.3935 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 0.3677 - accuracy: 0.9138 - val_loss: 0.4004 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3665 - accuracy: 0.9130 - val_loss: 0.3963 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3684 - accuracy: 0.9130 - val_loss: 0.3845 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3693 - accuracy: 0.9111 - val_loss: 0.3508 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3631 - accuracy: 0.9144 - val_loss: 0.3666 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3803 - accuracy: 0.9099 - val_loss: 0.3820 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3693 - accuracy: 0.9096 - val_loss: 0.3506 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3634 - accuracy: 0.9141 - val_loss: 0.6273 - val_accuracy: 0.8227 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3702 - accuracy: 0.9134 - val_loss: 0.3518 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3742 - accuracy: 0.9118 - val_loss: 0.3979 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3432 - accuracy: 0.9192 - val_loss: 0.3415 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3245 - accuracy: 0.9255 - val_loss: 0.3323 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3215 - accuracy: 0.9247 - val_loss: 0.3184 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3027 - accuracy: 0.9273 - val_loss: 0.3129 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2931 - accuracy: 0.9260 - val_loss: 0.3113 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2875 - accuracy: 0.9289 - val_loss: 0.2958 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2837 - accuracy: 0.9283 - val_loss: 0.2888 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2801 - accuracy: 0.9283 - val_loss: 0.2922 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2736 - accuracy: 0.9280 - val_loss: 0.2836 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2719 - accuracy: 0.9288 - val_loss: 0.2954 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2634 - accuracy: 0.9309 - val_loss: 0.2771 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2653 - accuracy: 0.9303 - val_loss: 0.2774 - val_accuracy: 0.9268 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2615 - accuracy: 0.9313 - val_loss: 0.2762 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2561 - accuracy: 0.9355 - val_loss: 0.2763 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2559 - accuracy: 0.9347 - val_loss: 0.2767 - val_accuracy: 0.9268 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2571 - accuracy: 0.9323 - val_loss: 0.2758 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2589 - accuracy: 0.9311 - val_loss: 0.2743 - val_accuracy: 0.9272 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2600 - accuracy: 0.9310 - val_loss: 0.2747 - val_accuracy: 0.9280 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2541 - accuracy: 0.9347 - val_loss: 0.2731 - val_accuracy: 0.9280 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2536 - accuracy: 0.9326 - val_loss: 0.2749 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2524 - accuracy: 0.9345 - val_loss: 0.2753 - val_accuracy: 0.9270 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2524 - accuracy: 0.9346 - val_loss: 0.2750 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2568 - accuracy: 0.9312 - val_loss: 0.2753 - val_accuracy: 0.9270 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2576 - accuracy: 0.9315 - val_loss: 0.2751 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2536 - accuracy: 0.9344 - val_loss: 0.2749 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2549 - accuracy: 0.9332 - val_loss: 0.2749 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2508 - accuracy: 0.9351 - val_loss: 0.2746 - val_accuracy: 0.9272 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2544 - accuracy: 0.9325 - val_loss: 0.2745 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2520 - accuracy: 0.9350 - val_loss: 0.2746 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2837 - accuracy: 0.9253\n",
      "17/17 [==============================] - 2s 27ms/step\n",
      "TP:5832, TN:9848, FP:607, FN:659, loss0.2837403416633606, acc0.9252921043314056, sn0.898474811277153, sp0.9419416547106647, f10.9020881670533643, auc0.9736813603738275\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 29s 65ms/step - loss: 3.8715 - accuracy: 0.6758 - val_loss: 3.8500 - val_accuracy: 0.3959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 2.4755 - accuracy: 0.8168 - val_loss: 2.3524 - val_accuracy: 0.5379 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.6483 - accuracy: 0.8449 - val_loss: 1.4035 - val_accuracy: 0.8297 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.1771 - accuracy: 0.8584 - val_loss: 1.0372 - val_accuracy: 0.8668 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.8962 - accuracy: 0.8746 - val_loss: 0.8013 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.7385 - accuracy: 0.8801 - val_loss: 0.6577 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.6407 - accuracy: 0.8861 - val_loss: 0.6069 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.5765 - accuracy: 0.8867 - val_loss: 0.5600 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.5185 - accuracy: 0.8978 - val_loss: 0.5005 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4958 - accuracy: 0.8993 - val_loss: 0.5622 - val_accuracy: 0.8744 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4807 - accuracy: 0.8992 - val_loss: 0.4579 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4604 - accuracy: 0.9009 - val_loss: 0.4448 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4510 - accuracy: 0.9002 - val_loss: 0.4381 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4427 - accuracy: 0.8989 - val_loss: 0.4920 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4281 - accuracy: 0.9031 - val_loss: 0.4988 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4176 - accuracy: 0.9034 - val_loss: 0.4542 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.4165 - accuracy: 0.9044 - val_loss: 0.4294 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.4156 - accuracy: 0.9054 - val_loss: 0.4043 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4082 - accuracy: 0.9073 - val_loss: 0.4169 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4161 - accuracy: 0.9016 - val_loss: 0.3934 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4023 - accuracy: 0.9077 - val_loss: 0.4239 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3953 - accuracy: 0.9090 - val_loss: 0.3872 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3989 - accuracy: 0.9087 - val_loss: 0.4375 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3942 - accuracy: 0.9115 - val_loss: 0.3972 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3951 - accuracy: 0.9093 - val_loss: 0.3881 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4068 - accuracy: 0.9057 - val_loss: 0.3796 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3943 - accuracy: 0.9087 - val_loss: 0.3798 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3979 - accuracy: 0.9058 - val_loss: 0.3882 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3867 - accuracy: 0.9099 - val_loss: 0.3790 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3990 - accuracy: 0.9079 - val_loss: 0.4178 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3825 - accuracy: 0.9107 - val_loss: 0.3837 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3871 - accuracy: 0.9073 - val_loss: 0.4205 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3875 - accuracy: 0.9084 - val_loss: 0.3748 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3850 - accuracy: 0.9115 - val_loss: 0.4158 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3880 - accuracy: 0.9095 - val_loss: 0.4317 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3863 - accuracy: 0.9073 - val_loss: 0.3622 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3836 - accuracy: 0.9088 - val_loss: 0.3589 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3756 - accuracy: 0.9119 - val_loss: 0.3756 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3704 - accuracy: 0.9125 - val_loss: 0.3844 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3758 - accuracy: 0.9104 - val_loss: 0.4399 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3755 - accuracy: 0.9093 - val_loss: 0.4233 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3870 - accuracy: 0.9090 - val_loss: 0.3726 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3728 - accuracy: 0.9114 - val_loss: 0.4171 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3730 - accuracy: 0.9129 - val_loss: 0.3669 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3772 - accuracy: 0.9118 - val_loss: 0.3707 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3767 - accuracy: 0.9105 - val_loss: 0.4355 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3834 - accuracy: 0.9056 - val_loss: 0.3914 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3713 - accuracy: 0.9120 - val_loss: 0.4125 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3781 - accuracy: 0.9115 - val_loss: 0.3702 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3742 - accuracy: 0.9108 - val_loss: 0.4914 - val_accuracy: 0.8624 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3763 - accuracy: 0.9129 - val_loss: 0.3640 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3718 - accuracy: 0.9098 - val_loss: 0.3686 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3774 - accuracy: 0.9080 - val_loss: 0.3559 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3806 - accuracy: 0.9123 - val_loss: 0.3724 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3744 - accuracy: 0.9126 - val_loss: 0.4716 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3738 - accuracy: 0.9121 - val_loss: 0.3597 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3757 - accuracy: 0.9125 - val_loss: 0.3689 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3652 - accuracy: 0.9123 - val_loss: 0.5648 - val_accuracy: 0.8309 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3624 - accuracy: 0.9135 - val_loss: 0.3639 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3686 - accuracy: 0.9126 - val_loss: 0.3811 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3714 - accuracy: 0.9131 - val_loss: 0.3559 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3726 - accuracy: 0.9125 - val_loss: 0.3685 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3744 - accuracy: 0.9104 - val_loss: 0.3691 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3652 - accuracy: 0.9111 - val_loss: 0.3666 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3686 - accuracy: 0.9146 - val_loss: 0.3806 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3736 - accuracy: 0.9130 - val_loss: 0.3537 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3659 - accuracy: 0.9126 - val_loss: 0.3707 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3614 - accuracy: 0.9123 - val_loss: 0.3613 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3659 - accuracy: 0.9128 - val_loss: 0.3775 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3568 - accuracy: 0.9136 - val_loss: 0.3496 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3634 - accuracy: 0.9160 - val_loss: 0.4179 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3487 - accuracy: 0.9191 - val_loss: 0.3450 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3268 - accuracy: 0.9274 - val_loss: 0.3331 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3133 - accuracy: 0.9259 - val_loss: 0.3263 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3104 - accuracy: 0.9238 - val_loss: 0.3174 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2984 - accuracy: 0.9285 - val_loss: 0.3040 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2926 - accuracy: 0.9277 - val_loss: 0.3099 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2808 - accuracy: 0.9314 - val_loss: 0.2954 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2826 - accuracy: 0.9268 - val_loss: 0.3046 - val_accuracy: 0.9228 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2732 - accuracy: 0.9285 - val_loss: 0.2882 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2699 - accuracy: 0.9280 - val_loss: 0.3056 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2678 - accuracy: 0.9313 - val_loss: 0.2832 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2587 - accuracy: 0.9318 - val_loss: 0.2820 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2563 - accuracy: 0.9353 - val_loss: 0.2843 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2583 - accuracy: 0.9319 - val_loss: 0.2822 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2590 - accuracy: 0.9334 - val_loss: 0.2815 - val_accuracy: 0.9272 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2550 - accuracy: 0.9367 - val_loss: 0.2818 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2551 - accuracy: 0.9343 - val_loss: 0.2814 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2573 - accuracy: 0.9337 - val_loss: 0.2799 - val_accuracy: 0.9272 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2543 - accuracy: 0.9360 - val_loss: 0.2810 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2588 - accuracy: 0.9318 - val_loss: 0.2792 - val_accuracy: 0.9256 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2520 - accuracy: 0.9336 - val_loss: 0.2795 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2536 - accuracy: 0.9338 - val_loss: 0.2794 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2496 - accuracy: 0.9349 - val_loss: 0.2795 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2589 - accuracy: 0.9320 - val_loss: 0.2800 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2542 - accuracy: 0.9338 - val_loss: 0.2797 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2504 - accuracy: 0.9352 - val_loss: 0.2795 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2552 - accuracy: 0.9340 - val_loss: 0.2799 - val_accuracy: 0.9266 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2565 - accuracy: 0.9338 - val_loss: 0.2796 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2557 - accuracy: 0.9328 - val_loss: 0.2796 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 7s 12ms/step - loss: 0.2891 - accuracy: 0.9232\n",
      "17/17 [==============================] - 2s 21ms/step\n",
      "TP:5792, TN:9853, FP:602, FN:699, loss0.2890869081020355, acc0.9232267201699517, sn0.8923124325989832, sp0.9424198947871831, f10.8990298797050834, auc0.9727283209558967\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 30s 70ms/step - loss: 3.8863 - accuracy: 0.6800 - val_loss: 3.9704 - val_accuracy: 0.4003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 2.5552 - accuracy: 0.8217 - val_loss: 2.2082 - val_accuracy: 0.7607 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 1.7438 - accuracy: 0.8477 - val_loss: 1.5294 - val_accuracy: 0.8039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 1.2385 - accuracy: 0.8629 - val_loss: 1.2479 - val_accuracy: 0.7920 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.9462 - accuracy: 0.8726 - val_loss: 0.9153 - val_accuracy: 0.8398 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.7591 - accuracy: 0.8791 - val_loss: 0.7190 - val_accuracy: 0.8740 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.6458 - accuracy: 0.8846 - val_loss: 0.6899 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5691 - accuracy: 0.8926 - val_loss: 0.5379 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.5327 - accuracy: 0.8900 - val_loss: 0.6360 - val_accuracy: 0.8604 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4938 - accuracy: 0.8975 - val_loss: 0.5695 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4746 - accuracy: 0.8988 - val_loss: 0.5597 - val_accuracy: 0.8680 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4574 - accuracy: 0.9029 - val_loss: 0.4530 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4445 - accuracy: 0.9061 - val_loss: 0.4458 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4379 - accuracy: 0.9029 - val_loss: 0.4504 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4285 - accuracy: 0.9052 - val_loss: 0.4389 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4204 - accuracy: 0.9038 - val_loss: 0.3961 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4110 - accuracy: 0.9053 - val_loss: 0.4163 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4071 - accuracy: 0.9061 - val_loss: 0.4410 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4038 - accuracy: 0.9046 - val_loss: 0.4169 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3992 - accuracy: 0.9079 - val_loss: 0.3795 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4022 - accuracy: 0.9057 - val_loss: 0.3985 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3875 - accuracy: 0.9074 - val_loss: 0.3643 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3976 - accuracy: 0.9048 - val_loss: 0.5559 - val_accuracy: 0.8359 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3989 - accuracy: 0.9067 - val_loss: 0.4021 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3985 - accuracy: 0.9066 - val_loss: 0.3823 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3926 - accuracy: 0.9067 - val_loss: 0.3720 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3936 - accuracy: 0.9088 - val_loss: 0.4483 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3881 - accuracy: 0.9098 - val_loss: 0.4321 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3951 - accuracy: 0.9057 - val_loss: 0.4913 - val_accuracy: 0.8584 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3931 - accuracy: 0.9078 - val_loss: 0.3918 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3864 - accuracy: 0.9122 - val_loss: 0.3920 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3960 - accuracy: 0.9049 - val_loss: 0.3795 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3877 - accuracy: 0.9082 - val_loss: 0.3656 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3911 - accuracy: 0.9085 - val_loss: 0.4040 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3821 - accuracy: 0.9089 - val_loss: 0.4419 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3888 - accuracy: 0.9067 - val_loss: 0.3925 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3728 - accuracy: 0.9157 - val_loss: 0.3908 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3858 - accuracy: 0.9084 - val_loss: 0.4614 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3823 - accuracy: 0.9083 - val_loss: 0.5453 - val_accuracy: 0.8544 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3859 - accuracy: 0.9103 - val_loss: 0.3782 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3859 - accuracy: 0.9113 - val_loss: 0.4220 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3818 - accuracy: 0.9114 - val_loss: 0.3675 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3725 - accuracy: 0.9120 - val_loss: 0.4563 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3853 - accuracy: 0.9094 - val_loss: 0.4515 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3738 - accuracy: 0.9114 - val_loss: 0.3872 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3799 - accuracy: 0.9102 - val_loss: 0.3633 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3855 - accuracy: 0.9057 - val_loss: 0.3764 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3722 - accuracy: 0.9114 - val_loss: 0.3817 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3807 - accuracy: 0.9097 - val_loss: 0.4962 - val_accuracy: 0.8640 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3701 - accuracy: 0.9104 - val_loss: 0.3609 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3830 - accuracy: 0.9097 - val_loss: 0.3917 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3804 - accuracy: 0.9108 - val_loss: 0.3945 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3752 - accuracy: 0.9109 - val_loss: 0.3746 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3770 - accuracy: 0.9117 - val_loss: 0.3930 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3746 - accuracy: 0.9118 - val_loss: 0.4252 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3778 - accuracy: 0.9097 - val_loss: 0.3776 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3816 - accuracy: 0.9097 - val_loss: 0.3724 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3717 - accuracy: 0.9132 - val_loss: 0.3563 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3832 - accuracy: 0.9106 - val_loss: 0.3636 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3706 - accuracy: 0.9114 - val_loss: 0.3693 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3732 - accuracy: 0.9133 - val_loss: 0.3746 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3710 - accuracy: 0.9158 - val_loss: 0.3555 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3723 - accuracy: 0.9097 - val_loss: 0.4106 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3690 - accuracy: 0.9132 - val_loss: 0.3757 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3711 - accuracy: 0.9105 - val_loss: 0.4766 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3625 - accuracy: 0.9142 - val_loss: 0.3556 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3737 - accuracy: 0.9123 - val_loss: 0.3512 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3609 - accuracy: 0.9117 - val_loss: 0.4508 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3662 - accuracy: 0.9122 - val_loss: 0.3856 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3640 - accuracy: 0.9129 - val_loss: 0.3540 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3741 - accuracy: 0.9074 - val_loss: 0.3757 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3483 - accuracy: 0.9228 - val_loss: 0.3449 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3358 - accuracy: 0.9224 - val_loss: 0.3316 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3196 - accuracy: 0.9271 - val_loss: 0.3539 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3080 - accuracy: 0.9258 - val_loss: 0.3220 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2969 - accuracy: 0.9307 - val_loss: 0.3148 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2878 - accuracy: 0.9301 - val_loss: 0.2998 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2842 - accuracy: 0.9270 - val_loss: 0.3001 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2814 - accuracy: 0.9281 - val_loss: 0.2900 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2707 - accuracy: 0.9314 - val_loss: 0.2856 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2732 - accuracy: 0.9271 - val_loss: 0.3155 - val_accuracy: 0.9172 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2637 - accuracy: 0.9319 - val_loss: 0.2821 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2593 - accuracy: 0.9330 - val_loss: 0.2811 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2594 - accuracy: 0.9332 - val_loss: 0.2800 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2609 - accuracy: 0.9310 - val_loss: 0.2816 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2569 - accuracy: 0.9326 - val_loss: 0.2821 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2569 - accuracy: 0.9356 - val_loss: 0.2809 - val_accuracy: 0.9248 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2623 - accuracy: 0.9297 - val_loss: 0.2801 - val_accuracy: 0.9250 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2546 - accuracy: 0.9341 - val_loss: 0.2814 - val_accuracy: 0.9240 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2569 - accuracy: 0.9336 - val_loss: 0.2787 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2511 - accuracy: 0.9340 - val_loss: 0.2772 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2529 - accuracy: 0.9350 - val_loss: 0.2787 - val_accuracy: 0.9240 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2552 - accuracy: 0.9324 - val_loss: 0.2803 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2540 - accuracy: 0.9325 - val_loss: 0.2805 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2514 - accuracy: 0.9335 - val_loss: 0.2804 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2538 - accuracy: 0.9359 - val_loss: 0.2803 - val_accuracy: 0.9240 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2550 - accuracy: 0.9316 - val_loss: 0.2800 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2503 - accuracy: 0.9357 - val_loss: 0.2797 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2515 - accuracy: 0.9339 - val_loss: 0.2797 - val_accuracy: 0.9236 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2498 - accuracy: 0.9352 - val_loss: 0.2798 - val_accuracy: 0.9236 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2859 - accuracy: 0.9238\n",
      "17/17 [==============================] - 2s 27ms/step\n",
      "TP:5832, TN:9822, FP:633, FN:659, loss0.28589242696762085, acc0.9237578189543255, sn0.898474811277153, sp0.939454806312769, f10.9002778635381291, auc0.9731985817687749\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 30s 75ms/step - loss: 3.9341 - accuracy: 0.6723 - val_loss: 4.4809 - val_accuracy: 0.3913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.5688 - accuracy: 0.8126 - val_loss: 2.5080 - val_accuracy: 0.4801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 1.7402 - accuracy: 0.8359 - val_loss: 1.4985 - val_accuracy: 0.8329 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 1.2243 - accuracy: 0.8601 - val_loss: 1.0646 - val_accuracy: 0.8698 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.9303 - accuracy: 0.8733 - val_loss: 0.9213 - val_accuracy: 0.8381 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.7696 - accuracy: 0.8768 - val_loss: 0.6967 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.6535 - accuracy: 0.8851 - val_loss: 0.7117 - val_accuracy: 0.8538 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5904 - accuracy: 0.8847 - val_loss: 0.6029 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.5327 - accuracy: 0.8923 - val_loss: 0.4849 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4967 - accuracy: 0.8964 - val_loss: 0.5122 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4908 - accuracy: 0.8957 - val_loss: 0.4481 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4547 - accuracy: 0.8992 - val_loss: 0.4866 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4517 - accuracy: 0.8977 - val_loss: 0.4915 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4329 - accuracy: 0.9026 - val_loss: 0.4435 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4332 - accuracy: 0.9034 - val_loss: 0.4165 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4217 - accuracy: 0.8993 - val_loss: 0.4048 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4163 - accuracy: 0.9041 - val_loss: 0.4118 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4100 - accuracy: 0.9057 - val_loss: 0.4710 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4155 - accuracy: 0.9026 - val_loss: 0.4211 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4165 - accuracy: 0.8996 - val_loss: 0.4756 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4023 - accuracy: 0.9060 - val_loss: 0.3856 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4059 - accuracy: 0.9010 - val_loss: 0.4152 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4065 - accuracy: 0.9052 - val_loss: 0.4346 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3968 - accuracy: 0.9057 - val_loss: 0.5273 - val_accuracy: 0.8514 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3969 - accuracy: 0.9039 - val_loss: 0.3950 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3965 - accuracy: 0.9041 - val_loss: 0.4266 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3962 - accuracy: 0.9077 - val_loss: 0.3990 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3859 - accuracy: 0.9095 - val_loss: 0.4230 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4016 - accuracy: 0.9067 - val_loss: 0.6208 - val_accuracy: 0.8271 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3998 - accuracy: 0.9040 - val_loss: 0.4026 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3913 - accuracy: 0.9069 - val_loss: 0.4753 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3935 - accuracy: 0.9072 - val_loss: 0.4034 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3892 - accuracy: 0.9085 - val_loss: 0.3832 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3874 - accuracy: 0.9105 - val_loss: 0.3859 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3896 - accuracy: 0.9076 - val_loss: 0.3850 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3904 - accuracy: 0.9069 - val_loss: 0.3865 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3805 - accuracy: 0.9109 - val_loss: 0.4577 - val_accuracy: 0.8853 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3869 - accuracy: 0.9076 - val_loss: 0.3625 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3831 - accuracy: 0.9064 - val_loss: 0.3932 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3838 - accuracy: 0.9082 - val_loss: 0.3836 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3884 - accuracy: 0.9044 - val_loss: 0.3738 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3753 - accuracy: 0.9104 - val_loss: 0.3755 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3762 - accuracy: 0.9105 - val_loss: 0.5263 - val_accuracy: 0.8594 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3766 - accuracy: 0.9126 - val_loss: 0.4132 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3748 - accuracy: 0.9100 - val_loss: 0.3717 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3805 - accuracy: 0.9081 - val_loss: 0.3999 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3750 - accuracy: 0.9115 - val_loss: 0.4176 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3738 - accuracy: 0.9141 - val_loss: 0.3874 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3947 - accuracy: 0.9031 - val_loss: 0.3659 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3766 - accuracy: 0.9073 - val_loss: 0.3618 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 0.3743 - accuracy: 0.9087 - val_loss: 0.4701 - val_accuracy: 0.8785 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3811 - accuracy: 0.9102 - val_loss: 0.4152 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3712 - accuracy: 0.9102 - val_loss: 0.3684 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3774 - accuracy: 0.9097 - val_loss: 0.3794 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3718 - accuracy: 0.9127 - val_loss: 0.3618 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3686 - accuracy: 0.9120 - val_loss: 0.3676 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3782 - accuracy: 0.9155 - val_loss: 0.3615 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3787 - accuracy: 0.9102 - val_loss: 0.4724 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3735 - accuracy: 0.9135 - val_loss: 0.3962 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3660 - accuracy: 0.9100 - val_loss: 0.3612 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3693 - accuracy: 0.9111 - val_loss: 0.3548 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3709 - accuracy: 0.9112 - val_loss: 0.3921 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3675 - accuracy: 0.9132 - val_loss: 0.3707 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3681 - accuracy: 0.9120 - val_loss: 0.3990 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3669 - accuracy: 0.9120 - val_loss: 0.3914 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3719 - accuracy: 0.9122 - val_loss: 0.3502 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3669 - accuracy: 0.9104 - val_loss: 0.3769 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3702 - accuracy: 0.9109 - val_loss: 0.3762 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3639 - accuracy: 0.9097 - val_loss: 0.3678 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3591 - accuracy: 0.9132 - val_loss: 0.4019 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3602 - accuracy: 0.9139 - val_loss: 0.4067 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3425 - accuracy: 0.9191 - val_loss: 0.3490 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3225 - accuracy: 0.9232 - val_loss: 0.3278 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3132 - accuracy: 0.9244 - val_loss: 0.3281 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3030 - accuracy: 0.9250 - val_loss: 0.3090 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2923 - accuracy: 0.9267 - val_loss: 0.3313 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2889 - accuracy: 0.9285 - val_loss: 0.2964 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2784 - accuracy: 0.9296 - val_loss: 0.3184 - val_accuracy: 0.9204 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2795 - accuracy: 0.9261 - val_loss: 0.2931 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2712 - accuracy: 0.9286 - val_loss: 0.3008 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2689 - accuracy: 0.9305 - val_loss: 0.3162 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2606 - accuracy: 0.9318 - val_loss: 0.2828 - val_accuracy: 0.9248 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2610 - accuracy: 0.9296 - val_loss: 0.2817 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2556 - accuracy: 0.9330 - val_loss: 0.2824 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2584 - accuracy: 0.9337 - val_loss: 0.2798 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2578 - accuracy: 0.9319 - val_loss: 0.2796 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2577 - accuracy: 0.9338 - val_loss: 0.2778 - val_accuracy: 0.9264 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2568 - accuracy: 0.9332 - val_loss: 0.2784 - val_accuracy: 0.9274 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2589 - accuracy: 0.9326 - val_loss: 0.2841 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2553 - accuracy: 0.9344 - val_loss: 0.2777 - val_accuracy: 0.9264 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2599 - accuracy: 0.9306 - val_loss: 0.2791 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2539 - accuracy: 0.9331 - val_loss: 0.2776 - val_accuracy: 0.9268 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2513 - accuracy: 0.9343 - val_loss: 0.2773 - val_accuracy: 0.9266 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2564 - accuracy: 0.9333 - val_loss: 0.2774 - val_accuracy: 0.9266 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2606 - accuracy: 0.9312 - val_loss: 0.2777 - val_accuracy: 0.9266 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2567 - accuracy: 0.9323 - val_loss: 0.2772 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2514 - accuracy: 0.9342 - val_loss: 0.2769 - val_accuracy: 0.9266 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2549 - accuracy: 0.9331 - val_loss: 0.2771 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2544 - accuracy: 0.9327 - val_loss: 0.2770 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2539 - accuracy: 0.9325 - val_loss: 0.2771 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2843 - accuracy: 0.9251\n",
      "17/17 [==============================] - 2s 24ms/step\n",
      "TP:5767, TN:9909, FP:546, FN:724, loss0.28433409333229065, acc0.9250560604272394, sn0.8884609459251271, sp0.9477761836441894, f10.9008122461730709, auc0.9732133909284392\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 29s 68ms/step - loss: 3.9359 - accuracy: 0.6516 - val_loss: 3.4185 - val_accuracy: 0.4785 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 2.5428 - accuracy: 0.8013 - val_loss: 2.2630 - val_accuracy: 0.6322 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 1.6876 - accuracy: 0.8452 - val_loss: 1.4152 - val_accuracy: 0.8554 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 1.1867 - accuracy: 0.8652 - val_loss: 1.0078 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.8982 - accuracy: 0.8817 - val_loss: 0.8299 - val_accuracy: 0.8646 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.7409 - accuracy: 0.8812 - val_loss: 0.7400 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.6247 - accuracy: 0.8920 - val_loss: 0.6304 - val_accuracy: 0.8767 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.5604 - accuracy: 0.8935 - val_loss: 0.5301 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.5239 - accuracy: 0.8949 - val_loss: 0.4961 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4861 - accuracy: 0.8993 - val_loss: 0.4693 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4582 - accuracy: 0.9016 - val_loss: 0.4726 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4493 - accuracy: 0.9007 - val_loss: 0.4844 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4430 - accuracy: 0.9036 - val_loss: 0.4300 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4319 - accuracy: 0.9045 - val_loss: 0.4575 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4335 - accuracy: 0.8999 - val_loss: 0.4386 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4173 - accuracy: 0.9019 - val_loss: 0.4697 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4097 - accuracy: 0.9030 - val_loss: 0.3959 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4090 - accuracy: 0.9023 - val_loss: 0.4044 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3933 - accuracy: 0.9044 - val_loss: 0.4494 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3993 - accuracy: 0.9057 - val_loss: 0.3838 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4098 - accuracy: 0.9049 - val_loss: 0.4090 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3995 - accuracy: 0.9053 - val_loss: 0.3854 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4038 - accuracy: 0.9056 - val_loss: 0.3863 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3960 - accuracy: 0.9076 - val_loss: 0.4691 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3933 - accuracy: 0.9095 - val_loss: 0.4445 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3960 - accuracy: 0.9083 - val_loss: 0.4188 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3992 - accuracy: 0.9085 - val_loss: 0.3815 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3865 - accuracy: 0.9054 - val_loss: 0.4872 - val_accuracy: 0.8644 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3892 - accuracy: 0.9072 - val_loss: 0.3870 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3927 - accuracy: 0.9067 - val_loss: 0.3821 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3922 - accuracy: 0.9093 - val_loss: 0.3900 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3939 - accuracy: 0.9044 - val_loss: 0.3603 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3780 - accuracy: 0.9097 - val_loss: 0.4133 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3871 - accuracy: 0.9072 - val_loss: 0.3725 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3884 - accuracy: 0.9079 - val_loss: 0.4163 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3806 - accuracy: 0.9097 - val_loss: 0.3950 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3857 - accuracy: 0.9098 - val_loss: 0.3908 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3859 - accuracy: 0.9103 - val_loss: 0.3910 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3992 - accuracy: 0.9034 - val_loss: 0.4014 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3841 - accuracy: 0.9096 - val_loss: 0.3854 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3713 - accuracy: 0.9132 - val_loss: 0.3642 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3697 - accuracy: 0.9099 - val_loss: 0.3886 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3869 - accuracy: 0.9085 - val_loss: 0.4681 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3763 - accuracy: 0.9104 - val_loss: 0.4144 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3933 - accuracy: 0.9072 - val_loss: 0.4022 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3814 - accuracy: 0.9109 - val_loss: 0.3595 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3775 - accuracy: 0.9100 - val_loss: 0.5257 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3763 - accuracy: 0.9115 - val_loss: 0.3796 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3714 - accuracy: 0.9092 - val_loss: 0.4232 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3698 - accuracy: 0.9130 - val_loss: 0.3883 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3862 - accuracy: 0.9055 - val_loss: 0.3614 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3754 - accuracy: 0.9090 - val_loss: 0.3615 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3687 - accuracy: 0.9115 - val_loss: 0.4123 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3800 - accuracy: 0.9075 - val_loss: 0.3580 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3647 - accuracy: 0.9127 - val_loss: 0.4562 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3664 - accuracy: 0.9133 - val_loss: 0.3848 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3724 - accuracy: 0.9123 - val_loss: 0.4366 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3701 - accuracy: 0.9121 - val_loss: 0.3647 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3781 - accuracy: 0.9080 - val_loss: 0.3831 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3702 - accuracy: 0.9118 - val_loss: 0.5316 - val_accuracy: 0.8424 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3691 - accuracy: 0.9113 - val_loss: 0.3606 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3692 - accuracy: 0.9103 - val_loss: 0.3625 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3692 - accuracy: 0.9114 - val_loss: 0.3735 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3696 - accuracy: 0.9122 - val_loss: 0.3903 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3720 - accuracy: 0.9110 - val_loss: 0.3542 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3703 - accuracy: 0.9088 - val_loss: 0.4477 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3675 - accuracy: 0.9120 - val_loss: 0.3693 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3650 - accuracy: 0.9151 - val_loss: 0.3949 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3686 - accuracy: 0.9139 - val_loss: 0.3640 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 0.3672 - accuracy: 0.9099 - val_loss: 0.3591 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3674 - accuracy: 0.9138 - val_loss: 0.3616 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3434 - accuracy: 0.9217 - val_loss: 0.3457 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3305 - accuracy: 0.9218 - val_loss: 0.3534 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3197 - accuracy: 0.9232 - val_loss: 0.3408 - val_accuracy: 0.9162 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3115 - accuracy: 0.9229 - val_loss: 0.3158 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.2957 - accuracy: 0.9285 - val_loss: 0.3129 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.2906 - accuracy: 0.9286 - val_loss: 0.3148 - val_accuracy: 0.9200 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2849 - accuracy: 0.9274 - val_loss: 0.3016 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2799 - accuracy: 0.9261 - val_loss: 0.2872 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2761 - accuracy: 0.9290 - val_loss: 0.2897 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2717 - accuracy: 0.9288 - val_loss: 0.2842 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2664 - accuracy: 0.9308 - val_loss: 0.2823 - val_accuracy: 0.9264 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2694 - accuracy: 0.9303 - val_loss: 0.2808 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2608 - accuracy: 0.9303 - val_loss: 0.2786 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2643 - accuracy: 0.9314 - val_loss: 0.2820 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2632 - accuracy: 0.9300 - val_loss: 0.2777 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2576 - accuracy: 0.9328 - val_loss: 0.2768 - val_accuracy: 0.9276 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2615 - accuracy: 0.9329 - val_loss: 0.2795 - val_accuracy: 0.9276 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2559 - accuracy: 0.9320 - val_loss: 0.2778 - val_accuracy: 0.9286 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2552 - accuracy: 0.9314 - val_loss: 0.2782 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2586 - accuracy: 0.9322 - val_loss: 0.2772 - val_accuracy: 0.9274 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2529 - accuracy: 0.9323 - val_loss: 0.2774 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2585 - accuracy: 0.9329 - val_loss: 0.2774 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2534 - accuracy: 0.9347 - val_loss: 0.2776 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2582 - accuracy: 0.9330 - val_loss: 0.2775 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2597 - accuracy: 0.9344 - val_loss: 0.2774 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2579 - accuracy: 0.9324 - val_loss: 0.2772 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2592 - accuracy: 0.9323 - val_loss: 0.2773 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2533 - accuracy: 0.9331 - val_loss: 0.2772 - val_accuracy: 0.9272 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2602 - accuracy: 0.9317 - val_loss: 0.2773 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2848 - accuracy: 0.9265\n",
      "17/17 [==============================] - 1s 26ms/step\n",
      "TP:5811, TN:9890, FP:565, FN:680, loss0.28481006622314453, acc0.9265313348282781, sn0.8952395624711138, sp0.9459588713534194, f10.9032408486826765, auc0.9729983413004403\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 31s 65ms/step - loss: 3.8570 - accuracy: 0.6981 - val_loss: 4.4105 - val_accuracy: 0.3897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 2.4813 - accuracy: 0.8283 - val_loss: 2.0806 - val_accuracy: 0.7970 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 1.6586 - accuracy: 0.8467 - val_loss: 1.3760 - val_accuracy: 0.8560 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 1.1704 - accuracy: 0.8595 - val_loss: 1.0174 - val_accuracy: 0.8522 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.8760 - accuracy: 0.8736 - val_loss: 0.8134 - val_accuracy: 0.8648 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.7100 - accuracy: 0.8817 - val_loss: 0.7866 - val_accuracy: 0.8394 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.6217 - accuracy: 0.8855 - val_loss: 0.7225 - val_accuracy: 0.8446 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.5533 - accuracy: 0.8898 - val_loss: 0.9244 - val_accuracy: 0.7880 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.5083 - accuracy: 0.9010 - val_loss: 0.4702 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4953 - accuracy: 0.8943 - val_loss: 0.4938 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4614 - accuracy: 0.9011 - val_loss: 0.4527 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4492 - accuracy: 0.9018 - val_loss: 0.5075 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4505 - accuracy: 0.8982 - val_loss: 0.4068 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4301 - accuracy: 0.9020 - val_loss: 0.4592 - val_accuracy: 0.8875 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4268 - accuracy: 0.9016 - val_loss: 0.4045 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4171 - accuracy: 0.9038 - val_loss: 0.4228 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4111 - accuracy: 0.8997 - val_loss: 0.3734 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4067 - accuracy: 0.9026 - val_loss: 0.3781 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4041 - accuracy: 0.9029 - val_loss: 0.3773 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3928 - accuracy: 0.9062 - val_loss: 0.4098 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4001 - accuracy: 0.9052 - val_loss: 0.3997 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4042 - accuracy: 0.9028 - val_loss: 0.4124 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3945 - accuracy: 0.9066 - val_loss: 0.4129 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3930 - accuracy: 0.9044 - val_loss: 0.3681 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3967 - accuracy: 0.9042 - val_loss: 0.4071 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3850 - accuracy: 0.9091 - val_loss: 0.3965 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3901 - accuracy: 0.9088 - val_loss: 0.4117 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3861 - accuracy: 0.9097 - val_loss: 0.3689 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3960 - accuracy: 0.9026 - val_loss: 0.3878 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3876 - accuracy: 0.9042 - val_loss: 0.4221 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3788 - accuracy: 0.9082 - val_loss: 0.3591 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3844 - accuracy: 0.9063 - val_loss: 0.3577 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3763 - accuracy: 0.9060 - val_loss: 0.3598 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3862 - accuracy: 0.9084 - val_loss: 0.3986 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3827 - accuracy: 0.9068 - val_loss: 0.3689 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3735 - accuracy: 0.9117 - val_loss: 0.3972 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3903 - accuracy: 0.9043 - val_loss: 0.3908 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3951 - accuracy: 0.9029 - val_loss: 0.3611 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3670 - accuracy: 0.9096 - val_loss: 0.3557 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3795 - accuracy: 0.9086 - val_loss: 0.4750 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3770 - accuracy: 0.9132 - val_loss: 0.3732 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3835 - accuracy: 0.9101 - val_loss: 0.4292 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3789 - accuracy: 0.9096 - val_loss: 0.3577 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3825 - accuracy: 0.9120 - val_loss: 0.3811 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3845 - accuracy: 0.9085 - val_loss: 0.3552 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3733 - accuracy: 0.9099 - val_loss: 0.4144 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3743 - accuracy: 0.9102 - val_loss: 0.4114 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3805 - accuracy: 0.9092 - val_loss: 0.3625 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3763 - accuracy: 0.9079 - val_loss: 0.4205 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3858 - accuracy: 0.9072 - val_loss: 0.4645 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3769 - accuracy: 0.9109 - val_loss: 0.3595 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3680 - accuracy: 0.9130 - val_loss: 0.3613 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3689 - accuracy: 0.9142 - val_loss: 0.4466 - val_accuracy: 0.8823 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3730 - accuracy: 0.9131 - val_loss: 0.3824 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3727 - accuracy: 0.9131 - val_loss: 0.4292 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3741 - accuracy: 0.9099 - val_loss: 0.3679 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3747 - accuracy: 0.9126 - val_loss: 0.3726 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3690 - accuracy: 0.9104 - val_loss: 0.3623 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3759 - accuracy: 0.9108 - val_loss: 0.4093 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3701 - accuracy: 0.9115 - val_loss: 0.3910 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3699 - accuracy: 0.9126 - val_loss: 0.3941 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3725 - accuracy: 0.9115 - val_loss: 0.3871 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3759 - accuracy: 0.9087 - val_loss: 0.3653 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3679 - accuracy: 0.9124 - val_loss: 0.4142 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3649 - accuracy: 0.9121 - val_loss: 0.4736 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3675 - accuracy: 0.9137 - val_loss: 0.3640 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3657 - accuracy: 0.9112 - val_loss: 0.3682 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3578 - accuracy: 0.9153 - val_loss: 0.3665 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3666 - accuracy: 0.9132 - val_loss: 0.3758 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3652 - accuracy: 0.9127 - val_loss: 0.4109 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3651 - accuracy: 0.9144 - val_loss: 0.3770 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3377 - accuracy: 0.9208 - val_loss: 0.3414 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3226 - accuracy: 0.9244 - val_loss: 0.3383 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3082 - accuracy: 0.9250 - val_loss: 0.3339 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3010 - accuracy: 0.9258 - val_loss: 0.3178 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2933 - accuracy: 0.9257 - val_loss: 0.3055 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2823 - accuracy: 0.9292 - val_loss: 0.2987 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2846 - accuracy: 0.9265 - val_loss: 0.2948 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2782 - accuracy: 0.9281 - val_loss: 0.2927 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2723 - accuracy: 0.9273 - val_loss: 0.2857 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2679 - accuracy: 0.9276 - val_loss: 0.2924 - val_accuracy: 0.9236 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2668 - accuracy: 0.9285 - val_loss: 0.2786 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2567 - accuracy: 0.9315 - val_loss: 0.2761 - val_accuracy: 0.9280 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2545 - accuracy: 0.9343 - val_loss: 0.2774 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2555 - accuracy: 0.9338 - val_loss: 0.2768 - val_accuracy: 0.9272 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2571 - accuracy: 0.9329 - val_loss: 0.2757 - val_accuracy: 0.9276 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.2521 - accuracy: 0.9328 - val_loss: 0.2766 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2529 - accuracy: 0.9341 - val_loss: 0.2758 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2549 - accuracy: 0.9323 - val_loss: 0.2757 - val_accuracy: 0.9266 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2511 - accuracy: 0.9338 - val_loss: 0.2734 - val_accuracy: 0.9280 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2511 - accuracy: 0.9342 - val_loss: 0.2754 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2538 - accuracy: 0.9340 - val_loss: 0.2751 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2482 - accuracy: 0.9350 - val_loss: 0.2749 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.2529 - accuracy: 0.9309 - val_loss: 0.2746 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2552 - accuracy: 0.9323 - val_loss: 0.2751 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2526 - accuracy: 0.9326 - val_loss: 0.2747 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2551 - accuracy: 0.9331 - val_loss: 0.2747 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2523 - accuracy: 0.9332 - val_loss: 0.2750 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2531 - accuracy: 0.9313 - val_loss: 0.2748 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2515 - accuracy: 0.9349 - val_loss: 0.2747 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 6s 11ms/step - loss: 0.2807 - accuracy: 0.9255\n",
      "17/17 [==============================] - 2s 22ms/step\n",
      "TP:5816, TN:9868, FP:587, FN:675, loss0.28068968653678894, acc0.9255281482355718, sn0.8960098598058851, sp0.9438546150167384, f10.9021250193888629, auc0.9733649453634106\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 24s 51ms/step - loss: 3.8606 - accuracy: 0.6719 - val_loss: 3.7440 - val_accuracy: 0.4222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 2.4369 - accuracy: 0.8183 - val_loss: 1.9922 - val_accuracy: 0.8255 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 1.6022 - accuracy: 0.8502 - val_loss: 1.3389 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 1.1361 - accuracy: 0.8594 - val_loss: 0.9790 - val_accuracy: 0.8672 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.8674 - accuracy: 0.8721 - val_loss: 0.8467 - val_accuracy: 0.8430 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.7112 - accuracy: 0.8824 - val_loss: 0.6677 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.6158 - accuracy: 0.8867 - val_loss: 0.5670 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.5538 - accuracy: 0.8916 - val_loss: 0.5108 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5094 - accuracy: 0.8951 - val_loss: 0.5466 - val_accuracy: 0.8777 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4723 - accuracy: 0.9004 - val_loss: 0.5032 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4600 - accuracy: 0.9013 - val_loss: 0.4395 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4543 - accuracy: 0.8989 - val_loss: 0.4289 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4364 - accuracy: 0.9003 - val_loss: 0.4284 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4298 - accuracy: 0.9029 - val_loss: 0.4364 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4239 - accuracy: 0.9007 - val_loss: 0.4840 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4175 - accuracy: 0.9049 - val_loss: 0.3860 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4092 - accuracy: 0.9042 - val_loss: 0.4951 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4054 - accuracy: 0.9018 - val_loss: 0.4285 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.4005 - accuracy: 0.9050 - val_loss: 0.4018 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3985 - accuracy: 0.9062 - val_loss: 0.4182 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3959 - accuracy: 0.9053 - val_loss: 0.3762 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3956 - accuracy: 0.9061 - val_loss: 0.5491 - val_accuracy: 0.8574 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3979 - accuracy: 0.9055 - val_loss: 0.3785 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3875 - accuracy: 0.9060 - val_loss: 0.4168 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3962 - accuracy: 0.9045 - val_loss: 0.3796 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3880 - accuracy: 0.9091 - val_loss: 0.3616 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3845 - accuracy: 0.9101 - val_loss: 0.3772 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3861 - accuracy: 0.9071 - val_loss: 0.4055 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3830 - accuracy: 0.9090 - val_loss: 0.3824 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3968 - accuracy: 0.9047 - val_loss: 0.4105 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3874 - accuracy: 0.9067 - val_loss: 0.4482 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3848 - accuracy: 0.9110 - val_loss: 0.4141 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3868 - accuracy: 0.9070 - val_loss: 0.3958 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3915 - accuracy: 0.9065 - val_loss: 0.3678 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3825 - accuracy: 0.9071 - val_loss: 0.3740 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3789 - accuracy: 0.9114 - val_loss: 0.3647 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3850 - accuracy: 0.9078 - val_loss: 0.4039 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3777 - accuracy: 0.9082 - val_loss: 0.4060 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3703 - accuracy: 0.9095 - val_loss: 0.4094 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3713 - accuracy: 0.9079 - val_loss: 0.3706 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3733 - accuracy: 0.9089 - val_loss: 0.4431 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3750 - accuracy: 0.9085 - val_loss: 0.3968 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3741 - accuracy: 0.9085 - val_loss: 0.3696 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3832 - accuracy: 0.9092 - val_loss: 0.4037 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3836 - accuracy: 0.9115 - val_loss: 0.3987 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3791 - accuracy: 0.9120 - val_loss: 0.3808 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3767 - accuracy: 0.9073 - val_loss: 0.4538 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3741 - accuracy: 0.9114 - val_loss: 0.3947 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3738 - accuracy: 0.9114 - val_loss: 0.3543 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3707 - accuracy: 0.9133 - val_loss: 0.3941 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3747 - accuracy: 0.9125 - val_loss: 0.3791 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3796 - accuracy: 0.9099 - val_loss: 0.7783 - val_accuracy: 0.7611 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3679 - accuracy: 0.9120 - val_loss: 0.3713 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3660 - accuracy: 0.9141 - val_loss: 0.3503 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3621 - accuracy: 0.9112 - val_loss: 0.3600 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3723 - accuracy: 0.9099 - val_loss: 0.3828 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3793 - accuracy: 0.9078 - val_loss: 0.3525 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3680 - accuracy: 0.9082 - val_loss: 0.3842 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3635 - accuracy: 0.9101 - val_loss: 0.4403 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3747 - accuracy: 0.9097 - val_loss: 0.3704 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3660 - accuracy: 0.9130 - val_loss: 0.3885 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3648 - accuracy: 0.9094 - val_loss: 0.3990 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3750 - accuracy: 0.9119 - val_loss: 0.3595 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3613 - accuracy: 0.9125 - val_loss: 0.3631 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3694 - accuracy: 0.9116 - val_loss: 0.3539 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3679 - accuracy: 0.9120 - val_loss: 0.3768 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3619 - accuracy: 0.9127 - val_loss: 0.4859 - val_accuracy: 0.8578 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3630 - accuracy: 0.9109 - val_loss: 0.3573 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3668 - accuracy: 0.9151 - val_loss: 0.3854 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3560 - accuracy: 0.9160 - val_loss: 0.3521 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3742 - accuracy: 0.9091 - val_loss: 0.3856 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3431 - accuracy: 0.9234 - val_loss: 0.3334 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3245 - accuracy: 0.9247 - val_loss: 0.3204 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3102 - accuracy: 0.9243 - val_loss: 0.3117 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3016 - accuracy: 0.9289 - val_loss: 0.3042 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2931 - accuracy: 0.9275 - val_loss: 0.3033 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2882 - accuracy: 0.9267 - val_loss: 0.2888 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2786 - accuracy: 0.9297 - val_loss: 0.2839 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2754 - accuracy: 0.9260 - val_loss: 0.2815 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2751 - accuracy: 0.9280 - val_loss: 0.2865 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2663 - accuracy: 0.9290 - val_loss: 0.2733 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2594 - accuracy: 0.9329 - val_loss: 0.2732 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2586 - accuracy: 0.9326 - val_loss: 0.2749 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2557 - accuracy: 0.9337 - val_loss: 0.2742 - val_accuracy: 0.9256 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2575 - accuracy: 0.9296 - val_loss: 0.2727 - val_accuracy: 0.9274 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2575 - accuracy: 0.9309 - val_loss: 0.2725 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2541 - accuracy: 0.9338 - val_loss: 0.2719 - val_accuracy: 0.9264 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2553 - accuracy: 0.9351 - val_loss: 0.2725 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2544 - accuracy: 0.9338 - val_loss: 0.2698 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2533 - accuracy: 0.9351 - val_loss: 0.2718 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2483 - accuracy: 0.9350 - val_loss: 0.2721 - val_accuracy: 0.9268 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2548 - accuracy: 0.9334 - val_loss: 0.2710 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2471 - accuracy: 0.9352 - val_loss: 0.2709 - val_accuracy: 0.9284 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2523 - accuracy: 0.9332 - val_loss: 0.2707 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2506 - accuracy: 0.9344 - val_loss: 0.2706 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2518 - accuracy: 0.9340 - val_loss: 0.2708 - val_accuracy: 0.9288 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2502 - accuracy: 0.9317 - val_loss: 0.2708 - val_accuracy: 0.9286 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2496 - accuracy: 0.9344 - val_loss: 0.2708 - val_accuracy: 0.9288 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2519 - accuracy: 0.9314 - val_loss: 0.2706 - val_accuracy: 0.9286 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2488 - accuracy: 0.9343 - val_loss: 0.2707 - val_accuracy: 0.9286 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 6s 10ms/step - loss: 0.2820 - accuracy: 0.9242\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5836, TN:9825, FP:630, FN:655, loss0.28204667568206787, acc0.9241708957866163, sn0.89909104914497, sp0.93974175035868, f10.9008258084433125, auc0.9730539677459449\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 51ms/step - loss: 3.9274 - accuracy: 0.6630 - val_loss: 3.5717 - val_accuracy: 0.3961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 2.5542 - accuracy: 0.7978 - val_loss: 2.6439 - val_accuracy: 0.4204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 1.7019 - accuracy: 0.8412 - val_loss: 1.5639 - val_accuracy: 0.7453 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 1.2086 - accuracy: 0.8593 - val_loss: 1.0303 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.9342 - accuracy: 0.8682 - val_loss: 0.8130 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.7681 - accuracy: 0.8748 - val_loss: 0.7344 - val_accuracy: 0.8664 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.6571 - accuracy: 0.8838 - val_loss: 0.6760 - val_accuracy: 0.8650 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5749 - accuracy: 0.8958 - val_loss: 0.5438 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.5402 - accuracy: 0.8920 - val_loss: 0.4957 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5054 - accuracy: 0.8975 - val_loss: 0.4654 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4782 - accuracy: 0.8991 - val_loss: 0.4930 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4766 - accuracy: 0.8941 - val_loss: 0.4346 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4515 - accuracy: 0.9011 - val_loss: 0.6217 - val_accuracy: 0.8345 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4337 - accuracy: 0.9002 - val_loss: 0.4183 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4323 - accuracy: 0.9048 - val_loss: 0.4217 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4214 - accuracy: 0.9051 - val_loss: 0.4223 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4216 - accuracy: 0.9042 - val_loss: 0.4405 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4070 - accuracy: 0.9073 - val_loss: 0.3790 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4063 - accuracy: 0.9070 - val_loss: 0.4052 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4036 - accuracy: 0.9061 - val_loss: 0.4022 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4095 - accuracy: 0.9065 - val_loss: 0.4250 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4054 - accuracy: 0.9054 - val_loss: 0.3808 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3987 - accuracy: 0.9099 - val_loss: 0.4386 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4027 - accuracy: 0.9077 - val_loss: 0.4643 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3977 - accuracy: 0.9073 - val_loss: 0.3932 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3992 - accuracy: 0.9066 - val_loss: 0.3865 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4029 - accuracy: 0.9059 - val_loss: 0.4033 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3963 - accuracy: 0.9081 - val_loss: 0.4031 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3964 - accuracy: 0.9055 - val_loss: 0.3992 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3947 - accuracy: 0.9088 - val_loss: 0.3861 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3862 - accuracy: 0.9105 - val_loss: 0.4408 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3879 - accuracy: 0.9103 - val_loss: 0.3727 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3880 - accuracy: 0.9059 - val_loss: 0.3603 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3828 - accuracy: 0.9104 - val_loss: 0.3904 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3842 - accuracy: 0.9112 - val_loss: 0.4099 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3800 - accuracy: 0.9114 - val_loss: 0.4312 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3902 - accuracy: 0.9031 - val_loss: 0.3675 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3854 - accuracy: 0.9110 - val_loss: 0.3999 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3900 - accuracy: 0.9056 - val_loss: 0.3848 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3826 - accuracy: 0.9067 - val_loss: 0.3973 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3786 - accuracy: 0.9086 - val_loss: 0.3609 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3753 - accuracy: 0.9110 - val_loss: 0.3602 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3763 - accuracy: 0.9113 - val_loss: 0.4065 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3767 - accuracy: 0.9073 - val_loss: 0.4410 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3799 - accuracy: 0.9072 - val_loss: 0.4533 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3767 - accuracy: 0.9106 - val_loss: 0.3841 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3711 - accuracy: 0.9093 - val_loss: 0.3483 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3762 - accuracy: 0.9113 - val_loss: 0.3617 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3790 - accuracy: 0.9132 - val_loss: 0.3708 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3850 - accuracy: 0.9088 - val_loss: 0.4468 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 6s 63ms/step - loss: 0.3733 - accuracy: 0.9120 - val_loss: 0.3618 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3745 - accuracy: 0.9110 - val_loss: 0.3731 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3763 - accuracy: 0.9114 - val_loss: 0.4112 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3701 - accuracy: 0.9108 - val_loss: 0.3930 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3742 - accuracy: 0.9102 - val_loss: 0.3619 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3703 - accuracy: 0.9142 - val_loss: 0.3851 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3737 - accuracy: 0.9126 - val_loss: 0.3718 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3725 - accuracy: 0.9146 - val_loss: 0.4609 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3822 - accuracy: 0.9117 - val_loss: 0.3614 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3746 - accuracy: 0.9119 - val_loss: 0.3871 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3768 - accuracy: 0.9102 - val_loss: 0.3614 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3689 - accuracy: 0.9120 - val_loss: 0.3928 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3792 - accuracy: 0.9070 - val_loss: 0.3556 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3676 - accuracy: 0.9091 - val_loss: 0.3583 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3716 - accuracy: 0.9094 - val_loss: 0.4173 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3752 - accuracy: 0.9107 - val_loss: 0.3707 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3633 - accuracy: 0.9155 - val_loss: 0.3611 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3699 - accuracy: 0.9091 - val_loss: 0.3766 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3601 - accuracy: 0.9153 - val_loss: 0.3932 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3701 - accuracy: 0.9072 - val_loss: 0.3657 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3703 - accuracy: 0.9126 - val_loss: 0.4004 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3492 - accuracy: 0.9222 - val_loss: 0.3482 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3383 - accuracy: 0.9214 - val_loss: 0.3503 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3281 - accuracy: 0.9217 - val_loss: 0.3332 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3118 - accuracy: 0.9265 - val_loss: 0.3248 - val_accuracy: 0.9226 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3073 - accuracy: 0.9266 - val_loss: 0.3056 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2969 - accuracy: 0.9272 - val_loss: 0.2981 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2902 - accuracy: 0.9293 - val_loss: 0.2935 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2823 - accuracy: 0.9273 - val_loss: 0.2912 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2788 - accuracy: 0.9247 - val_loss: 0.2887 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2746 - accuracy: 0.9293 - val_loss: 0.3021 - val_accuracy: 0.9224 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2709 - accuracy: 0.9291 - val_loss: 0.2825 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2669 - accuracy: 0.9304 - val_loss: 0.2802 - val_accuracy: 0.9266 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2649 - accuracy: 0.9318 - val_loss: 0.2795 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2656 - accuracy: 0.9321 - val_loss: 0.2792 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2637 - accuracy: 0.9322 - val_loss: 0.2784 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2640 - accuracy: 0.9309 - val_loss: 0.2786 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2594 - accuracy: 0.9334 - val_loss: 0.2769 - val_accuracy: 0.9282 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2638 - accuracy: 0.9301 - val_loss: 0.2778 - val_accuracy: 0.9272 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2607 - accuracy: 0.9347 - val_loss: 0.2771 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2602 - accuracy: 0.9331 - val_loss: 0.2748 - val_accuracy: 0.9268 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2553 - accuracy: 0.9318 - val_loss: 0.2758 - val_accuracy: 0.9266 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2574 - accuracy: 0.9340 - val_loss: 0.2761 - val_accuracy: 0.9266 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2592 - accuracy: 0.9330 - val_loss: 0.2762 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2590 - accuracy: 0.9314 - val_loss: 0.2762 - val_accuracy: 0.9270 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2533 - accuracy: 0.9332 - val_loss: 0.2763 - val_accuracy: 0.9270 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2570 - accuracy: 0.9326 - val_loss: 0.2761 - val_accuracy: 0.9272 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2575 - accuracy: 0.9335 - val_loss: 0.2761 - val_accuracy: 0.9268 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2555 - accuracy: 0.9324 - val_loss: 0.2761 - val_accuracy: 0.9268 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2613 - accuracy: 0.9312 - val_loss: 0.2762 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2870 - accuracy: 0.9249\n",
      "17/17 [==============================] - 2s 25ms/step\n",
      "TP:5800, TN:9874, FP:581, FN:691, loss0.2870049476623535, acc0.9249380384751564, sn0.8935449083346172, sp0.9444285031085605, f10.901180857675575, auc0.973077618194961\n",
      "Average Test loss:  0.28405198752880095\n",
      "Average Accuracy:  0.9250206538416146\n",
      "Average Sensitivity:  0.8953936219380679\n",
      "Average Specificity:  0.9434146341463416\n",
      "Average F1 Score:  0.9014630985258888\n",
      "Average AUC Score:  0.9732108969480677\n",
      "AUC for ROC curve 1: 0.9735\n",
      "AUC for ROC curve 2: 0.9735\n",
      "AUC for ROC curve 3: 0.9733\n",
      "AUC for ROC curve 4: 0.9733\n",
      "AUC for ROC curve 5: 0.9737\n",
      "AUC for ROC curve 6: 0.9737\n",
      "AUC for ROC curve 7: 0.9727\n",
      "AUC for ROC curve 8: 0.9727\n",
      "AUC for ROC curve 9: 0.9732\n",
      "AUC for ROC curve 10: 0.9732\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f9963-cfa7-47f5-8701-2a24e58e20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
