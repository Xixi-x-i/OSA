{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 16:33:00.737310: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = BatchNormalization()(input1)    \n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "    x1 = Conv1D(256, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "\n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "    x2 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    \n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    \n",
    "    \n",
    "\n",
    "    # CNN-3\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "    x3 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2) \n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 21s 40ms/step - loss: 4.3696 - accuracy: 0.6424 - val_loss: 5.1540 - val_accuracy: 0.3889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 3.2963 - accuracy: 0.7897 - val_loss: 2.9367 - val_accuracy: 0.7718 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 2.5109 - accuracy: 0.8329 - val_loss: 2.2829 - val_accuracy: 0.7714 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 1.9157 - accuracy: 0.8484 - val_loss: 1.7670 - val_accuracy: 0.8103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.4880 - accuracy: 0.8632 - val_loss: 1.4017 - val_accuracy: 0.8237 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.1795 - accuracy: 0.8711 - val_loss: 1.1503 - val_accuracy: 0.8289 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.9523 - accuracy: 0.8799 - val_loss: 1.0163 - val_accuracy: 0.8263 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.7985 - accuracy: 0.8925 - val_loss: 0.9387 - val_accuracy: 0.8213 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6893 - accuracy: 0.8925 - val_loss: 0.6976 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6075 - accuracy: 0.8991 - val_loss: 0.7453 - val_accuracy: 0.8410 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5663 - accuracy: 0.8967 - val_loss: 0.5176 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5075 - accuracy: 0.9038 - val_loss: 0.4804 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4852 - accuracy: 0.8993 - val_loss: 0.4981 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4618 - accuracy: 0.9032 - val_loss: 0.5292 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4583 - accuracy: 0.8995 - val_loss: 0.5035 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4341 - accuracy: 0.9051 - val_loss: 0.5006 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4279 - accuracy: 0.9043 - val_loss: 0.4094 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4123 - accuracy: 0.9069 - val_loss: 0.4501 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4037 - accuracy: 0.9044 - val_loss: 0.4127 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3991 - accuracy: 0.9074 - val_loss: 0.3857 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3993 - accuracy: 0.9077 - val_loss: 0.4515 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4019 - accuracy: 0.9045 - val_loss: 0.3724 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3874 - accuracy: 0.9077 - val_loss: 0.5286 - val_accuracy: 0.8654 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3973 - accuracy: 0.9026 - val_loss: 0.3919 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3893 - accuracy: 0.9066 - val_loss: 0.4928 - val_accuracy: 0.8642 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3868 - accuracy: 0.9058 - val_loss: 0.3865 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3849 - accuracy: 0.9093 - val_loss: 0.4028 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3908 - accuracy: 0.9073 - val_loss: 0.3777 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3788 - accuracy: 0.9097 - val_loss: 0.3655 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3766 - accuracy: 0.9060 - val_loss: 0.4914 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3775 - accuracy: 0.9085 - val_loss: 0.4160 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3865 - accuracy: 0.9081 - val_loss: 0.4536 - val_accuracy: 0.8777 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3794 - accuracy: 0.9102 - val_loss: 0.4769 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3804 - accuracy: 0.9094 - val_loss: 0.3733 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3824 - accuracy: 0.9063 - val_loss: 0.3812 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3748 - accuracy: 0.9109 - val_loss: 0.5100 - val_accuracy: 0.8634 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3850 - accuracy: 0.9075 - val_loss: 0.4404 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3765 - accuracy: 0.9061 - val_loss: 0.3646 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3701 - accuracy: 0.9108 - val_loss: 0.3611 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3792 - accuracy: 0.9079 - val_loss: 0.3630 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3611 - accuracy: 0.9165 - val_loss: 0.4924 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3758 - accuracy: 0.9088 - val_loss: 0.3714 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3731 - accuracy: 0.9091 - val_loss: 0.3643 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3714 - accuracy: 0.9093 - val_loss: 0.3779 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3690 - accuracy: 0.9091 - val_loss: 0.4212 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3706 - accuracy: 0.9096 - val_loss: 0.4610 - val_accuracy: 0.8767 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3661 - accuracy: 0.9092 - val_loss: 0.3965 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3651 - accuracy: 0.9095 - val_loss: 0.4151 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3658 - accuracy: 0.9105 - val_loss: 0.4459 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3698 - accuracy: 0.9091 - val_loss: 0.3669 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3673 - accuracy: 0.9104 - val_loss: 0.3725 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3731 - accuracy: 0.9085 - val_loss: 0.4052 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3610 - accuracy: 0.9099 - val_loss: 0.3688 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3683 - accuracy: 0.9126 - val_loss: 0.3555 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3671 - accuracy: 0.9100 - val_loss: 0.3831 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3623 - accuracy: 0.9088 - val_loss: 0.3862 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3645 - accuracy: 0.9119 - val_loss: 0.3537 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3599 - accuracy: 0.9112 - val_loss: 0.4181 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3490 - accuracy: 0.9173 - val_loss: 0.3379 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3539 - accuracy: 0.9150 - val_loss: 0.3586 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3612 - accuracy: 0.9128 - val_loss: 0.5511 - val_accuracy: 0.8570 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3674 - accuracy: 0.9091 - val_loss: 0.3569 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3644 - accuracy: 0.9087 - val_loss: 0.3574 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3584 - accuracy: 0.9133 - val_loss: 0.3667 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3657 - accuracy: 0.9114 - val_loss: 0.4350 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3601 - accuracy: 0.9133 - val_loss: 0.3720 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3604 - accuracy: 0.9125 - val_loss: 0.3532 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3628 - accuracy: 0.9126 - val_loss: 0.3640 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3593 - accuracy: 0.9156 - val_loss: 0.3673 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3679 - accuracy: 0.9114 - val_loss: 0.4163 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3601 - accuracy: 0.9102 - val_loss: 0.3463 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3343 - accuracy: 0.9213 - val_loss: 0.3579 - val_accuracy: 0.9105 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3206 - accuracy: 0.9238 - val_loss: 0.3496 - val_accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3066 - accuracy: 0.9284 - val_loss: 0.3300 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2925 - accuracy: 0.9267 - val_loss: 0.3191 - val_accuracy: 0.9128 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2872 - accuracy: 0.9290 - val_loss: 0.3232 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2854 - accuracy: 0.9273 - val_loss: 0.3194 - val_accuracy: 0.9107 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2737 - accuracy: 0.9279 - val_loss: 0.3017 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2698 - accuracy: 0.9273 - val_loss: 0.3415 - val_accuracy: 0.9037 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2631 - accuracy: 0.9309 - val_loss: 0.2814 - val_accuracy: 0.9172 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2618 - accuracy: 0.9311 - val_loss: 0.2829 - val_accuracy: 0.9182 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2514 - accuracy: 0.9355 - val_loss: 0.2724 - val_accuracy: 0.9212 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2527 - accuracy: 0.9327 - val_loss: 0.2712 - val_accuracy: 0.9208 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2479 - accuracy: 0.9328 - val_loss: 0.2746 - val_accuracy: 0.9208 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2533 - accuracy: 0.9323 - val_loss: 0.2732 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2480 - accuracy: 0.9314 - val_loss: 0.2731 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2467 - accuracy: 0.9330 - val_loss: 0.2699 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2488 - accuracy: 0.9321 - val_loss: 0.2702 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2400 - accuracy: 0.9382 - val_loss: 0.2720 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 29ms/step - loss: 0.2480 - accuracy: 0.9344 - val_loss: 0.2701 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2478 - accuracy: 0.9351 - val_loss: 0.2687 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2436 - accuracy: 0.9344 - val_loss: 0.2674 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.2513 - accuracy: 0.9300 - val_loss: 0.2673 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2420 - accuracy: 0.9359 - val_loss: 0.2670 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2457 - accuracy: 0.9335 - val_loss: 0.2668 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 0.2448 - accuracy: 0.9344 - val_loss: 0.2672 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.2461 - accuracy: 0.9317 - val_loss: 0.2671 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2418 - accuracy: 0.9355 - val_loss: 0.2671 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2479 - accuracy: 0.9312 - val_loss: 0.2664 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2432 - accuracy: 0.9369 - val_loss: 0.2665 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2746 - accuracy: 0.9245\n",
      "17/17 [==============================] - 1s 28ms/step\n",
      "TP:5686, TN:9980, FP:475, FN:805, loss0.27464884519577026, acc0.924465950666824, sn0.8759821291018333, sp0.9545671927307509, f10.8988302244704394, auc0.9735838556877598\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 43ms/step - loss: 4.4445 - accuracy: 0.6383 - val_loss: 4.1934 - val_accuracy: 0.4105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 3.3930 - accuracy: 0.7724 - val_loss: 3.0249 - val_accuracy: 0.7603 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 2.6320 - accuracy: 0.8224 - val_loss: 2.3006 - val_accuracy: 0.8369 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.0341 - accuracy: 0.8459 - val_loss: 1.8087 - val_accuracy: 0.8408 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.5919 - accuracy: 0.8580 - val_loss: 1.4052 - val_accuracy: 0.8608 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.2656 - accuracy: 0.8705 - val_loss: 1.1804 - val_accuracy: 0.8458 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.0336 - accuracy: 0.8762 - val_loss: 0.9462 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.8547 - accuracy: 0.8882 - val_loss: 0.9310 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.7366 - accuracy: 0.8920 - val_loss: 0.6726 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6422 - accuracy: 0.8969 - val_loss: 0.6315 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5862 - accuracy: 0.8979 - val_loss: 0.5538 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5362 - accuracy: 0.9008 - val_loss: 0.6729 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5059 - accuracy: 0.8991 - val_loss: 0.4823 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4848 - accuracy: 0.8962 - val_loss: 0.4810 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4526 - accuracy: 0.9024 - val_loss: 0.4552 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4418 - accuracy: 0.9013 - val_loss: 0.4835 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4299 - accuracy: 0.9039 - val_loss: 0.5034 - val_accuracy: 0.8738 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4233 - accuracy: 0.9019 - val_loss: 0.4044 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4170 - accuracy: 0.9008 - val_loss: 0.4023 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4143 - accuracy: 0.9038 - val_loss: 0.4506 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3972 - accuracy: 0.9049 - val_loss: 0.3797 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3970 - accuracy: 0.9070 - val_loss: 0.3931 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3955 - accuracy: 0.9069 - val_loss: 0.3897 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3957 - accuracy: 0.9076 - val_loss: 0.3828 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3897 - accuracy: 0.9080 - val_loss: 0.4289 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3836 - accuracy: 0.9075 - val_loss: 0.4523 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3864 - accuracy: 0.9066 - val_loss: 0.4006 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3841 - accuracy: 0.9083 - val_loss: 0.4121 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3802 - accuracy: 0.9105 - val_loss: 0.4291 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3842 - accuracy: 0.9078 - val_loss: 0.3542 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3772 - accuracy: 0.9079 - val_loss: 0.4063 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3806 - accuracy: 0.9073 - val_loss: 0.5220 - val_accuracy: 0.8580 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3765 - accuracy: 0.9078 - val_loss: 0.3611 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3723 - accuracy: 0.9099 - val_loss: 0.3933 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3704 - accuracy: 0.9108 - val_loss: 0.3788 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3771 - accuracy: 0.9091 - val_loss: 0.3521 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3772 - accuracy: 0.9060 - val_loss: 0.4187 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3760 - accuracy: 0.9073 - val_loss: 0.4224 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3687 - accuracy: 0.9085 - val_loss: 0.3426 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3669 - accuracy: 0.9108 - val_loss: 0.4033 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3720 - accuracy: 0.9065 - val_loss: 0.3561 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3691 - accuracy: 0.9114 - val_loss: 0.3739 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3671 - accuracy: 0.9110 - val_loss: 0.3749 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3694 - accuracy: 0.9110 - val_loss: 0.3619 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3737 - accuracy: 0.9077 - val_loss: 0.3558 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3606 - accuracy: 0.9154 - val_loss: 0.3944 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3692 - accuracy: 0.9080 - val_loss: 0.3873 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3560 - accuracy: 0.9148 - val_loss: 0.3962 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3753 - accuracy: 0.9120 - val_loss: 0.3455 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3598 - accuracy: 0.9114 - val_loss: 0.3607 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3686 - accuracy: 0.9098 - val_loss: 0.3857 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3625 - accuracy: 0.9104 - val_loss: 0.3987 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3694 - accuracy: 0.9106 - val_loss: 0.4533 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3624 - accuracy: 0.9127 - val_loss: 0.3558 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3682 - accuracy: 0.9124 - val_loss: 0.3842 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3662 - accuracy: 0.9133 - val_loss: 0.4131 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3656 - accuracy: 0.9119 - val_loss: 0.3760 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3617 - accuracy: 0.9138 - val_loss: 0.3468 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3647 - accuracy: 0.9113 - val_loss: 0.3623 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3546 - accuracy: 0.9123 - val_loss: 0.3531 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3676 - accuracy: 0.9127 - val_loss: 0.3741 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3638 - accuracy: 0.9102 - val_loss: 0.3534 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3527 - accuracy: 0.9095 - val_loss: 0.3623 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3590 - accuracy: 0.9114 - val_loss: 0.3601 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3594 - accuracy: 0.9138 - val_loss: 0.3983 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3572 - accuracy: 0.9125 - val_loss: 0.3835 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3670 - accuracy: 0.9109 - val_loss: 0.4813 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3520 - accuracy: 0.9145 - val_loss: 0.3364 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3566 - accuracy: 0.9128 - val_loss: 0.3625 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3540 - accuracy: 0.9139 - val_loss: 0.3555 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3646 - accuracy: 0.9088 - val_loss: 0.4001 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3358 - accuracy: 0.9211 - val_loss: 0.3456 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3146 - accuracy: 0.9257 - val_loss: 0.3351 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3018 - accuracy: 0.9272 - val_loss: 0.3467 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2967 - accuracy: 0.9267 - val_loss: 0.3150 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2895 - accuracy: 0.9278 - val_loss: 0.3076 - val_accuracy: 0.9146 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2799 - accuracy: 0.9295 - val_loss: 0.3082 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2742 - accuracy: 0.9304 - val_loss: 0.3114 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2687 - accuracy: 0.9293 - val_loss: 0.3016 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2680 - accuracy: 0.9303 - val_loss: 0.2805 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2630 - accuracy: 0.9283 - val_loss: 0.2675 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2560 - accuracy: 0.9314 - val_loss: 0.2747 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2528 - accuracy: 0.9320 - val_loss: 0.2708 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2482 - accuracy: 0.9325 - val_loss: 0.2665 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2530 - accuracy: 0.9331 - val_loss: 0.2662 - val_accuracy: 0.9240 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2463 - accuracy: 0.9352 - val_loss: 0.2643 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2484 - accuracy: 0.9343 - val_loss: 0.2632 - val_accuracy: 0.9248 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2467 - accuracy: 0.9344 - val_loss: 0.2636 - val_accuracy: 0.9250 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2495 - accuracy: 0.9316 - val_loss: 0.2658 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2499 - accuracy: 0.9316 - val_loss: 0.2644 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2492 - accuracy: 0.9349 - val_loss: 0.2615 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2459 - accuracy: 0.9346 - val_loss: 0.2610 - val_accuracy: 0.9272 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2447 - accuracy: 0.9326 - val_loss: 0.2608 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2463 - accuracy: 0.9359 - val_loss: 0.2609 - val_accuracy: 0.9272 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2465 - accuracy: 0.9345 - val_loss: 0.2610 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2491 - accuracy: 0.9326 - val_loss: 0.2611 - val_accuracy: 0.9272 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2463 - accuracy: 0.9320 - val_loss: 0.2611 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2486 - accuracy: 0.9337 - val_loss: 0.2611 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2508 - accuracy: 0.9312 - val_loss: 0.2606 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2426 - accuracy: 0.9334 - val_loss: 0.2609 - val_accuracy: 0.9270 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2703 - accuracy: 0.9271\n",
      "17/17 [==============================] - 1s 18ms/step\n",
      "TP:5756, TN:9954, FP:501, FN:735, loss0.2702564001083374, acc0.927062433612652, sn0.8867662917886304, sp0.952080344332855, f10.9030436146846565, auc0.974138580284912\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 21s 47ms/step - loss: 4.3690 - accuracy: 0.6522 - val_loss: 4.3559 - val_accuracy: 0.4408 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 3.3306 - accuracy: 0.7861 - val_loss: 2.8910 - val_accuracy: 0.8153 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.5792 - accuracy: 0.8170 - val_loss: 2.2427 - val_accuracy: 0.8339 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.9702 - accuracy: 0.8484 - val_loss: 1.7269 - val_accuracy: 0.8408 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.5367 - accuracy: 0.8566 - val_loss: 1.4207 - val_accuracy: 0.8349 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.2223 - accuracy: 0.8709 - val_loss: 1.0923 - val_accuracy: 0.8658 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.9907 - accuracy: 0.8732 - val_loss: 0.9630 - val_accuracy: 0.8484 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.8318 - accuracy: 0.8814 - val_loss: 0.8112 - val_accuracy: 0.8632 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7105 - accuracy: 0.8893 - val_loss: 0.8368 - val_accuracy: 0.8355 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.6238 - accuracy: 0.8969 - val_loss: 0.6309 - val_accuracy: 0.8740 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5723 - accuracy: 0.8944 - val_loss: 0.6118 - val_accuracy: 0.8738 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5297 - accuracy: 0.8965 - val_loss: 0.5020 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4938 - accuracy: 0.8989 - val_loss: 0.5034 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4594 - accuracy: 0.9059 - val_loss: 0.4645 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4493 - accuracy: 0.9055 - val_loss: 0.4430 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4366 - accuracy: 0.9074 - val_loss: 0.4122 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4267 - accuracy: 0.9081 - val_loss: 0.4046 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4225 - accuracy: 0.9068 - val_loss: 0.4081 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4193 - accuracy: 0.9028 - val_loss: 0.4285 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3984 - accuracy: 0.9091 - val_loss: 0.5666 - val_accuracy: 0.8524 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4009 - accuracy: 0.9073 - val_loss: 0.3850 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3948 - accuracy: 0.9096 - val_loss: 0.3880 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3956 - accuracy: 0.9085 - val_loss: 0.4098 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3875 - accuracy: 0.9112 - val_loss: 0.3774 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3937 - accuracy: 0.9061 - val_loss: 0.4545 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3892 - accuracy: 0.9086 - val_loss: 0.4006 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3916 - accuracy: 0.9053 - val_loss: 0.4506 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3798 - accuracy: 0.9095 - val_loss: 0.4162 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3860 - accuracy: 0.9073 - val_loss: 0.3780 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3895 - accuracy: 0.9095 - val_loss: 0.4336 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3877 - accuracy: 0.9079 - val_loss: 0.3632 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3819 - accuracy: 0.9091 - val_loss: 0.4483 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3715 - accuracy: 0.9101 - val_loss: 0.3703 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3752 - accuracy: 0.9097 - val_loss: 0.5253 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3836 - accuracy: 0.9042 - val_loss: 0.4042 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3786 - accuracy: 0.9101 - val_loss: 0.3418 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3758 - accuracy: 0.9083 - val_loss: 0.3867 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3732 - accuracy: 0.9086 - val_loss: 0.4074 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3711 - accuracy: 0.9080 - val_loss: 0.4978 - val_accuracy: 0.8688 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3761 - accuracy: 0.9086 - val_loss: 0.3657 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3677 - accuracy: 0.9131 - val_loss: 0.3957 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3690 - accuracy: 0.9125 - val_loss: 0.4089 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3721 - accuracy: 0.9092 - val_loss: 0.3666 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3659 - accuracy: 0.9112 - val_loss: 0.6746 - val_accuracy: 0.8207 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3775 - accuracy: 0.9100 - val_loss: 0.3895 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3624 - accuracy: 0.9146 - val_loss: 0.3494 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3711 - accuracy: 0.9109 - val_loss: 0.3647 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3710 - accuracy: 0.9113 - val_loss: 0.3667 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3626 - accuracy: 0.9138 - val_loss: 0.3595 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3682 - accuracy: 0.9107 - val_loss: 0.3973 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3644 - accuracy: 0.9127 - val_loss: 0.3498 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3677 - accuracy: 0.9126 - val_loss: 0.5173 - val_accuracy: 0.8650 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3674 - accuracy: 0.9138 - val_loss: 0.5272 - val_accuracy: 0.8516 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3641 - accuracy: 0.9106 - val_loss: 0.3682 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3575 - accuracy: 0.9160 - val_loss: 0.3551 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3671 - accuracy: 0.9123 - val_loss: 0.3901 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3660 - accuracy: 0.9088 - val_loss: 0.4063 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3571 - accuracy: 0.9119 - val_loss: 0.3777 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3724 - accuracy: 0.9107 - val_loss: 0.3708 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3749 - accuracy: 0.9084 - val_loss: 0.3824 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3643 - accuracy: 0.9117 - val_loss: 0.3687 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3633 - accuracy: 0.9137 - val_loss: 0.3432 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3593 - accuracy: 0.9132 - val_loss: 0.3582 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3596 - accuracy: 0.9130 - val_loss: 0.3503 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3583 - accuracy: 0.9149 - val_loss: 0.3529 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3580 - accuracy: 0.9121 - val_loss: 0.3461 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3631 - accuracy: 0.9141 - val_loss: 0.3988 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3643 - accuracy: 0.9122 - val_loss: 0.3867 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3599 - accuracy: 0.9101 - val_loss: 0.3401 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3553 - accuracy: 0.9126 - val_loss: 0.4078 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3536 - accuracy: 0.9123 - val_loss: 0.3746 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3373 - accuracy: 0.9201 - val_loss: 0.3594 - val_accuracy: 0.9083 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3089 - accuracy: 0.9244 - val_loss: 0.3425 - val_accuracy: 0.9107 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3017 - accuracy: 0.9264 - val_loss: 0.3387 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2969 - accuracy: 0.9245 - val_loss: 0.3033 - val_accuracy: 0.9158 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2868 - accuracy: 0.9255 - val_loss: 0.3029 - val_accuracy: 0.9190 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2795 - accuracy: 0.9278 - val_loss: 0.3087 - val_accuracy: 0.9132 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2680 - accuracy: 0.9303 - val_loss: 0.2918 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2635 - accuracy: 0.9285 - val_loss: 0.3185 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2668 - accuracy: 0.9298 - val_loss: 0.2844 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2615 - accuracy: 0.9335 - val_loss: 0.2788 - val_accuracy: 0.9216 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2511 - accuracy: 0.9337 - val_loss: 0.2734 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2554 - accuracy: 0.9301 - val_loss: 0.2719 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2475 - accuracy: 0.9352 - val_loss: 0.2723 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2543 - accuracy: 0.9324 - val_loss: 0.2710 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2500 - accuracy: 0.9332 - val_loss: 0.2686 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2515 - accuracy: 0.9306 - val_loss: 0.2669 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2506 - accuracy: 0.9334 - val_loss: 0.2665 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2431 - accuracy: 0.9353 - val_loss: 0.2677 - val_accuracy: 0.9240 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2426 - accuracy: 0.9348 - val_loss: 0.2663 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2469 - accuracy: 0.9334 - val_loss: 0.2682 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2417 - accuracy: 0.9338 - val_loss: 0.2662 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2436 - accuracy: 0.9344 - val_loss: 0.2650 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2453 - accuracy: 0.9320 - val_loss: 0.2641 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2455 - accuracy: 0.9334 - val_loss: 0.2639 - val_accuracy: 0.9258 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2426 - accuracy: 0.9350 - val_loss: 0.2645 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2440 - accuracy: 0.9333 - val_loss: 0.2638 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2405 - accuracy: 0.9333 - val_loss: 0.2635 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2370 - accuracy: 0.9377 - val_loss: 0.2638 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2427 - accuracy: 0.9330 - val_loss: 0.2637 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2734 - accuracy: 0.9245\n",
      "17/17 [==============================] - 1s 18ms/step\n",
      "TP:5680, TN:9987, FP:468, FN:811, loss0.27344536781311035, acc0.9245249616428656, sn0.8750577723001078, sp0.9552367288378766, f10.8988052852282616, auc0.9736613937953746\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 48ms/step - loss: 4.3336 - accuracy: 0.6499 - val_loss: 4.0447 - val_accuracy: 0.4581 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 3.2523 - accuracy: 0.7905 - val_loss: 2.8016 - val_accuracy: 0.8239 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.4702 - accuracy: 0.8274 - val_loss: 2.1359 - val_accuracy: 0.8422 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.8823 - accuracy: 0.8454 - val_loss: 1.7095 - val_accuracy: 0.8179 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.4449 - accuracy: 0.8638 - val_loss: 1.3213 - val_accuracy: 0.8363 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.1432 - accuracy: 0.8738 - val_loss: 1.0759 - val_accuracy: 0.8560 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.9281 - accuracy: 0.8852 - val_loss: 0.9401 - val_accuracy: 0.8442 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.7922 - accuracy: 0.8835 - val_loss: 0.7421 - val_accuracy: 0.8879 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.6733 - accuracy: 0.8964 - val_loss: 0.7349 - val_accuracy: 0.8550 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6051 - accuracy: 0.8975 - val_loss: 0.6127 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5526 - accuracy: 0.8989 - val_loss: 0.5814 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5123 - accuracy: 0.8992 - val_loss: 0.5127 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4865 - accuracy: 0.9002 - val_loss: 0.4482 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4651 - accuracy: 0.9016 - val_loss: 0.4831 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4517 - accuracy: 0.9001 - val_loss: 0.4741 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4320 - accuracy: 0.9047 - val_loss: 0.4090 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4237 - accuracy: 0.9076 - val_loss: 0.5054 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4247 - accuracy: 0.9056 - val_loss: 0.4184 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4104 - accuracy: 0.9068 - val_loss: 0.4417 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4101 - accuracy: 0.9054 - val_loss: 0.3935 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4031 - accuracy: 0.9072 - val_loss: 0.4407 - val_accuracy: 0.8965 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3968 - accuracy: 0.9071 - val_loss: 0.3753 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3899 - accuracy: 0.9101 - val_loss: 0.3853 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4078 - accuracy: 0.8997 - val_loss: 0.4016 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3943 - accuracy: 0.9056 - val_loss: 0.3915 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3846 - accuracy: 0.9101 - val_loss: 0.3724 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3930 - accuracy: 0.9062 - val_loss: 0.4490 - val_accuracy: 0.8853 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3871 - accuracy: 0.9074 - val_loss: 0.3650 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3811 - accuracy: 0.9079 - val_loss: 0.3676 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3869 - accuracy: 0.9055 - val_loss: 0.3935 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3779 - accuracy: 0.9113 - val_loss: 0.3717 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3792 - accuracy: 0.9086 - val_loss: 0.3882 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3887 - accuracy: 0.9045 - val_loss: 0.3851 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3819 - accuracy: 0.9080 - val_loss: 0.4244 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3756 - accuracy: 0.9128 - val_loss: 0.3846 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3727 - accuracy: 0.9085 - val_loss: 0.4340 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3789 - accuracy: 0.9080 - val_loss: 0.4618 - val_accuracy: 0.8714 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3752 - accuracy: 0.9068 - val_loss: 0.3972 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3753 - accuracy: 0.9109 - val_loss: 0.3593 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3698 - accuracy: 0.9104 - val_loss: 0.3678 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3680 - accuracy: 0.9099 - val_loss: 0.3666 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3682 - accuracy: 0.9107 - val_loss: 0.3656 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3615 - accuracy: 0.9114 - val_loss: 0.3899 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3717 - accuracy: 0.9078 - val_loss: 0.4032 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3705 - accuracy: 0.9133 - val_loss: 0.3807 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3733 - accuracy: 0.9122 - val_loss: 0.3963 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3712 - accuracy: 0.9112 - val_loss: 0.3638 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3738 - accuracy: 0.9088 - val_loss: 0.3678 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3579 - accuracy: 0.9142 - val_loss: 0.4481 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3729 - accuracy: 0.9071 - val_loss: 0.3985 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3690 - accuracy: 0.9104 - val_loss: 0.3933 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3673 - accuracy: 0.9110 - val_loss: 0.4030 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3650 - accuracy: 0.9124 - val_loss: 0.3484 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3676 - accuracy: 0.9130 - val_loss: 0.3886 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3648 - accuracy: 0.9099 - val_loss: 0.4308 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3734 - accuracy: 0.9114 - val_loss: 0.3611 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3673 - accuracy: 0.9105 - val_loss: 0.3807 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3582 - accuracy: 0.9159 - val_loss: 0.4420 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3623 - accuracy: 0.9143 - val_loss: 0.3583 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3655 - accuracy: 0.9121 - val_loss: 0.4095 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3656 - accuracy: 0.9115 - val_loss: 0.4116 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3780 - accuracy: 0.9091 - val_loss: 0.4523 - val_accuracy: 0.8765 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3577 - accuracy: 0.9149 - val_loss: 0.3714 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3757 - accuracy: 0.9079 - val_loss: 0.3917 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3616 - accuracy: 0.9109 - val_loss: 0.3503 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3586 - accuracy: 0.9144 - val_loss: 0.3866 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3594 - accuracy: 0.9114 - val_loss: 0.3570 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3614 - accuracy: 0.9130 - val_loss: 0.3528 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3534 - accuracy: 0.9153 - val_loss: 0.4483 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3627 - accuracy: 0.9112 - val_loss: 0.3416 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3717 - accuracy: 0.9091 - val_loss: 0.4504 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3330 - accuracy: 0.9231 - val_loss: 0.3484 - val_accuracy: 0.9122 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3119 - accuracy: 0.9238 - val_loss: 0.3384 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3026 - accuracy: 0.9259 - val_loss: 0.3390 - val_accuracy: 0.9089 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2923 - accuracy: 0.9269 - val_loss: 0.3363 - val_accuracy: 0.9097 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2797 - accuracy: 0.9298 - val_loss: 0.3177 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2787 - accuracy: 0.9291 - val_loss: 0.3036 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2706 - accuracy: 0.9288 - val_loss: 0.2941 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2637 - accuracy: 0.9305 - val_loss: 0.2924 - val_accuracy: 0.9162 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2641 - accuracy: 0.9277 - val_loss: 0.2824 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2599 - accuracy: 0.9300 - val_loss: 0.2931 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2534 - accuracy: 0.9313 - val_loss: 0.2735 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2527 - accuracy: 0.9320 - val_loss: 0.2699 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2487 - accuracy: 0.9325 - val_loss: 0.2705 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2507 - accuracy: 0.9336 - val_loss: 0.2666 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2489 - accuracy: 0.9317 - val_loss: 0.2664 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2458 - accuracy: 0.9333 - val_loss: 0.2670 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2471 - accuracy: 0.9361 - val_loss: 0.2676 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2494 - accuracy: 0.9331 - val_loss: 0.2647 - val_accuracy: 0.9248 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2455 - accuracy: 0.9352 - val_loss: 0.2640 - val_accuracy: 0.9240 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2487 - accuracy: 0.9318 - val_loss: 0.2651 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2492 - accuracy: 0.9304 - val_loss: 0.2651 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2412 - accuracy: 0.9342 - val_loss: 0.2642 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2443 - accuracy: 0.9358 - val_loss: 0.2644 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2402 - accuracy: 0.9353 - val_loss: 0.2649 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2415 - accuracy: 0.9367 - val_loss: 0.2644 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2428 - accuracy: 0.9367 - val_loss: 0.2642 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2453 - accuracy: 0.9328 - val_loss: 0.2633 - val_accuracy: 0.9258 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2401 - accuracy: 0.9350 - val_loss: 0.2636 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2459 - accuracy: 0.9338 - val_loss: 0.2636 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2719 - accuracy: 0.9259\n",
      "17/17 [==============================] - 1s 19ms/step\n",
      "TP:5704, TN:9986, FP:469, FN:787, loss0.27193382382392883, acc0.925882214091821, sn0.8787551995070098, sp0.955141080822573, f10.9008212255211624, auc0.9737094167320369\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 48ms/step - loss: 4.3998 - accuracy: 0.6405 - val_loss: 4.5203 - val_accuracy: 0.4122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 3.3255 - accuracy: 0.7871 - val_loss: 3.0538 - val_accuracy: 0.6576 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 2.5398 - accuracy: 0.8324 - val_loss: 2.2136 - val_accuracy: 0.8408 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.9700 - accuracy: 0.8419 - val_loss: 1.7315 - val_accuracy: 0.8454 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.5254 - accuracy: 0.8670 - val_loss: 1.5190 - val_accuracy: 0.7996 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.2176 - accuracy: 0.8747 - val_loss: 1.0964 - val_accuracy: 0.8658 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.9956 - accuracy: 0.8802 - val_loss: 0.9306 - val_accuracy: 0.8664 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.8288 - accuracy: 0.8891 - val_loss: 0.8822 - val_accuracy: 0.8536 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.7239 - accuracy: 0.8896 - val_loss: 0.6549 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.6423 - accuracy: 0.8938 - val_loss: 0.6481 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5846 - accuracy: 0.8956 - val_loss: 0.6244 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5371 - accuracy: 0.8963 - val_loss: 0.5158 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5044 - accuracy: 0.9004 - val_loss: 0.4635 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4743 - accuracy: 0.9004 - val_loss: 0.5030 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4685 - accuracy: 0.8957 - val_loss: 0.4317 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4440 - accuracy: 0.9034 - val_loss: 0.4218 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4333 - accuracy: 0.9029 - val_loss: 0.4080 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4208 - accuracy: 0.9069 - val_loss: 0.4636 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4216 - accuracy: 0.9021 - val_loss: 0.4706 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4126 - accuracy: 0.9037 - val_loss: 0.4127 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4048 - accuracy: 0.9032 - val_loss: 0.4476 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4055 - accuracy: 0.9042 - val_loss: 0.3782 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3967 - accuracy: 0.9053 - val_loss: 0.5383 - val_accuracy: 0.8650 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3943 - accuracy: 0.9077 - val_loss: 0.4039 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3897 - accuracy: 0.9090 - val_loss: 0.3796 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3955 - accuracy: 0.9053 - val_loss: 0.4217 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3840 - accuracy: 0.9088 - val_loss: 0.4744 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3930 - accuracy: 0.9055 - val_loss: 0.3627 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3811 - accuracy: 0.9086 - val_loss: 0.3741 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3821 - accuracy: 0.9105 - val_loss: 0.4056 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3758 - accuracy: 0.9111 - val_loss: 0.4566 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3755 - accuracy: 0.9085 - val_loss: 0.4130 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3835 - accuracy: 0.9108 - val_loss: 0.4195 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3758 - accuracy: 0.9126 - val_loss: 0.5862 - val_accuracy: 0.8446 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3810 - accuracy: 0.9117 - val_loss: 0.5830 - val_accuracy: 0.8113 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3890 - accuracy: 0.9043 - val_loss: 0.3567 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3642 - accuracy: 0.9117 - val_loss: 0.6856 - val_accuracy: 0.7844 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3688 - accuracy: 0.9122 - val_loss: 0.3877 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3718 - accuracy: 0.9075 - val_loss: 0.4084 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3734 - accuracy: 0.9081 - val_loss: 0.3403 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3604 - accuracy: 0.9122 - val_loss: 0.4205 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3808 - accuracy: 0.9091 - val_loss: 0.3627 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3750 - accuracy: 0.9109 - val_loss: 0.3911 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3878 - accuracy: 0.9070 - val_loss: 0.4850 - val_accuracy: 0.8686 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3787 - accuracy: 0.9079 - val_loss: 0.4158 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3688 - accuracy: 0.9097 - val_loss: 0.3517 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3708 - accuracy: 0.9089 - val_loss: 0.5447 - val_accuracy: 0.8558 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3736 - accuracy: 0.9122 - val_loss: 0.3741 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3694 - accuracy: 0.9100 - val_loss: 0.4248 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3741 - accuracy: 0.9110 - val_loss: 0.3683 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3762 - accuracy: 0.9086 - val_loss: 0.3784 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3644 - accuracy: 0.9118 - val_loss: 0.3476 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3722 - accuracy: 0.9110 - val_loss: 0.3546 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3672 - accuracy: 0.9099 - val_loss: 0.3943 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3682 - accuracy: 0.9104 - val_loss: 0.4033 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3702 - accuracy: 0.9118 - val_loss: 0.3516 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3578 - accuracy: 0.9142 - val_loss: 0.3585 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3690 - accuracy: 0.9103 - val_loss: 0.3703 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3602 - accuracy: 0.9142 - val_loss: 0.4046 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3676 - accuracy: 0.9141 - val_loss: 0.3739 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3689 - accuracy: 0.9138 - val_loss: 0.4042 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3666 - accuracy: 0.9096 - val_loss: 0.4243 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3618 - accuracy: 0.9149 - val_loss: 0.4436 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3690 - accuracy: 0.9127 - val_loss: 0.3549 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3659 - accuracy: 0.9107 - val_loss: 0.3790 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3645 - accuracy: 0.9106 - val_loss: 0.4307 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3673 - accuracy: 0.9077 - val_loss: 0.3406 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3592 - accuracy: 0.9124 - val_loss: 0.4222 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3584 - accuracy: 0.9162 - val_loss: 0.4814 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3616 - accuracy: 0.9142 - val_loss: 0.3619 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3675 - accuracy: 0.9114 - val_loss: 0.4236 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3359 - accuracy: 0.9190 - val_loss: 0.3350 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3174 - accuracy: 0.9226 - val_loss: 0.3322 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2998 - accuracy: 0.9263 - val_loss: 0.3220 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3001 - accuracy: 0.9260 - val_loss: 0.3088 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2879 - accuracy: 0.9273 - val_loss: 0.3021 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2820 - accuracy: 0.9291 - val_loss: 0.2988 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2774 - accuracy: 0.9278 - val_loss: 0.3248 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2725 - accuracy: 0.9291 - val_loss: 0.2823 - val_accuracy: 0.9220 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2663 - accuracy: 0.9289 - val_loss: 0.2923 - val_accuracy: 0.9186 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2609 - accuracy: 0.9308 - val_loss: 0.2732 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2532 - accuracy: 0.9335 - val_loss: 0.2749 - val_accuracy: 0.9206 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2507 - accuracy: 0.9337 - val_loss: 0.2760 - val_accuracy: 0.9202 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2559 - accuracy: 0.9296 - val_loss: 0.2712 - val_accuracy: 0.9216 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2522 - accuracy: 0.9344 - val_loss: 0.2694 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2476 - accuracy: 0.9342 - val_loss: 0.2724 - val_accuracy: 0.9212 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2502 - accuracy: 0.9350 - val_loss: 0.2702 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2489 - accuracy: 0.9361 - val_loss: 0.2688 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2494 - accuracy: 0.9351 - val_loss: 0.2679 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2472 - accuracy: 0.9341 - val_loss: 0.2672 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2521 - accuracy: 0.9313 - val_loss: 0.2676 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2472 - accuracy: 0.9347 - val_loss: 0.2670 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2467 - accuracy: 0.9340 - val_loss: 0.2663 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2428 - accuracy: 0.9337 - val_loss: 0.2665 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2456 - accuracy: 0.9350 - val_loss: 0.2659 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2443 - accuracy: 0.9343 - val_loss: 0.2658 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2443 - accuracy: 0.9361 - val_loss: 0.2661 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2414 - accuracy: 0.9363 - val_loss: 0.2655 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2425 - accuracy: 0.9361 - val_loss: 0.2658 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2463 - accuracy: 0.9321 - val_loss: 0.2659 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2737 - accuracy: 0.9265\n",
      "17/17 [==============================] - 1s 17ms/step\n",
      "TP:5741, TN:9959, FP:496, FN:750, loss0.2737240195274353, acc0.9264723238522365, sn0.8844553997843168, sp0.9525585844093735, f10.9021055939660592, auc0.9735614724902176\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 52ms/step - loss: 4.4541 - accuracy: 0.6243 - val_loss: 4.1873 - val_accuracy: 0.4192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 3.3705 - accuracy: 0.7810 - val_loss: 3.1611 - val_accuracy: 0.6031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 2.6102 - accuracy: 0.8164 - val_loss: 2.2841 - val_accuracy: 0.8229 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 2.0159 - accuracy: 0.8414 - val_loss: 1.8175 - val_accuracy: 0.8165 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.5669 - accuracy: 0.8612 - val_loss: 1.4299 - val_accuracy: 0.8387 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.2505 - accuracy: 0.8702 - val_loss: 1.1546 - val_accuracy: 0.8548 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.0138 - accuracy: 0.8801 - val_loss: 0.9464 - val_accuracy: 0.8686 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.8350 - accuracy: 0.8900 - val_loss: 0.8808 - val_accuracy: 0.8496 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7313 - accuracy: 0.8909 - val_loss: 0.9356 - val_accuracy: 0.8057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.6441 - accuracy: 0.8971 - val_loss: 0.7916 - val_accuracy: 0.8355 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5877 - accuracy: 0.8980 - val_loss: 0.5919 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5433 - accuracy: 0.8975 - val_loss: 0.5241 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5015 - accuracy: 0.9002 - val_loss: 0.4746 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4731 - accuracy: 0.9031 - val_loss: 0.5954 - val_accuracy: 0.8554 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.4650 - accuracy: 0.9040 - val_loss: 0.4729 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4481 - accuracy: 0.9050 - val_loss: 0.4284 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4258 - accuracy: 0.9056 - val_loss: 0.4213 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4228 - accuracy: 0.9035 - val_loss: 0.4256 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4115 - accuracy: 0.9079 - val_loss: 0.3979 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4037 - accuracy: 0.9075 - val_loss: 0.4990 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4039 - accuracy: 0.9054 - val_loss: 0.4582 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4017 - accuracy: 0.9054 - val_loss: 0.3956 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3913 - accuracy: 0.9078 - val_loss: 0.4223 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3950 - accuracy: 0.9074 - val_loss: 0.3744 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3861 - accuracy: 0.9078 - val_loss: 0.3983 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3822 - accuracy: 0.9091 - val_loss: 0.4000 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3772 - accuracy: 0.9071 - val_loss: 0.3832 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3858 - accuracy: 0.9099 - val_loss: 0.4688 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3795 - accuracy: 0.9117 - val_loss: 0.3730 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3805 - accuracy: 0.9087 - val_loss: 0.3912 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3790 - accuracy: 0.9088 - val_loss: 0.4624 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3741 - accuracy: 0.9125 - val_loss: 0.3929 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3770 - accuracy: 0.9070 - val_loss: 0.3935 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3827 - accuracy: 0.9024 - val_loss: 0.3725 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3709 - accuracy: 0.9079 - val_loss: 0.3720 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3663 - accuracy: 0.9119 - val_loss: 0.3798 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3741 - accuracy: 0.9079 - val_loss: 0.3762 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3697 - accuracy: 0.9090 - val_loss: 0.3947 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3676 - accuracy: 0.9102 - val_loss: 0.3956 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3710 - accuracy: 0.9109 - val_loss: 0.3663 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3742 - accuracy: 0.9091 - val_loss: 0.3918 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3645 - accuracy: 0.9120 - val_loss: 0.3595 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3701 - accuracy: 0.9091 - val_loss: 0.3502 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3684 - accuracy: 0.9110 - val_loss: 0.3847 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3751 - accuracy: 0.9102 - val_loss: 0.5365 - val_accuracy: 0.8618 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3744 - accuracy: 0.9069 - val_loss: 0.3517 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3715 - accuracy: 0.9100 - val_loss: 0.3660 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3729 - accuracy: 0.9100 - val_loss: 0.3563 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3817 - accuracy: 0.9069 - val_loss: 0.3810 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3633 - accuracy: 0.9101 - val_loss: 0.4810 - val_accuracy: 0.8626 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3611 - accuracy: 0.9128 - val_loss: 0.3809 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3718 - accuracy: 0.9088 - val_loss: 0.3821 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3628 - accuracy: 0.9132 - val_loss: 0.4281 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3614 - accuracy: 0.9115 - val_loss: 0.3981 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3603 - accuracy: 0.9122 - val_loss: 0.3690 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3588 - accuracy: 0.9119 - val_loss: 0.3616 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3539 - accuracy: 0.9108 - val_loss: 0.3700 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3618 - accuracy: 0.9126 - val_loss: 0.3506 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3540 - accuracy: 0.9146 - val_loss: 0.3385 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3644 - accuracy: 0.9100 - val_loss: 0.3819 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3585 - accuracy: 0.9145 - val_loss: 0.3451 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3633 - accuracy: 0.9141 - val_loss: 0.3755 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3566 - accuracy: 0.9138 - val_loss: 0.3482 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3614 - accuracy: 0.9115 - val_loss: 0.3567 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3608 - accuracy: 0.9129 - val_loss: 0.3574 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3578 - accuracy: 0.9132 - val_loss: 0.4054 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3612 - accuracy: 0.9084 - val_loss: 0.3586 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3520 - accuracy: 0.9144 - val_loss: 0.3678 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3556 - accuracy: 0.9125 - val_loss: 0.3728 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3504 - accuracy: 0.9132 - val_loss: 0.3533 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3554 - accuracy: 0.9146 - val_loss: 0.3517 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3285 - accuracy: 0.9235 - val_loss: 0.3418 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3095 - accuracy: 0.9279 - val_loss: 0.3459 - val_accuracy: 0.9093 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3008 - accuracy: 0.9230 - val_loss: 0.3296 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2929 - accuracy: 0.9279 - val_loss: 0.3107 - val_accuracy: 0.9160 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2828 - accuracy: 0.9285 - val_loss: 0.2954 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2827 - accuracy: 0.9260 - val_loss: 0.2919 - val_accuracy: 0.9196 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2702 - accuracy: 0.9296 - val_loss: 0.2852 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2671 - accuracy: 0.9302 - val_loss: 0.2936 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2600 - accuracy: 0.9318 - val_loss: 0.2818 - val_accuracy: 0.9188 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2609 - accuracy: 0.9297 - val_loss: 0.2852 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2517 - accuracy: 0.9327 - val_loss: 0.2748 - val_accuracy: 0.9216 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2546 - accuracy: 0.9314 - val_loss: 0.2726 - val_accuracy: 0.9216 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2502 - accuracy: 0.9337 - val_loss: 0.2702 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2445 - accuracy: 0.9341 - val_loss: 0.2721 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2474 - accuracy: 0.9310 - val_loss: 0.2664 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2491 - accuracy: 0.9331 - val_loss: 0.2683 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2443 - accuracy: 0.9345 - val_loss: 0.2686 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2420 - accuracy: 0.9356 - val_loss: 0.2693 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2454 - accuracy: 0.9337 - val_loss: 0.2689 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2428 - accuracy: 0.9355 - val_loss: 0.2678 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2419 - accuracy: 0.9343 - val_loss: 0.2668 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2468 - accuracy: 0.9332 - val_loss: 0.2664 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2472 - accuracy: 0.9320 - val_loss: 0.2661 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2396 - accuracy: 0.9357 - val_loss: 0.2662 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2389 - accuracy: 0.9364 - val_loss: 0.2657 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2423 - accuracy: 0.9333 - val_loss: 0.2655 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2464 - accuracy: 0.9332 - val_loss: 0.2658 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2433 - accuracy: 0.9341 - val_loss: 0.2662 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2418 - accuracy: 0.9349 - val_loss: 0.2660 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2744 - accuracy: 0.9256\n",
      "17/17 [==============================] - 1s 18ms/step\n",
      "TP:5710, TN:9975, FP:480, FN:781, loss0.27439382672309875, acc0.9255871592116134, sn0.8796795563087352, sp0.9540889526542324, f10.9005598927529375, auc0.9729503699379657\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 50ms/step - loss: 4.4414 - accuracy: 0.6306 - val_loss: 4.5998 - val_accuracy: 0.4320 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 3.3938 - accuracy: 0.7786 - val_loss: 3.4209 - val_accuracy: 0.4507 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 2.6303 - accuracy: 0.8294 - val_loss: 2.3929 - val_accuracy: 0.7914 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 2.0644 - accuracy: 0.8449 - val_loss: 1.8435 - val_accuracy: 0.8398 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.6002 - accuracy: 0.8685 - val_loss: 1.4580 - val_accuracy: 0.8528 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 1.2759 - accuracy: 0.8732 - val_loss: 1.2169 - val_accuracy: 0.8446 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 1.0377 - accuracy: 0.8795 - val_loss: 0.9969 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.8668 - accuracy: 0.8883 - val_loss: 0.8012 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.7480 - accuracy: 0.8931 - val_loss: 0.8004 - val_accuracy: 0.8580 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.6448 - accuracy: 0.8992 - val_loss: 0.7919 - val_accuracy: 0.8293 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.5816 - accuracy: 0.8995 - val_loss: 0.5505 - val_accuracy: 0.8983 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5371 - accuracy: 0.9017 - val_loss: 0.4944 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5015 - accuracy: 0.9005 - val_loss: 0.5561 - val_accuracy: 0.8759 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4814 - accuracy: 0.9022 - val_loss: 0.5421 - val_accuracy: 0.8746 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4606 - accuracy: 0.9042 - val_loss: 0.5500 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4390 - accuracy: 0.9059 - val_loss: 0.4540 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4245 - accuracy: 0.9048 - val_loss: 0.3918 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4107 - accuracy: 0.9061 - val_loss: 0.3759 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4124 - accuracy: 0.9058 - val_loss: 0.4297 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4026 - accuracy: 0.9050 - val_loss: 0.3711 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3997 - accuracy: 0.9040 - val_loss: 0.4534 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3897 - accuracy: 0.9097 - val_loss: 0.3720 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3889 - accuracy: 0.9084 - val_loss: 0.3737 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3980 - accuracy: 0.9038 - val_loss: 0.4254 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3922 - accuracy: 0.9078 - val_loss: 0.3639 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3884 - accuracy: 0.9073 - val_loss: 0.3977 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3866 - accuracy: 0.9075 - val_loss: 0.4351 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3745 - accuracy: 0.9091 - val_loss: 0.3760 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3766 - accuracy: 0.9077 - val_loss: 0.4304 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3870 - accuracy: 0.9068 - val_loss: 0.3642 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3795 - accuracy: 0.9112 - val_loss: 0.3922 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3749 - accuracy: 0.9096 - val_loss: 0.3936 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3713 - accuracy: 0.9059 - val_loss: 0.4162 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3744 - accuracy: 0.9086 - val_loss: 0.3650 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3744 - accuracy: 0.9103 - val_loss: 0.3758 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3647 - accuracy: 0.9108 - val_loss: 0.3823 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3715 - accuracy: 0.9082 - val_loss: 0.4090 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3684 - accuracy: 0.9138 - val_loss: 0.3914 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3824 - accuracy: 0.9060 - val_loss: 0.3627 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3749 - accuracy: 0.9082 - val_loss: 0.4341 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3663 - accuracy: 0.9132 - val_loss: 0.3909 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3657 - accuracy: 0.9124 - val_loss: 0.3748 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3677 - accuracy: 0.9125 - val_loss: 0.3963 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3713 - accuracy: 0.9097 - val_loss: 0.3871 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3656 - accuracy: 0.9116 - val_loss: 0.3592 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3667 - accuracy: 0.9113 - val_loss: 0.3784 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3656 - accuracy: 0.9099 - val_loss: 0.3765 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3623 - accuracy: 0.9126 - val_loss: 0.3840 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3732 - accuracy: 0.9073 - val_loss: 0.3445 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3657 - accuracy: 0.9107 - val_loss: 0.3705 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3687 - accuracy: 0.9085 - val_loss: 0.3816 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3691 - accuracy: 0.9102 - val_loss: 0.3717 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3576 - accuracy: 0.9140 - val_loss: 0.4136 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3609 - accuracy: 0.9126 - val_loss: 0.3843 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3651 - accuracy: 0.9132 - val_loss: 0.4818 - val_accuracy: 0.8596 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3602 - accuracy: 0.9104 - val_loss: 0.3826 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3653 - accuracy: 0.9101 - val_loss: 0.3676 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3560 - accuracy: 0.9110 - val_loss: 0.3768 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3700 - accuracy: 0.9081 - val_loss: 0.4209 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3623 - accuracy: 0.9091 - val_loss: 0.3648 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3618 - accuracy: 0.9094 - val_loss: 0.3686 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3569 - accuracy: 0.9136 - val_loss: 0.4259 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3591 - accuracy: 0.9117 - val_loss: 0.3564 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3619 - accuracy: 0.9114 - val_loss: 0.3713 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3628 - accuracy: 0.9144 - val_loss: 0.3697 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3629 - accuracy: 0.9114 - val_loss: 0.3794 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3664 - accuracy: 0.9089 - val_loss: 0.3749 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3544 - accuracy: 0.9130 - val_loss: 0.3602 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3631 - accuracy: 0.9100 - val_loss: 0.3728 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3559 - accuracy: 0.9132 - val_loss: 0.3704 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3551 - accuracy: 0.9154 - val_loss: 0.3389 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3268 - accuracy: 0.9238 - val_loss: 0.3455 - val_accuracy: 0.9083 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3165 - accuracy: 0.9229 - val_loss: 0.3338 - val_accuracy: 0.9105 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2995 - accuracy: 0.9256 - val_loss: 0.3508 - val_accuracy: 0.9059 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2877 - accuracy: 0.9287 - val_loss: 0.3212 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2807 - accuracy: 0.9270 - val_loss: 0.3042 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2800 - accuracy: 0.9257 - val_loss: 0.2847 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2665 - accuracy: 0.9333 - val_loss: 0.2955 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2649 - accuracy: 0.9287 - val_loss: 0.3082 - val_accuracy: 0.9122 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2607 - accuracy: 0.9285 - val_loss: 0.2899 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2587 - accuracy: 0.9267 - val_loss: 0.2689 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2527 - accuracy: 0.9302 - val_loss: 0.2680 - val_accuracy: 0.9208 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2515 - accuracy: 0.9332 - val_loss: 0.2704 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2457 - accuracy: 0.9334 - val_loss: 0.2689 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2463 - accuracy: 0.9323 - val_loss: 0.2673 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2492 - accuracy: 0.9285 - val_loss: 0.2671 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2472 - accuracy: 0.9328 - val_loss: 0.2635 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2477 - accuracy: 0.9338 - val_loss: 0.2660 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2462 - accuracy: 0.9342 - val_loss: 0.2645 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2413 - accuracy: 0.9357 - val_loss: 0.2664 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2414 - accuracy: 0.9364 - val_loss: 0.2639 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2407 - accuracy: 0.9349 - val_loss: 0.2641 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2412 - accuracy: 0.9322 - val_loss: 0.2640 - val_accuracy: 0.9232 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2391 - accuracy: 0.9357 - val_loss: 0.2637 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2487 - accuracy: 0.9338 - val_loss: 0.2637 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2398 - accuracy: 0.9359 - val_loss: 0.2640 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2372 - accuracy: 0.9361 - val_loss: 0.2643 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2408 - accuracy: 0.9351 - val_loss: 0.2641 - val_accuracy: 0.9232 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2433 - accuracy: 0.9353 - val_loss: 0.2636 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2383 - accuracy: 0.9367 - val_loss: 0.2633 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2722 - accuracy: 0.9256\n",
      "17/17 [==============================] - 1s 17ms/step\n",
      "TP:5735, TN:9951, FP:504, FN:756, loss0.27218523621559143, acc0.9256461701876549, sn0.8835310429825913, sp0.951793400286944, f10.9010212097407698, auc0.9731793888031407\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 47ms/step - loss: 4.4511 - accuracy: 0.6369 - val_loss: 4.3057 - val_accuracy: 0.4093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 3.3947 - accuracy: 0.7719 - val_loss: 3.0436 - val_accuracy: 0.7499 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 2.6248 - accuracy: 0.8201 - val_loss: 2.2805 - val_accuracy: 0.8353 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 2.0246 - accuracy: 0.8414 - val_loss: 1.8352 - val_accuracy: 0.8213 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 1.5773 - accuracy: 0.8603 - val_loss: 1.5880 - val_accuracy: 0.7942 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.2588 - accuracy: 0.8726 - val_loss: 1.1805 - val_accuracy: 0.8488 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 1.0209 - accuracy: 0.8816 - val_loss: 1.0419 - val_accuracy: 0.8440 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.8584 - accuracy: 0.8882 - val_loss: 0.7993 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.7366 - accuracy: 0.8912 - val_loss: 0.7124 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.6502 - accuracy: 0.8964 - val_loss: 0.7643 - val_accuracy: 0.8373 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.5812 - accuracy: 0.9020 - val_loss: 0.5498 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.5403 - accuracy: 0.9002 - val_loss: 0.7163 - val_accuracy: 0.8273 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5069 - accuracy: 0.8997 - val_loss: 0.5183 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4797 - accuracy: 0.9030 - val_loss: 0.5581 - val_accuracy: 0.8724 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4596 - accuracy: 0.9026 - val_loss: 0.4413 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4399 - accuracy: 0.9058 - val_loss: 0.4341 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4341 - accuracy: 0.9048 - val_loss: 0.4005 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4229 - accuracy: 0.9056 - val_loss: 0.5159 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4204 - accuracy: 0.9043 - val_loss: 0.3994 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4036 - accuracy: 0.9095 - val_loss: 0.3853 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4084 - accuracy: 0.9086 - val_loss: 0.4839 - val_accuracy: 0.8771 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3909 - accuracy: 0.9089 - val_loss: 0.3943 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3946 - accuracy: 0.9088 - val_loss: 0.3925 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3968 - accuracy: 0.9068 - val_loss: 0.4802 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3952 - accuracy: 0.9073 - val_loss: 0.3740 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3963 - accuracy: 0.9090 - val_loss: 0.4730 - val_accuracy: 0.8720 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3849 - accuracy: 0.9078 - val_loss: 0.4225 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3846 - accuracy: 0.9090 - val_loss: 0.4570 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3761 - accuracy: 0.9111 - val_loss: 0.4376 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3877 - accuracy: 0.9089 - val_loss: 0.4340 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3841 - accuracy: 0.9070 - val_loss: 0.3899 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3780 - accuracy: 0.9098 - val_loss: 0.3959 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3846 - accuracy: 0.9076 - val_loss: 0.5396 - val_accuracy: 0.8570 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3746 - accuracy: 0.9126 - val_loss: 0.4127 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3691 - accuracy: 0.9121 - val_loss: 0.4482 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3774 - accuracy: 0.9084 - val_loss: 0.4350 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3736 - accuracy: 0.9092 - val_loss: 0.4612 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3746 - accuracy: 0.9072 - val_loss: 0.3820 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3701 - accuracy: 0.9101 - val_loss: 0.3675 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3745 - accuracy: 0.9094 - val_loss: 0.3628 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3676 - accuracy: 0.9097 - val_loss: 0.3600 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3724 - accuracy: 0.9091 - val_loss: 0.4313 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3698 - accuracy: 0.9108 - val_loss: 0.3676 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3758 - accuracy: 0.9092 - val_loss: 0.4039 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3692 - accuracy: 0.9106 - val_loss: 0.4390 - val_accuracy: 0.8859 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3711 - accuracy: 0.9119 - val_loss: 0.4124 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3792 - accuracy: 0.9085 - val_loss: 0.3638 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3667 - accuracy: 0.9108 - val_loss: 0.3681 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3742 - accuracy: 0.9111 - val_loss: 0.3920 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3693 - accuracy: 0.9119 - val_loss: 0.4312 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3768 - accuracy: 0.9116 - val_loss: 0.4691 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3686 - accuracy: 0.9116 - val_loss: 0.3584 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3656 - accuracy: 0.9143 - val_loss: 0.3903 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3800 - accuracy: 0.9082 - val_loss: 0.4602 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3648 - accuracy: 0.9152 - val_loss: 0.3864 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3752 - accuracy: 0.9096 - val_loss: 0.4171 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3678 - accuracy: 0.9127 - val_loss: 0.3563 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3667 - accuracy: 0.9132 - val_loss: 0.3712 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3690 - accuracy: 0.9120 - val_loss: 0.4649 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3619 - accuracy: 0.9144 - val_loss: 0.4464 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3705 - accuracy: 0.9135 - val_loss: 0.3667 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3632 - accuracy: 0.9128 - val_loss: 0.3447 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3499 - accuracy: 0.9170 - val_loss: 0.3604 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3726 - accuracy: 0.9110 - val_loss: 0.4578 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3730 - accuracy: 0.9136 - val_loss: 0.3616 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3639 - accuracy: 0.9137 - val_loss: 0.3520 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3676 - accuracy: 0.9120 - val_loss: 0.3386 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3578 - accuracy: 0.9149 - val_loss: 0.3833 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3593 - accuracy: 0.9138 - val_loss: 0.4405 - val_accuracy: 0.8841 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3575 - accuracy: 0.9148 - val_loss: 0.3613 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3678 - accuracy: 0.9120 - val_loss: 0.3611 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3248 - accuracy: 0.9268 - val_loss: 0.3468 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3112 - accuracy: 0.9284 - val_loss: 0.3448 - val_accuracy: 0.9116 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3025 - accuracy: 0.9281 - val_loss: 0.3599 - val_accuracy: 0.9087 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2959 - accuracy: 0.9301 - val_loss: 0.3277 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2917 - accuracy: 0.9283 - val_loss: 0.3077 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2783 - accuracy: 0.9334 - val_loss: 0.3035 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2764 - accuracy: 0.9303 - val_loss: 0.3007 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2641 - accuracy: 0.9324 - val_loss: 0.2844 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2650 - accuracy: 0.9292 - val_loss: 0.2923 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2614 - accuracy: 0.9322 - val_loss: 0.2828 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2520 - accuracy: 0.9333 - val_loss: 0.2761 - val_accuracy: 0.9214 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2513 - accuracy: 0.9325 - val_loss: 0.2750 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2505 - accuracy: 0.9373 - val_loss: 0.2733 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2510 - accuracy: 0.9332 - val_loss: 0.2706 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2515 - accuracy: 0.9330 - val_loss: 0.2686 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2481 - accuracy: 0.9351 - val_loss: 0.2697 - val_accuracy: 0.9256 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2501 - accuracy: 0.9344 - val_loss: 0.2731 - val_accuracy: 0.9240 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2444 - accuracy: 0.9372 - val_loss: 0.2691 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2470 - accuracy: 0.9356 - val_loss: 0.2676 - val_accuracy: 0.9250 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2459 - accuracy: 0.9366 - val_loss: 0.2683 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2407 - accuracy: 0.9367 - val_loss: 0.2672 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2453 - accuracy: 0.9367 - val_loss: 0.2669 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2408 - accuracy: 0.9386 - val_loss: 0.2672 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2414 - accuracy: 0.9379 - val_loss: 0.2669 - val_accuracy: 0.9258 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2443 - accuracy: 0.9373 - val_loss: 0.2668 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2485 - accuracy: 0.9366 - val_loss: 0.2668 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2431 - accuracy: 0.9373 - val_loss: 0.2669 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2442 - accuracy: 0.9349 - val_loss: 0.2670 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2446 - accuracy: 0.9362 - val_loss: 0.2671 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2781 - accuracy: 0.9246\n",
      "17/17 [==============================] - 1s 18ms/step\n",
      "TP:5706, TN:9964, FP:491, FN:785, loss0.2781435251235962, acc0.9247019945709902, sn0.8790633184409182, sp0.9530368244858919, f10.8994325346784363, auc0.9736025402792565\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 47ms/step - loss: 4.4243 - accuracy: 0.6385 - val_loss: 4.2625 - val_accuracy: 0.4324 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 3.3433 - accuracy: 0.7866 - val_loss: 2.9696 - val_accuracy: 0.7603 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 2.5755 - accuracy: 0.8168 - val_loss: 2.2323 - val_accuracy: 0.8299 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 1.9732 - accuracy: 0.8438 - val_loss: 1.7734 - val_accuracy: 0.8277 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 1.5262 - accuracy: 0.8660 - val_loss: 1.4336 - val_accuracy: 0.8293 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.2235 - accuracy: 0.8707 - val_loss: 1.2676 - val_accuracy: 0.8099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.0022 - accuracy: 0.8812 - val_loss: 0.9587 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.8353 - accuracy: 0.8897 - val_loss: 0.7895 - val_accuracy: 0.8765 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.7172 - accuracy: 0.8961 - val_loss: 0.7469 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.6491 - accuracy: 0.8937 - val_loss: 0.6294 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5851 - accuracy: 0.8959 - val_loss: 0.5775 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5366 - accuracy: 0.8999 - val_loss: 0.4951 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5024 - accuracy: 0.9022 - val_loss: 0.5259 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4767 - accuracy: 0.9040 - val_loss: 0.5034 - val_accuracy: 0.8903 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4631 - accuracy: 0.9020 - val_loss: 0.5049 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4483 - accuracy: 0.9011 - val_loss: 0.4082 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4285 - accuracy: 0.9074 - val_loss: 0.4233 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4235 - accuracy: 0.9044 - val_loss: 0.4430 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4100 - accuracy: 0.9067 - val_loss: 0.4248 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4099 - accuracy: 0.9059 - val_loss: 0.4055 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4125 - accuracy: 0.9015 - val_loss: 0.4983 - val_accuracy: 0.8616 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3979 - accuracy: 0.9072 - val_loss: 0.3772 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3945 - accuracy: 0.9041 - val_loss: 0.3821 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3867 - accuracy: 0.9076 - val_loss: 0.4133 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3821 - accuracy: 0.9073 - val_loss: 0.3980 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3894 - accuracy: 0.9075 - val_loss: 0.3764 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3808 - accuracy: 0.9084 - val_loss: 0.3898 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3800 - accuracy: 0.9088 - val_loss: 0.3755 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3774 - accuracy: 0.9090 - val_loss: 0.4123 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3786 - accuracy: 0.9094 - val_loss: 0.3844 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3759 - accuracy: 0.9091 - val_loss: 0.3694 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3717 - accuracy: 0.9090 - val_loss: 0.5471 - val_accuracy: 0.8562 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3749 - accuracy: 0.9109 - val_loss: 0.3876 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3655 - accuracy: 0.9112 - val_loss: 0.3814 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3700 - accuracy: 0.9125 - val_loss: 0.4012 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3722 - accuracy: 0.9084 - val_loss: 0.3788 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3676 - accuracy: 0.9092 - val_loss: 0.3754 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3706 - accuracy: 0.9110 - val_loss: 0.4436 - val_accuracy: 0.8694 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3687 - accuracy: 0.9088 - val_loss: 0.3993 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3725 - accuracy: 0.9097 - val_loss: 0.4521 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3741 - accuracy: 0.9126 - val_loss: 0.4495 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3742 - accuracy: 0.9083 - val_loss: 0.3853 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3688 - accuracy: 0.9083 - val_loss: 0.4137 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3541 - accuracy: 0.9120 - val_loss: 0.3624 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3677 - accuracy: 0.9094 - val_loss: 0.4099 - val_accuracy: 0.8847 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3624 - accuracy: 0.9120 - val_loss: 0.3491 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3662 - accuracy: 0.9104 - val_loss: 0.3849 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3663 - accuracy: 0.9122 - val_loss: 0.3579 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3621 - accuracy: 0.9103 - val_loss: 0.4275 - val_accuracy: 0.8817 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3648 - accuracy: 0.9114 - val_loss: 0.4097 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3678 - accuracy: 0.9082 - val_loss: 0.3426 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3642 - accuracy: 0.9109 - val_loss: 0.3584 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3537 - accuracy: 0.9126 - val_loss: 0.4093 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3628 - accuracy: 0.9087 - val_loss: 0.3779 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3611 - accuracy: 0.9125 - val_loss: 0.3721 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3663 - accuracy: 0.9101 - val_loss: 0.3770 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3586 - accuracy: 0.9117 - val_loss: 0.4460 - val_accuracy: 0.8829 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3574 - accuracy: 0.9127 - val_loss: 0.3778 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3597 - accuracy: 0.9140 - val_loss: 0.3614 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3646 - accuracy: 0.9097 - val_loss: 0.3453 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3595 - accuracy: 0.9121 - val_loss: 0.3522 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3598 - accuracy: 0.9125 - val_loss: 0.3565 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3564 - accuracy: 0.9147 - val_loss: 0.4553 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3657 - accuracy: 0.9103 - val_loss: 0.3697 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3593 - accuracy: 0.9126 - val_loss: 0.3545 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3579 - accuracy: 0.9136 - val_loss: 0.3927 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3644 - accuracy: 0.9127 - val_loss: 0.4140 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3574 - accuracy: 0.9132 - val_loss: 0.3574 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3557 - accuracy: 0.9126 - val_loss: 0.3822 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3492 - accuracy: 0.9150 - val_loss: 0.3407 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3575 - accuracy: 0.9138 - val_loss: 0.3645 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3380 - accuracy: 0.9192 - val_loss: 0.3446 - val_accuracy: 0.9126 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3173 - accuracy: 0.9253 - val_loss: 0.3478 - val_accuracy: 0.9081 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3091 - accuracy: 0.9248 - val_loss: 0.3524 - val_accuracy: 0.9077 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2981 - accuracy: 0.9266 - val_loss: 0.3421 - val_accuracy: 0.9081 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2888 - accuracy: 0.9261 - val_loss: 0.3106 - val_accuracy: 0.9136 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2754 - accuracy: 0.9297 - val_loss: 0.3290 - val_accuracy: 0.9097 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2799 - accuracy: 0.9269 - val_loss: 0.2806 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2697 - accuracy: 0.9294 - val_loss: 0.2858 - val_accuracy: 0.9182 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2696 - accuracy: 0.9262 - val_loss: 0.2780 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2646 - accuracy: 0.9276 - val_loss: 0.2778 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2562 - accuracy: 0.9317 - val_loss: 0.2787 - val_accuracy: 0.9192 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2505 - accuracy: 0.9332 - val_loss: 0.2769 - val_accuracy: 0.9214 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2548 - accuracy: 0.9295 - val_loss: 0.2723 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2522 - accuracy: 0.9308 - val_loss: 0.2702 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2503 - accuracy: 0.9335 - val_loss: 0.2702 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2488 - accuracy: 0.9352 - val_loss: 0.2717 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2483 - accuracy: 0.9333 - val_loss: 0.2704 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2537 - accuracy: 0.9290 - val_loss: 0.2663 - val_accuracy: 0.9250 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2521 - accuracy: 0.9302 - val_loss: 0.2687 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2482 - accuracy: 0.9334 - val_loss: 0.2672 - val_accuracy: 0.9246 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2457 - accuracy: 0.9338 - val_loss: 0.2664 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2490 - accuracy: 0.9320 - val_loss: 0.2668 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2458 - accuracy: 0.9331 - val_loss: 0.2670 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2458 - accuracy: 0.9349 - val_loss: 0.2673 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2440 - accuracy: 0.9321 - val_loss: 0.2671 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2434 - accuracy: 0.9356 - val_loss: 0.2669 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2428 - accuracy: 0.9344 - val_loss: 0.2670 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2438 - accuracy: 0.9351 - val_loss: 0.2665 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2460 - accuracy: 0.9350 - val_loss: 0.2671 - val_accuracy: 0.9246 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2745 - accuracy: 0.9261\n",
      "17/17 [==============================] - 1s 18ms/step\n",
      "TP:5700, TN:9993, FP:462, FN:791, loss0.2744547426700592, acc0.9260592470199457, sn0.8781389616391927, sp0.9558106169296987, f10.9009721014779103, auc0.9734565484888358\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 54ms/step - loss: 4.3508 - accuracy: 0.6499 - val_loss: 5.0337 - val_accuracy: 0.3943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 3.3078 - accuracy: 0.7894 - val_loss: 2.9253 - val_accuracy: 0.7655 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 2.5432 - accuracy: 0.8225 - val_loss: 2.2268 - val_accuracy: 0.8408 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.9466 - accuracy: 0.8480 - val_loss: 1.6948 - val_accuracy: 0.8550 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.5031 - accuracy: 0.8590 - val_loss: 1.3199 - val_accuracy: 0.8688 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 1.1887 - accuracy: 0.8723 - val_loss: 1.0945 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.9625 - accuracy: 0.8783 - val_loss: 1.0584 - val_accuracy: 0.8151 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.8012 - accuracy: 0.8849 - val_loss: 0.8469 - val_accuracy: 0.8470 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.6823 - accuracy: 0.8953 - val_loss: 0.7549 - val_accuracy: 0.8480 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.6149 - accuracy: 0.8956 - val_loss: 0.5956 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5510 - accuracy: 0.9021 - val_loss: 0.5952 - val_accuracy: 0.8736 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5134 - accuracy: 0.9002 - val_loss: 0.4831 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4835 - accuracy: 0.9034 - val_loss: 0.4802 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4702 - accuracy: 0.9006 - val_loss: 0.4611 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4526 - accuracy: 0.9055 - val_loss: 0.4567 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4401 - accuracy: 0.9022 - val_loss: 0.4473 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.4213 - accuracy: 0.9056 - val_loss: 0.3908 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4129 - accuracy: 0.9052 - val_loss: 0.3946 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4029 - accuracy: 0.9073 - val_loss: 0.3981 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4080 - accuracy: 0.9054 - val_loss: 0.4032 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3979 - accuracy: 0.9067 - val_loss: 0.4098 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3947 - accuracy: 0.9080 - val_loss: 0.4112 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3899 - accuracy: 0.9100 - val_loss: 0.3774 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3971 - accuracy: 0.9017 - val_loss: 0.3903 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3876 - accuracy: 0.9077 - val_loss: 0.3878 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3882 - accuracy: 0.9070 - val_loss: 0.4161 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3862 - accuracy: 0.9080 - val_loss: 0.3819 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3861 - accuracy: 0.9065 - val_loss: 0.3711 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3842 - accuracy: 0.9077 - val_loss: 0.6748 - val_accuracy: 0.8173 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3822 - accuracy: 0.9107 - val_loss: 0.3728 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3783 - accuracy: 0.9108 - val_loss: 0.3956 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3808 - accuracy: 0.9065 - val_loss: 0.3825 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3731 - accuracy: 0.9089 - val_loss: 0.4138 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3733 - accuracy: 0.9091 - val_loss: 0.4111 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3806 - accuracy: 0.9077 - val_loss: 0.3743 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3748 - accuracy: 0.9080 - val_loss: 0.4281 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3739 - accuracy: 0.9102 - val_loss: 0.3673 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3730 - accuracy: 0.9089 - val_loss: 0.3599 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3773 - accuracy: 0.9080 - val_loss: 0.3640 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3773 - accuracy: 0.9085 - val_loss: 0.3516 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3695 - accuracy: 0.9096 - val_loss: 0.3689 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3631 - accuracy: 0.9122 - val_loss: 0.4360 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3679 - accuracy: 0.9127 - val_loss: 0.3760 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3722 - accuracy: 0.9108 - val_loss: 0.3799 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3707 - accuracy: 0.9128 - val_loss: 0.3545 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3609 - accuracy: 0.9133 - val_loss: 0.3527 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3603 - accuracy: 0.9165 - val_loss: 0.3570 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3729 - accuracy: 0.9105 - val_loss: 0.3993 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3731 - accuracy: 0.9092 - val_loss: 0.4575 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3642 - accuracy: 0.9139 - val_loss: 0.3497 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3739 - accuracy: 0.9091 - val_loss: 0.3596 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3641 - accuracy: 0.9125 - val_loss: 0.3550 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3685 - accuracy: 0.9117 - val_loss: 0.3537 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3685 - accuracy: 0.9119 - val_loss: 0.3906 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3597 - accuracy: 0.9131 - val_loss: 0.3701 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3636 - accuracy: 0.9106 - val_loss: 0.4590 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3598 - accuracy: 0.9124 - val_loss: 0.3574 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3588 - accuracy: 0.9121 - val_loss: 0.3701 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3637 - accuracy: 0.9127 - val_loss: 0.4638 - val_accuracy: 0.8907 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3588 - accuracy: 0.9161 - val_loss: 0.4821 - val_accuracy: 0.8640 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3526 - accuracy: 0.9164 - val_loss: 0.3763 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3653 - accuracy: 0.9117 - val_loss: 0.3510 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3583 - accuracy: 0.9129 - val_loss: 0.3986 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3581 - accuracy: 0.9117 - val_loss: 0.4168 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3565 - accuracy: 0.9138 - val_loss: 0.3830 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3603 - accuracy: 0.9105 - val_loss: 0.3570 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3568 - accuracy: 0.9149 - val_loss: 0.3901 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3608 - accuracy: 0.9125 - val_loss: 0.3397 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3666 - accuracy: 0.9119 - val_loss: 0.3423 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3569 - accuracy: 0.9151 - val_loss: 0.3739 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3554 - accuracy: 0.9154 - val_loss: 0.3742 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3397 - accuracy: 0.9234 - val_loss: 0.3446 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3214 - accuracy: 0.9245 - val_loss: 0.3377 - val_accuracy: 0.9128 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3011 - accuracy: 0.9284 - val_loss: 0.3349 - val_accuracy: 0.9110 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2953 - accuracy: 0.9246 - val_loss: 0.3215 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2853 - accuracy: 0.9283 - val_loss: 0.3010 - val_accuracy: 0.9198 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2842 - accuracy: 0.9295 - val_loss: 0.3132 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2732 - accuracy: 0.9314 - val_loss: 0.3045 - val_accuracy: 0.9146 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2746 - accuracy: 0.9279 - val_loss: 0.3307 - val_accuracy: 0.9073 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2666 - accuracy: 0.9291 - val_loss: 0.2813 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2640 - accuracy: 0.9285 - val_loss: 0.2786 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2582 - accuracy: 0.9321 - val_loss: 0.2721 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2508 - accuracy: 0.9343 - val_loss: 0.2732 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2499 - accuracy: 0.9346 - val_loss: 0.2746 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2460 - accuracy: 0.9374 - val_loss: 0.2729 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2464 - accuracy: 0.9351 - val_loss: 0.2691 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2471 - accuracy: 0.9341 - val_loss: 0.2692 - val_accuracy: 0.9260 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2455 - accuracy: 0.9351 - val_loss: 0.2693 - val_accuracy: 0.9246 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2448 - accuracy: 0.9361 - val_loss: 0.2699 - val_accuracy: 0.9240 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2445 - accuracy: 0.9340 - val_loss: 0.2691 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2463 - accuracy: 0.9338 - val_loss: 0.2693 - val_accuracy: 0.9246 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2451 - accuracy: 0.9365 - val_loss: 0.2681 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2433 - accuracy: 0.9355 - val_loss: 0.2676 - val_accuracy: 0.9268 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2410 - accuracy: 0.9356 - val_loss: 0.2675 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2458 - accuracy: 0.9337 - val_loss: 0.2679 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2403 - accuracy: 0.9362 - val_loss: 0.2676 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2436 - accuracy: 0.9339 - val_loss: 0.2675 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2480 - accuracy: 0.9315 - val_loss: 0.2674 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.2469 - accuracy: 0.9348 - val_loss: 0.2675 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2450 - accuracy: 0.9347 - val_loss: 0.2673 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2783 - accuracy: 0.9258\n",
      "17/17 [==============================] - 1s 18ms/step\n",
      "TP:5718, TN:9971, FP:484, FN:773, loss0.2783259451389313, acc0.9258232031157795, sn0.8809120320443691, sp0.9537063605930177, f10.90096903805247, auc0.973344691443054\n",
      "Average Test loss:  0.2741511732339859\n",
      "Average Accuracy:  0.9256225657972383\n",
      "Average Sensitivity:  0.8802341703897705\n",
      "Average Specificity:  0.9538020086083213\n",
      "Average F1 Score:  0.9006560720573102\n",
      "Average AUC Score:  0.9735188257942553\n",
      "AUC for ROC curve 1: 0.9736\n",
      "AUC for ROC curve 2: 0.9736\n",
      "AUC for ROC curve 3: 0.9741\n",
      "AUC for ROC curve 4: 0.9741\n",
      "AUC for ROC curve 5: 0.9737\n",
      "AUC for ROC curve 6: 0.9737\n",
      "AUC for ROC curve 7: 0.9737\n",
      "AUC for ROC curve 8: 0.9737\n",
      "AUC for ROC curve 9: 0.9736\n",
      "AUC for ROC curve 10: 0.9736\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9678324-aa4f-4e5e-8203-326ed29ccaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
