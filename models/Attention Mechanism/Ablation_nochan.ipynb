{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:57:48.217501: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd7222a-ca10-4ac3-b7ca-6272cbbd2fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers=2, d_model=64, nhead=1, dim_feedforward=128, dropout=0.30):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.precnnlayer = layers.Dense(d_model)\n",
    "        self.multi_head_attention = layers.MultiHeadAttention(num_heads=nhead, key_dim=d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dim_feedforward, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.pred_layer = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(2, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.precnnlayer(x)\n",
    "        x += self.multi_head_attention(x, x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x += ffn_output\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = tf.reduce_mean(x, axis=1)\n",
    "        x = self.pred_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "    \n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "    x1 = BatchNormalization()(input1)    \n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "    x1 = Conv1D(256, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "\n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "    x2 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    \n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "    \n",
    "    \n",
    "\n",
    "    # CNN-3\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "    x3 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    attention1 = ResidualAttentionBlock(256, 256)\n",
    "    attention2 = ResidualAttentionBlock(64, 64)\n",
    "    x1 = attention1(x1)\n",
    "    x2 = attention1(x2)\n",
    "    x3 = attention2(x3)\n",
    "    \n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1,input2,input3], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2) \n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 37s 78ms/step - loss: 4.7668 - accuracy: 0.6274 - val_loss: 4.5562 - val_accuracy: 0.5421 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 3.8051 - accuracy: 0.7707 - val_loss: 3.7164 - val_accuracy: 0.5622 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 3.1043 - accuracy: 0.8143 - val_loss: 2.7588 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 59ms/step - loss: 2.5055 - accuracy: 0.8396 - val_loss: 2.2422 - val_accuracy: 0.8472 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.0212 - accuracy: 0.8539 - val_loss: 1.8448 - val_accuracy: 0.8404 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 1.6385 - accuracy: 0.8596 - val_loss: 1.4852 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 1.3481 - accuracy: 0.8679 - val_loss: 1.2524 - val_accuracy: 0.8484 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 1.0996 - accuracy: 0.8816 - val_loss: 1.1048 - val_accuracy: 0.8367 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.9324 - accuracy: 0.8863 - val_loss: 0.9949 - val_accuracy: 0.8319 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.7937 - accuracy: 0.8938 - val_loss: 0.8711 - val_accuracy: 0.8596 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.7024 - accuracy: 0.8938 - val_loss: 0.6837 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.6299 - accuracy: 0.8966 - val_loss: 0.5746 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.5759 - accuracy: 0.8985 - val_loss: 0.5503 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.5299 - accuracy: 0.9030 - val_loss: 0.5461 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4982 - accuracy: 0.9038 - val_loss: 0.4643 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4673 - accuracy: 0.9063 - val_loss: 0.6213 - val_accuracy: 0.8468 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4437 - accuracy: 0.9097 - val_loss: 0.4553 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4477 - accuracy: 0.9060 - val_loss: 0.4608 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4360 - accuracy: 0.9079 - val_loss: 0.4175 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4196 - accuracy: 0.9071 - val_loss: 0.4668 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4059 - accuracy: 0.9102 - val_loss: 0.4241 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4104 - accuracy: 0.9067 - val_loss: 0.4350 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4035 - accuracy: 0.9067 - val_loss: 0.4538 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3891 - accuracy: 0.9112 - val_loss: 0.4749 - val_accuracy: 0.8746 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4010 - accuracy: 0.9073 - val_loss: 0.3893 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3879 - accuracy: 0.9108 - val_loss: 0.3724 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3958 - accuracy: 0.9063 - val_loss: 0.3993 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3851 - accuracy: 0.9112 - val_loss: 0.6085 - val_accuracy: 0.8279 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3907 - accuracy: 0.9083 - val_loss: 0.3945 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3770 - accuracy: 0.9138 - val_loss: 0.5276 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3856 - accuracy: 0.9118 - val_loss: 0.3755 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3753 - accuracy: 0.9147 - val_loss: 0.3824 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3812 - accuracy: 0.9121 - val_loss: 0.3867 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3946 - accuracy: 0.9080 - val_loss: 0.4461 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3692 - accuracy: 0.9141 - val_loss: 0.4019 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3834 - accuracy: 0.9098 - val_loss: 0.4263 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3801 - accuracy: 0.9085 - val_loss: 0.4169 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3733 - accuracy: 0.9131 - val_loss: 0.3587 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3781 - accuracy: 0.9107 - val_loss: 0.3840 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3819 - accuracy: 0.9102 - val_loss: 0.4078 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3769 - accuracy: 0.9089 - val_loss: 0.3732 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3680 - accuracy: 0.9127 - val_loss: 0.4930 - val_accuracy: 0.8674 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3666 - accuracy: 0.9133 - val_loss: 0.4460 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3778 - accuracy: 0.9140 - val_loss: 0.3962 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3689 - accuracy: 0.9142 - val_loss: 0.3546 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3733 - accuracy: 0.9114 - val_loss: 0.3686 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3725 - accuracy: 0.9148 - val_loss: 0.4027 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3679 - accuracy: 0.9149 - val_loss: 0.4075 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3805 - accuracy: 0.9092 - val_loss: 0.3868 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3734 - accuracy: 0.9145 - val_loss: 0.3870 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3702 - accuracy: 0.9124 - val_loss: 0.4183 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3669 - accuracy: 0.9166 - val_loss: 0.4345 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3784 - accuracy: 0.9132 - val_loss: 0.3874 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3797 - accuracy: 0.9097 - val_loss: 0.3924 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3673 - accuracy: 0.9150 - val_loss: 0.4990 - val_accuracy: 0.8694 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3728 - accuracy: 0.9161 - val_loss: 0.3716 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3765 - accuracy: 0.9128 - val_loss: 0.4032 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3710 - accuracy: 0.9154 - val_loss: 0.3642 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3649 - accuracy: 0.9133 - val_loss: 0.4364 - val_accuracy: 0.8891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3691 - accuracy: 0.9147 - val_loss: 0.3601 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3671 - accuracy: 0.9152 - val_loss: 0.3769 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3589 - accuracy: 0.9163 - val_loss: 0.4300 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3759 - accuracy: 0.9113 - val_loss: 0.3776 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3737 - accuracy: 0.9115 - val_loss: 0.5507 - val_accuracy: 0.8436 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3740 - accuracy: 0.9133 - val_loss: 0.4002 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3698 - accuracy: 0.9158 - val_loss: 0.3988 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3666 - accuracy: 0.9165 - val_loss: 0.3873 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3675 - accuracy: 0.9152 - val_loss: 0.3705 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3803 - accuracy: 0.9132 - val_loss: 0.3709 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3664 - accuracy: 0.9135 - val_loss: 0.3685 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3697 - accuracy: 0.9138 - val_loss: 0.3701 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3405 - accuracy: 0.9247 - val_loss: 0.3784 - val_accuracy: 0.9057 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3191 - accuracy: 0.9278 - val_loss: 0.3753 - val_accuracy: 0.9039 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3103 - accuracy: 0.9285 - val_loss: 0.3746 - val_accuracy: 0.9025 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3051 - accuracy: 0.9263 - val_loss: 0.3488 - val_accuracy: 0.9079 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2945 - accuracy: 0.9306 - val_loss: 0.3276 - val_accuracy: 0.9122 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2835 - accuracy: 0.9316 - val_loss: 0.3153 - val_accuracy: 0.9150 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2764 - accuracy: 0.9304 - val_loss: 0.3149 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2719 - accuracy: 0.9317 - val_loss: 0.3358 - val_accuracy: 0.9051 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2669 - accuracy: 0.9327 - val_loss: 0.3221 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2692 - accuracy: 0.9295 - val_loss: 0.2781 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2552 - accuracy: 0.9342 - val_loss: 0.2834 - val_accuracy: 0.9186 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2546 - accuracy: 0.9338 - val_loss: 0.2811 - val_accuracy: 0.9198 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2534 - accuracy: 0.9357 - val_loss: 0.2809 - val_accuracy: 0.9202 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2490 - accuracy: 0.9330 - val_loss: 0.2789 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2524 - accuracy: 0.9346 - val_loss: 0.2796 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2526 - accuracy: 0.9338 - val_loss: 0.2785 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2518 - accuracy: 0.9364 - val_loss: 0.2777 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2421 - accuracy: 0.9381 - val_loss: 0.2768 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2444 - accuracy: 0.9390 - val_loss: 0.2764 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2487 - accuracy: 0.9367 - val_loss: 0.2768 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2423 - accuracy: 0.9388 - val_loss: 0.2759 - val_accuracy: 0.9236 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2482 - accuracy: 0.9357 - val_loss: 0.2753 - val_accuracy: 0.9240 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2474 - accuracy: 0.9367 - val_loss: 0.2754 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2481 - accuracy: 0.9349 - val_loss: 0.2757 - val_accuracy: 0.9236 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2414 - accuracy: 0.9380 - val_loss: 0.2755 - val_accuracy: 0.9236 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2452 - accuracy: 0.9346 - val_loss: 0.2749 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2456 - accuracy: 0.9365 - val_loss: 0.2751 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2450 - accuracy: 0.9379 - val_loss: 0.2748 - val_accuracy: 0.9240 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2478 - accuracy: 0.9356 - val_loss: 0.2746 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 6s 12ms/step - loss: 0.2830 - accuracy: 0.9248\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5647, TN:10025, FP:430, FN:844, loss0.2829645872116089, acc0.9248200165230733, sn0.8699738098906178, sp0.9588713534194165, f10.8986314449395291, auc0.9731825495640839\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 32s 60ms/step - loss: 4.8745 - accuracy: 0.6203 - val_loss: 4.1584 - val_accuracy: 0.6448 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 3.9218 - accuracy: 0.7385 - val_loss: 3.5366 - val_accuracy: 0.7292 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 3.1986 - accuracy: 0.8069 - val_loss: 2.8854 - val_accuracy: 0.8289 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.6145 - accuracy: 0.8318 - val_loss: 2.3439 - val_accuracy: 0.8385 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.1291 - accuracy: 0.8467 - val_loss: 1.9145 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 1.7512 - accuracy: 0.8620 - val_loss: 1.5617 - val_accuracy: 0.8678 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 1.4452 - accuracy: 0.8629 - val_loss: 1.2999 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 1.1957 - accuracy: 0.8743 - val_loss: 1.1674 - val_accuracy: 0.8536 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 1.0072 - accuracy: 0.8834 - val_loss: 0.9220 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.8730 - accuracy: 0.8881 - val_loss: 0.8925 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.7618 - accuracy: 0.8904 - val_loss: 0.7176 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.6768 - accuracy: 0.8955 - val_loss: 0.7262 - val_accuracy: 0.8634 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.6150 - accuracy: 0.8978 - val_loss: 0.6202 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.5760 - accuracy: 0.8982 - val_loss: 0.5753 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.5360 - accuracy: 0.9011 - val_loss: 0.5190 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.5042 - accuracy: 0.9030 - val_loss: 0.5450 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.4780 - accuracy: 0.9060 - val_loss: 0.4669 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.4535 - accuracy: 0.9088 - val_loss: 0.4832 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.4423 - accuracy: 0.9066 - val_loss: 0.4531 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4376 - accuracy: 0.9074 - val_loss: 0.4116 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.4195 - accuracy: 0.9108 - val_loss: 0.4650 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4155 - accuracy: 0.9086 - val_loss: 0.4323 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4121 - accuracy: 0.9075 - val_loss: 0.4052 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.4053 - accuracy: 0.9072 - val_loss: 0.3949 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.4022 - accuracy: 0.9102 - val_loss: 0.4248 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3931 - accuracy: 0.9124 - val_loss: 0.4710 - val_accuracy: 0.8773 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3967 - accuracy: 0.9089 - val_loss: 0.3957 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3966 - accuracy: 0.9109 - val_loss: 0.3963 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3882 - accuracy: 0.9128 - val_loss: 0.3770 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3956 - accuracy: 0.9076 - val_loss: 0.5114 - val_accuracy: 0.8700 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3827 - accuracy: 0.9099 - val_loss: 0.3808 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3889 - accuracy: 0.9119 - val_loss: 0.3745 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3854 - accuracy: 0.9107 - val_loss: 0.5806 - val_accuracy: 0.8351 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3908 - accuracy: 0.9115 - val_loss: 0.4762 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 4s 44ms/step - loss: 0.3830 - accuracy: 0.9136 - val_loss: 0.4378 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3861 - accuracy: 0.9136 - val_loss: 0.3900 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3784 - accuracy: 0.9152 - val_loss: 0.3996 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3889 - accuracy: 0.9132 - val_loss: 0.4234 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3841 - accuracy: 0.9120 - val_loss: 0.3768 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3712 - accuracy: 0.9132 - val_loss: 0.4001 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 0.3775 - accuracy: 0.9117 - val_loss: 0.4167 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3787 - accuracy: 0.9111 - val_loss: 0.4817 - val_accuracy: 0.8736 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3830 - accuracy: 0.9124 - val_loss: 0.4222 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3787 - accuracy: 0.9113 - val_loss: 0.4152 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3796 - accuracy: 0.9109 - val_loss: 0.3849 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3746 - accuracy: 0.9147 - val_loss: 0.3545 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3769 - accuracy: 0.9126 - val_loss: 0.4968 - val_accuracy: 0.8732 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3838 - accuracy: 0.9125 - val_loss: 0.4115 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3834 - accuracy: 0.9114 - val_loss: 0.3853 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3711 - accuracy: 0.9140 - val_loss: 0.3908 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3725 - accuracy: 0.9154 - val_loss: 0.4017 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3761 - accuracy: 0.9112 - val_loss: 0.3853 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3641 - accuracy: 0.9144 - val_loss: 0.4060 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3689 - accuracy: 0.9126 - val_loss: 0.3780 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3690 - accuracy: 0.9125 - val_loss: 0.3632 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3641 - accuracy: 0.9181 - val_loss: 0.4342 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3734 - accuracy: 0.9134 - val_loss: 0.4064 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3689 - accuracy: 0.9112 - val_loss: 0.3611 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3612 - accuracy: 0.9150 - val_loss: 0.3712 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3675 - accuracy: 0.9116 - val_loss: 0.3658 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3650 - accuracy: 0.9157 - val_loss: 0.4072 - val_accuracy: 0.9027 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3659 - accuracy: 0.9143 - val_loss: 0.4072 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3657 - accuracy: 0.9166 - val_loss: 0.4326 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3695 - accuracy: 0.9149 - val_loss: 0.3724 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3693 - accuracy: 0.9161 - val_loss: 0.3648 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3787 - accuracy: 0.9138 - val_loss: 0.3826 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3622 - accuracy: 0.9151 - val_loss: 0.3927 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3621 - accuracy: 0.9199 - val_loss: 0.4028 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3763 - accuracy: 0.9126 - val_loss: 0.4383 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3751 - accuracy: 0.9132 - val_loss: 0.3532 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3704 - accuracy: 0.9126 - val_loss: 0.4274 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3363 - accuracy: 0.9229 - val_loss: 0.3818 - val_accuracy: 0.9043 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3145 - accuracy: 0.9296 - val_loss: 0.3798 - val_accuracy: 0.9057 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3065 - accuracy: 0.9274 - val_loss: 0.3417 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2975 - accuracy: 0.9288 - val_loss: 0.3221 - val_accuracy: 0.9140 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2879 - accuracy: 0.9341 - val_loss: 0.3184 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2832 - accuracy: 0.9300 - val_loss: 0.3381 - val_accuracy: 0.9091 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2715 - accuracy: 0.9316 - val_loss: 0.3219 - val_accuracy: 0.9079 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2721 - accuracy: 0.9308 - val_loss: 0.3153 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2628 - accuracy: 0.9332 - val_loss: 0.3041 - val_accuracy: 0.9130 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.2590 - accuracy: 0.9352 - val_loss: 0.2921 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2543 - accuracy: 0.9332 - val_loss: 0.2808 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2479 - accuracy: 0.9357 - val_loss: 0.2774 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2494 - accuracy: 0.9360 - val_loss: 0.2773 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2453 - accuracy: 0.9387 - val_loss: 0.2747 - val_accuracy: 0.9272 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2443 - accuracy: 0.9368 - val_loss: 0.2743 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2466 - accuracy: 0.9361 - val_loss: 0.2749 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2445 - accuracy: 0.9365 - val_loss: 0.2756 - val_accuracy: 0.9246 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2482 - accuracy: 0.9356 - val_loss: 0.2741 - val_accuracy: 0.9264 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2434 - accuracy: 0.9386 - val_loss: 0.2731 - val_accuracy: 0.9270 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2423 - accuracy: 0.9358 - val_loss: 0.2706 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2493 - accuracy: 0.9348 - val_loss: 0.2711 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2405 - accuracy: 0.9367 - val_loss: 0.2711 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2424 - accuracy: 0.9396 - val_loss: 0.2713 - val_accuracy: 0.9278 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2454 - accuracy: 0.9363 - val_loss: 0.2713 - val_accuracy: 0.9274 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2410 - accuracy: 0.9373 - val_loss: 0.2713 - val_accuracy: 0.9276 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2449 - accuracy: 0.9363 - val_loss: 0.2713 - val_accuracy: 0.9282 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2429 - accuracy: 0.9373 - val_loss: 0.2714 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2431 - accuracy: 0.9391 - val_loss: 0.2715 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2426 - accuracy: 0.9372 - val_loss: 0.2712 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 7s 12ms/step - loss: 0.2843 - accuracy: 0.9245\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5673, TN:9993, FP:462, FN:818, loss0.2843373417854309, acc0.924465950666824, sn0.8739793560314282, sp0.9558106169296987, f10.8986218913353399, auc0.9730215570527297\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 31s 57ms/step - loss: 4.7512 - accuracy: 0.6377 - val_loss: 5.1391 - val_accuracy: 0.4509 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 3.7757 - accuracy: 0.7855 - val_loss: 3.5389 - val_accuracy: 0.6795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 3.0711 - accuracy: 0.8353 - val_loss: 2.7488 - val_accuracy: 0.8462 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 2.5046 - accuracy: 0.8482 - val_loss: 2.2693 - val_accuracy: 0.8389 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 2.0236 - accuracy: 0.8606 - val_loss: 1.8164 - val_accuracy: 0.8606 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.6412 - accuracy: 0.8665 - val_loss: 1.5244 - val_accuracy: 0.8440 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 1.3375 - accuracy: 0.8777 - val_loss: 1.3380 - val_accuracy: 0.8331 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 1.1058 - accuracy: 0.8836 - val_loss: 1.0379 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.9296 - accuracy: 0.8902 - val_loss: 1.0169 - val_accuracy: 0.8396 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.7970 - accuracy: 0.8977 - val_loss: 0.7648 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.6972 - accuracy: 0.8992 - val_loss: 0.6865 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.6301 - accuracy: 0.8984 - val_loss: 0.5910 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.5730 - accuracy: 0.9025 - val_loss: 0.5447 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5310 - accuracy: 0.9047 - val_loss: 0.5631 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5127 - accuracy: 0.9026 - val_loss: 0.4698 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4776 - accuracy: 0.9028 - val_loss: 0.6230 - val_accuracy: 0.8554 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4567 - accuracy: 0.9038 - val_loss: 0.4282 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4332 - accuracy: 0.9098 - val_loss: 0.4430 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4369 - accuracy: 0.9061 - val_loss: 0.4400 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4199 - accuracy: 0.9075 - val_loss: 0.3897 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4156 - accuracy: 0.9055 - val_loss: 0.3980 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3994 - accuracy: 0.9108 - val_loss: 0.4106 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3994 - accuracy: 0.9130 - val_loss: 0.4499 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3949 - accuracy: 0.9097 - val_loss: 0.3981 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4023 - accuracy: 0.9052 - val_loss: 0.4818 - val_accuracy: 0.8795 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3950 - accuracy: 0.9091 - val_loss: 0.4824 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3910 - accuracy: 0.9108 - val_loss: 0.3908 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3838 - accuracy: 0.9133 - val_loss: 0.3748 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3832 - accuracy: 0.9128 - val_loss: 0.3735 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4038 - accuracy: 0.9085 - val_loss: 0.4110 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3836 - accuracy: 0.9138 - val_loss: 0.3970 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3786 - accuracy: 0.9141 - val_loss: 0.3953 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3921 - accuracy: 0.9102 - val_loss: 0.4224 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3857 - accuracy: 0.9070 - val_loss: 0.3734 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3765 - accuracy: 0.9099 - val_loss: 0.4093 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3722 - accuracy: 0.9124 - val_loss: 0.3618 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3775 - accuracy: 0.9100 - val_loss: 0.3941 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3701 - accuracy: 0.9150 - val_loss: 0.3772 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3727 - accuracy: 0.9121 - val_loss: 0.3782 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3770 - accuracy: 0.9104 - val_loss: 0.3731 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3695 - accuracy: 0.9140 - val_loss: 0.4169 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3707 - accuracy: 0.9173 - val_loss: 0.4704 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3859 - accuracy: 0.9118 - val_loss: 0.3907 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3662 - accuracy: 0.9158 - val_loss: 0.3655 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3754 - accuracy: 0.9114 - val_loss: 0.5140 - val_accuracy: 0.8664 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3730 - accuracy: 0.9131 - val_loss: 0.4406 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3667 - accuracy: 0.9134 - val_loss: 0.4113 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3715 - accuracy: 0.9133 - val_loss: 0.3832 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3781 - accuracy: 0.9093 - val_loss: 0.3910 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3713 - accuracy: 0.9108 - val_loss: 0.3900 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3742 - accuracy: 0.9102 - val_loss: 0.3824 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3665 - accuracy: 0.9136 - val_loss: 0.4547 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3698 - accuracy: 0.9130 - val_loss: 0.3634 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3726 - accuracy: 0.9118 - val_loss: 0.3841 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3659 - accuracy: 0.9141 - val_loss: 0.4519 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3589 - accuracy: 0.9155 - val_loss: 0.3991 - val_accuracy: 0.9065 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3720 - accuracy: 0.9132 - val_loss: 0.3563 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3737 - accuracy: 0.9135 - val_loss: 0.3750 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3622 - accuracy: 0.9206 - val_loss: 0.3903 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3721 - accuracy: 0.9115 - val_loss: 0.5338 - val_accuracy: 0.8506 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3786 - accuracy: 0.9131 - val_loss: 0.3622 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3704 - accuracy: 0.9164 - val_loss: 0.3993 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3661 - accuracy: 0.9111 - val_loss: 0.4212 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3620 - accuracy: 0.9175 - val_loss: 0.4263 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3811 - accuracy: 0.9103 - val_loss: 0.3579 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3713 - accuracy: 0.9138 - val_loss: 0.3817 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3671 - accuracy: 0.9144 - val_loss: 0.5329 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3609 - accuracy: 0.9155 - val_loss: 0.3513 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3655 - accuracy: 0.9173 - val_loss: 0.3709 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3606 - accuracy: 0.9180 - val_loss: 0.4066 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3730 - accuracy: 0.9163 - val_loss: 0.4185 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3437 - accuracy: 0.9250 - val_loss: 0.3571 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3245 - accuracy: 0.9283 - val_loss: 0.3664 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3100 - accuracy: 0.9298 - val_loss: 0.3458 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3041 - accuracy: 0.9281 - val_loss: 0.3656 - val_accuracy: 0.9061 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2970 - accuracy: 0.9292 - val_loss: 0.3342 - val_accuracy: 0.9105 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2821 - accuracy: 0.9309 - val_loss: 0.3853 - val_accuracy: 0.8973 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2794 - accuracy: 0.9302 - val_loss: 0.3227 - val_accuracy: 0.9107 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2734 - accuracy: 0.9310 - val_loss: 0.3183 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2699 - accuracy: 0.9329 - val_loss: 0.2878 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 59ms/step - loss: 0.2566 - accuracy: 0.9346 - val_loss: 0.2974 - val_accuracy: 0.9180 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.2554 - accuracy: 0.9385 - val_loss: 0.2807 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2540 - accuracy: 0.9356 - val_loss: 0.2832 - val_accuracy: 0.9234 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2507 - accuracy: 0.9371 - val_loss: 0.2811 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2496 - accuracy: 0.9364 - val_loss: 0.2808 - val_accuracy: 0.9248 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2471 - accuracy: 0.9385 - val_loss: 0.2792 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2558 - accuracy: 0.9332 - val_loss: 0.2819 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2490 - accuracy: 0.9369 - val_loss: 0.2799 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2506 - accuracy: 0.9360 - val_loss: 0.2777 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2471 - accuracy: 0.9373 - val_loss: 0.2780 - val_accuracy: 0.9258 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2479 - accuracy: 0.9373 - val_loss: 0.2757 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2469 - accuracy: 0.9375 - val_loss: 0.2757 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2462 - accuracy: 0.9376 - val_loss: 0.2756 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2460 - accuracy: 0.9378 - val_loss: 0.2752 - val_accuracy: 0.9258 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2436 - accuracy: 0.9400 - val_loss: 0.2750 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2457 - accuracy: 0.9377 - val_loss: 0.2751 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2500 - accuracy: 0.9367 - val_loss: 0.2753 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2443 - accuracy: 0.9379 - val_loss: 0.2752 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2481 - accuracy: 0.9367 - val_loss: 0.2748 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2424 - accuracy: 0.9391 - val_loss: 0.2746 - val_accuracy: 0.9258 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 7s 13ms/step - loss: 0.2871 - accuracy: 0.9240\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5636, TN:10022, FP:433, FN:855, loss0.2870902419090271, acc0.9239938628584917, sn0.8682791557541211, sp0.9585844093735055, f10.8974522292993631, auc0.9731662226497478\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 29s 68ms/step - loss: 4.7652 - accuracy: 0.6328 - val_loss: 5.7165 - val_accuracy: 0.3925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 3.8066 - accuracy: 0.7811 - val_loss: 3.4391 - val_accuracy: 0.7764 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 3.1318 - accuracy: 0.8186 - val_loss: 2.8395 - val_accuracy: 0.8181 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 2.5502 - accuracy: 0.8351 - val_loss: 2.2918 - val_accuracy: 0.8448 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 2.0672 - accuracy: 0.8521 - val_loss: 1.8851 - val_accuracy: 0.8414 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 1.6683 - accuracy: 0.8673 - val_loss: 1.7107 - val_accuracy: 0.7986 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 1.3606 - accuracy: 0.8737 - val_loss: 1.4132 - val_accuracy: 0.8263 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 1.1266 - accuracy: 0.8834 - val_loss: 1.1177 - val_accuracy: 0.8514 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.9570 - accuracy: 0.8845 - val_loss: 1.0167 - val_accuracy: 0.8470 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.8227 - accuracy: 0.8925 - val_loss: 0.8139 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.7132 - accuracy: 0.8987 - val_loss: 0.7826 - val_accuracy: 0.8594 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.6458 - accuracy: 0.8985 - val_loss: 0.6755 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.5871 - accuracy: 0.8999 - val_loss: 0.5867 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5306 - accuracy: 0.9071 - val_loss: 0.7228 - val_accuracy: 0.8391 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.5112 - accuracy: 0.9024 - val_loss: 0.4683 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4821 - accuracy: 0.9050 - val_loss: 0.4646 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4649 - accuracy: 0.9051 - val_loss: 0.5264 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4446 - accuracy: 0.9092 - val_loss: 0.4258 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4284 - accuracy: 0.9086 - val_loss: 0.4403 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4163 - accuracy: 0.9080 - val_loss: 0.4725 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4123 - accuracy: 0.9091 - val_loss: 0.3972 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4166 - accuracy: 0.9078 - val_loss: 0.4531 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4129 - accuracy: 0.9047 - val_loss: 0.5102 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4068 - accuracy: 0.9065 - val_loss: 0.4411 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3982 - accuracy: 0.9081 - val_loss: 0.5073 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3942 - accuracy: 0.9091 - val_loss: 0.3797 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3987 - accuracy: 0.9091 - val_loss: 0.3723 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3921 - accuracy: 0.9103 - val_loss: 0.4174 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3922 - accuracy: 0.9079 - val_loss: 0.4473 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3861 - accuracy: 0.9104 - val_loss: 0.3879 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3760 - accuracy: 0.9116 - val_loss: 0.4005 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3887 - accuracy: 0.9089 - val_loss: 0.4043 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3814 - accuracy: 0.9112 - val_loss: 0.4320 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3760 - accuracy: 0.9109 - val_loss: 0.4392 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3810 - accuracy: 0.9097 - val_loss: 0.3799 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3851 - accuracy: 0.9064 - val_loss: 0.3650 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3742 - accuracy: 0.9108 - val_loss: 0.4189 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3758 - accuracy: 0.9114 - val_loss: 0.3811 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3817 - accuracy: 0.9103 - val_loss: 0.3937 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3736 - accuracy: 0.9121 - val_loss: 0.4275 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3764 - accuracy: 0.9111 - val_loss: 0.4383 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3826 - accuracy: 0.9118 - val_loss: 0.3587 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3723 - accuracy: 0.9134 - val_loss: 0.3976 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3794 - accuracy: 0.9108 - val_loss: 0.4440 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3686 - accuracy: 0.9147 - val_loss: 0.3973 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3758 - accuracy: 0.9133 - val_loss: 0.4797 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3769 - accuracy: 0.9127 - val_loss: 0.4083 - val_accuracy: 0.9031 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3795 - accuracy: 0.9100 - val_loss: 0.3876 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3651 - accuracy: 0.9164 - val_loss: 0.4132 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.3796 - accuracy: 0.9119 - val_loss: 0.3966 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 6s 61ms/step - loss: 0.3751 - accuracy: 0.9121 - val_loss: 0.4296 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3738 - accuracy: 0.9109 - val_loss: 0.3932 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3728 - accuracy: 0.9108 - val_loss: 0.4055 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3697 - accuracy: 0.9161 - val_loss: 0.3587 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3689 - accuracy: 0.9135 - val_loss: 0.3964 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3666 - accuracy: 0.9125 - val_loss: 0.3770 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3715 - accuracy: 0.9155 - val_loss: 0.3767 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3621 - accuracy: 0.9184 - val_loss: 0.4716 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3696 - accuracy: 0.9141 - val_loss: 0.3874 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3648 - accuracy: 0.9144 - val_loss: 0.3730 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3753 - accuracy: 0.9102 - val_loss: 0.5085 - val_accuracy: 0.8530 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3724 - accuracy: 0.9132 - val_loss: 0.3645 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3671 - accuracy: 0.9131 - val_loss: 0.3703 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3508 - accuracy: 0.9184 - val_loss: 0.4800 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3638 - accuracy: 0.9161 - val_loss: 0.4099 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3733 - accuracy: 0.9132 - val_loss: 0.3989 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3605 - accuracy: 0.9165 - val_loss: 0.4193 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3684 - accuracy: 0.9145 - val_loss: 0.3762 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3656 - accuracy: 0.9148 - val_loss: 0.3723 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3669 - accuracy: 0.9164 - val_loss: 0.4106 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3800 - accuracy: 0.9150 - val_loss: 0.4015 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3458 - accuracy: 0.9261 - val_loss: 0.3887 - val_accuracy: 0.9071 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3280 - accuracy: 0.9276 - val_loss: 0.3932 - val_accuracy: 0.9025 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3157 - accuracy: 0.9261 - val_loss: 0.3782 - val_accuracy: 0.9063 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3086 - accuracy: 0.9307 - val_loss: 0.3862 - val_accuracy: 0.8997 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2994 - accuracy: 0.9266 - val_loss: 0.3324 - val_accuracy: 0.9154 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2884 - accuracy: 0.9302 - val_loss: 0.3137 - val_accuracy: 0.9156 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2824 - accuracy: 0.9331 - val_loss: 0.3068 - val_accuracy: 0.9178 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2812 - accuracy: 0.9283 - val_loss: 0.3017 - val_accuracy: 0.9182 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2669 - accuracy: 0.9333 - val_loss: 0.3218 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2658 - accuracy: 0.9330 - val_loss: 0.3055 - val_accuracy: 0.9150 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2573 - accuracy: 0.9366 - val_loss: 0.2956 - val_accuracy: 0.9182 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2557 - accuracy: 0.9361 - val_loss: 0.2903 - val_accuracy: 0.9202 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2555 - accuracy: 0.9355 - val_loss: 0.2885 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2549 - accuracy: 0.9350 - val_loss: 0.2859 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2560 - accuracy: 0.9362 - val_loss: 0.2840 - val_accuracy: 0.9224 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2528 - accuracy: 0.9359 - val_loss: 0.2854 - val_accuracy: 0.9218 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2548 - accuracy: 0.9366 - val_loss: 0.2849 - val_accuracy: 0.9202 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 59ms/step - loss: 0.2515 - accuracy: 0.9365 - val_loss: 0.2855 - val_accuracy: 0.9216 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2517 - accuracy: 0.9366 - val_loss: 0.2840 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2552 - accuracy: 0.9356 - val_loss: 0.2816 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2531 - accuracy: 0.9364 - val_loss: 0.2816 - val_accuracy: 0.9232 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2520 - accuracy: 0.9362 - val_loss: 0.2816 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2560 - accuracy: 0.9339 - val_loss: 0.2814 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2483 - accuracy: 0.9353 - val_loss: 0.2818 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2479 - accuracy: 0.9387 - val_loss: 0.2818 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2499 - accuracy: 0.9368 - val_loss: 0.2816 - val_accuracy: 0.9234 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2529 - accuracy: 0.9342 - val_loss: 0.2814 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2513 - accuracy: 0.9361 - val_loss: 0.2817 - val_accuracy: 0.9240 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2460 - accuracy: 0.9387 - val_loss: 0.2814 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 7s 13ms/step - loss: 0.2886 - accuracy: 0.9247\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5646, TN:10024, FP:431, FN:845, loss0.28855541348457336, acc0.9247019945709902, sn0.8698197504236636, sp0.9587757054041128, f10.8984723106301719, auc0.9737890399103907\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 30s 70ms/step - loss: 4.8103 - accuracy: 0.6195 - val_loss: 4.4711 - val_accuracy: 0.5331 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 3.8352 - accuracy: 0.7742 - val_loss: 3.4604 - val_accuracy: 0.7812 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 3.1546 - accuracy: 0.8141 - val_loss: 2.8037 - val_accuracy: 0.8371 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 2.5695 - accuracy: 0.8393 - val_loss: 2.2951 - val_accuracy: 0.8494 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 2.0838 - accuracy: 0.8576 - val_loss: 1.9228 - val_accuracy: 0.8371 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 1.7019 - accuracy: 0.8608 - val_loss: 1.5721 - val_accuracy: 0.8484 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.3928 - accuracy: 0.8727 - val_loss: 1.3154 - val_accuracy: 0.8554 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.1501 - accuracy: 0.8789 - val_loss: 1.1145 - val_accuracy: 0.8628 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.9664 - accuracy: 0.8880 - val_loss: 0.9500 - val_accuracy: 0.8668 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.8288 - accuracy: 0.8923 - val_loss: 0.8024 - val_accuracy: 0.8811 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.7203 - accuracy: 0.8964 - val_loss: 0.7179 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.6455 - accuracy: 0.8961 - val_loss: 0.6416 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.5962 - accuracy: 0.8961 - val_loss: 0.5956 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.5405 - accuracy: 0.9008 - val_loss: 0.6166 - val_accuracy: 0.8674 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4956 - accuracy: 0.9092 - val_loss: 0.5134 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.4786 - accuracy: 0.9065 - val_loss: 0.5612 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4679 - accuracy: 0.9047 - val_loss: 0.4965 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4478 - accuracy: 0.9059 - val_loss: 0.4540 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.4289 - accuracy: 0.9089 - val_loss: 0.4109 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.4256 - accuracy: 0.9067 - val_loss: 0.3997 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4120 - accuracy: 0.9107 - val_loss: 0.4635 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.4154 - accuracy: 0.9060 - val_loss: 0.3995 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3982 - accuracy: 0.9099 - val_loss: 0.4102 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3960 - accuracy: 0.9138 - val_loss: 0.3836 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3981 - accuracy: 0.9114 - val_loss: 0.3979 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3891 - accuracy: 0.9121 - val_loss: 0.4047 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3838 - accuracy: 0.9114 - val_loss: 0.3695 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3820 - accuracy: 0.9133 - val_loss: 0.3760 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3866 - accuracy: 0.9121 - val_loss: 0.3746 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3876 - accuracy: 0.9099 - val_loss: 0.3811 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3865 - accuracy: 0.9125 - val_loss: 0.3807 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3865 - accuracy: 0.9111 - val_loss: 0.4233 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3688 - accuracy: 0.9131 - val_loss: 0.3896 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3835 - accuracy: 0.9102 - val_loss: 0.4256 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3788 - accuracy: 0.9119 - val_loss: 0.3688 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3802 - accuracy: 0.9136 - val_loss: 0.4148 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3728 - accuracy: 0.9135 - val_loss: 0.4339 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3912 - accuracy: 0.9067 - val_loss: 0.3788 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3837 - accuracy: 0.9105 - val_loss: 0.3755 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3705 - accuracy: 0.9155 - val_loss: 0.3717 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3733 - accuracy: 0.9118 - val_loss: 0.6059 - val_accuracy: 0.8414 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3835 - accuracy: 0.9114 - val_loss: 0.4798 - val_accuracy: 0.8628 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3799 - accuracy: 0.9132 - val_loss: 0.3843 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3708 - accuracy: 0.9131 - val_loss: 0.3840 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3739 - accuracy: 0.9115 - val_loss: 0.3632 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 49ms/step - loss: 0.3674 - accuracy: 0.9140 - val_loss: 0.4090 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3660 - accuracy: 0.9161 - val_loss: 0.4225 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3754 - accuracy: 0.9123 - val_loss: 0.3847 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3759 - accuracy: 0.9115 - val_loss: 0.3584 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3637 - accuracy: 0.9142 - val_loss: 0.4039 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3684 - accuracy: 0.9119 - val_loss: 0.3933 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3778 - accuracy: 0.9155 - val_loss: 0.3630 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3732 - accuracy: 0.9145 - val_loss: 0.3788 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3775 - accuracy: 0.9134 - val_loss: 0.4054 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3681 - accuracy: 0.9161 - val_loss: 0.4729 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3708 - accuracy: 0.9134 - val_loss: 0.3944 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3670 - accuracy: 0.9143 - val_loss: 0.4068 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3678 - accuracy: 0.9139 - val_loss: 0.3574 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3610 - accuracy: 0.9140 - val_loss: 0.3690 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3600 - accuracy: 0.9166 - val_loss: 0.4082 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3761 - accuracy: 0.9098 - val_loss: 0.4197 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3569 - accuracy: 0.9155 - val_loss: 0.3940 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3742 - accuracy: 0.9146 - val_loss: 0.3613 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3669 - accuracy: 0.9100 - val_loss: 0.4610 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3595 - accuracy: 0.9151 - val_loss: 0.3574 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3638 - accuracy: 0.9167 - val_loss: 0.3857 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3676 - accuracy: 0.9142 - val_loss: 0.3964 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3707 - accuracy: 0.9139 - val_loss: 0.4115 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3627 - accuracy: 0.9124 - val_loss: 0.3708 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3632 - accuracy: 0.9109 - val_loss: 0.4136 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3659 - accuracy: 0.9106 - val_loss: 0.3631 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3298 - accuracy: 0.9255 - val_loss: 0.3579 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3121 - accuracy: 0.9296 - val_loss: 0.3605 - val_accuracy: 0.9069 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3035 - accuracy: 0.9285 - val_loss: 0.3664 - val_accuracy: 0.9053 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2963 - accuracy: 0.9292 - val_loss: 0.3229 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2872 - accuracy: 0.9289 - val_loss: 0.3116 - val_accuracy: 0.9176 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2788 - accuracy: 0.9320 - val_loss: 0.3156 - val_accuracy: 0.9140 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2700 - accuracy: 0.9317 - val_loss: 0.2906 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2666 - accuracy: 0.9336 - val_loss: 0.3242 - val_accuracy: 0.9103 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2586 - accuracy: 0.9354 - val_loss: 0.2986 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2561 - accuracy: 0.9344 - val_loss: 0.3039 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2502 - accuracy: 0.9371 - val_loss: 0.2853 - val_accuracy: 0.9188 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2515 - accuracy: 0.9335 - val_loss: 0.2825 - val_accuracy: 0.9184 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2459 - accuracy: 0.9385 - val_loss: 0.2770 - val_accuracy: 0.9206 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2477 - accuracy: 0.9379 - val_loss: 0.2773 - val_accuracy: 0.9216 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2468 - accuracy: 0.9355 - val_loss: 0.2743 - val_accuracy: 0.9208 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2475 - accuracy: 0.9379 - val_loss: 0.2742 - val_accuracy: 0.9220 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2437 - accuracy: 0.9377 - val_loss: 0.2736 - val_accuracy: 0.9204 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2418 - accuracy: 0.9386 - val_loss: 0.2736 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2454 - accuracy: 0.9379 - val_loss: 0.2748 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2402 - accuracy: 0.9381 - val_loss: 0.2727 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2431 - accuracy: 0.9363 - val_loss: 0.2725 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2390 - accuracy: 0.9407 - val_loss: 0.2722 - val_accuracy: 0.9232 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2441 - accuracy: 0.9367 - val_loss: 0.2724 - val_accuracy: 0.9224 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2388 - accuracy: 0.9373 - val_loss: 0.2723 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2429 - accuracy: 0.9361 - val_loss: 0.2722 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2425 - accuracy: 0.9388 - val_loss: 0.2717 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2440 - accuracy: 0.9368 - val_loss: 0.2719 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.2430 - accuracy: 0.9377 - val_loss: 0.2720 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2424 - accuracy: 0.9384 - val_loss: 0.2719 - val_accuracy: 0.9226 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 7s 13ms/step - loss: 0.2818 - accuracy: 0.9252\n",
      "17/17 [==============================] - 1s 20ms/step\n",
      "TP:5645, TN:10034, FP:421, FN:846, loss0.2818465530872345, acc0.9252330933553641, sn0.8696656909567093, sp0.9597321855571497, f10.8991001035279127, auc0.9739250778825495\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 33s 64ms/step - loss: 4.7766 - accuracy: 0.6316 - val_loss: 5.0009 - val_accuracy: 0.4172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 3.8078 - accuracy: 0.7781 - val_loss: 3.5030 - val_accuracy: 0.7304 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 3.1064 - accuracy: 0.8232 - val_loss: 2.7641 - val_accuracy: 0.8494 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 2.5242 - accuracy: 0.8464 - val_loss: 2.2490 - val_accuracy: 0.8616 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 2.0369 - accuracy: 0.8645 - val_loss: 1.9014 - val_accuracy: 0.8339 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 1.6544 - accuracy: 0.8721 - val_loss: 1.5695 - val_accuracy: 0.8462 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 1.3428 - accuracy: 0.8798 - val_loss: 1.3144 - val_accuracy: 0.8472 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 1.1236 - accuracy: 0.8809 - val_loss: 1.1368 - val_accuracy: 0.8432 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.9430 - accuracy: 0.8855 - val_loss: 0.9835 - val_accuracy: 0.8472 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.8029 - accuracy: 0.8938 - val_loss: 1.1022 - val_accuracy: 0.7772 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.7056 - accuracy: 0.8955 - val_loss: 0.7015 - val_accuracy: 0.8803 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.6213 - accuracy: 0.9040 - val_loss: 0.9390 - val_accuracy: 0.8041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.5848 - accuracy: 0.8943 - val_loss: 0.5755 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.5342 - accuracy: 0.9006 - val_loss: 0.6076 - val_accuracy: 0.8702 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4908 - accuracy: 0.9080 - val_loss: 0.4603 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4692 - accuracy: 0.9066 - val_loss: 0.4556 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4622 - accuracy: 0.9044 - val_loss: 0.4466 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4345 - accuracy: 0.9125 - val_loss: 0.4346 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4343 - accuracy: 0.9050 - val_loss: 0.4064 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4167 - accuracy: 0.9095 - val_loss: 0.3973 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4162 - accuracy: 0.9062 - val_loss: 0.4523 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3990 - accuracy: 0.9102 - val_loss: 0.4168 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.4003 - accuracy: 0.9089 - val_loss: 0.4790 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3978 - accuracy: 0.9083 - val_loss: 0.4134 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3884 - accuracy: 0.9132 - val_loss: 0.4076 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3981 - accuracy: 0.9105 - val_loss: 0.3789 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3947 - accuracy: 0.9079 - val_loss: 0.4237 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3874 - accuracy: 0.9101 - val_loss: 0.4439 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3898 - accuracy: 0.9104 - val_loss: 0.3677 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3810 - accuracy: 0.9114 - val_loss: 0.3808 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3839 - accuracy: 0.9109 - val_loss: 0.4195 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3766 - accuracy: 0.9121 - val_loss: 0.5061 - val_accuracy: 0.8620 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3894 - accuracy: 0.9116 - val_loss: 0.4816 - val_accuracy: 0.8801 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3924 - accuracy: 0.9071 - val_loss: 0.3842 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3811 - accuracy: 0.9095 - val_loss: 0.4212 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3810 - accuracy: 0.9081 - val_loss: 0.3702 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3700 - accuracy: 0.9108 - val_loss: 0.3626 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3790 - accuracy: 0.9089 - val_loss: 0.3983 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3764 - accuracy: 0.9114 - val_loss: 0.3613 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3682 - accuracy: 0.9173 - val_loss: 0.3998 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3802 - accuracy: 0.9085 - val_loss: 0.3567 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3715 - accuracy: 0.9114 - val_loss: 0.4686 - val_accuracy: 0.8771 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3739 - accuracy: 0.9138 - val_loss: 0.3791 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3813 - accuracy: 0.9125 - val_loss: 0.4943 - val_accuracy: 0.8684 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3707 - accuracy: 0.9130 - val_loss: 0.4207 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3791 - accuracy: 0.9131 - val_loss: 0.3856 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3729 - accuracy: 0.9138 - val_loss: 0.4190 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3815 - accuracy: 0.9105 - val_loss: 0.3740 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3687 - accuracy: 0.9130 - val_loss: 0.3709 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3722 - accuracy: 0.9134 - val_loss: 0.4066 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 59ms/step - loss: 0.3779 - accuracy: 0.9103 - val_loss: 0.4697 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3645 - accuracy: 0.9149 - val_loss: 0.3718 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3689 - accuracy: 0.9142 - val_loss: 0.3935 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3701 - accuracy: 0.9144 - val_loss: 0.3704 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3737 - accuracy: 0.9141 - val_loss: 0.4081 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3696 - accuracy: 0.9146 - val_loss: 0.4402 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3651 - accuracy: 0.9161 - val_loss: 0.4052 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3797 - accuracy: 0.9116 - val_loss: 0.3835 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3724 - accuracy: 0.9161 - val_loss: 0.3955 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3587 - accuracy: 0.9169 - val_loss: 0.4950 - val_accuracy: 0.8570 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3662 - accuracy: 0.9130 - val_loss: 0.3622 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3715 - accuracy: 0.9117 - val_loss: 0.3710 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3624 - accuracy: 0.9138 - val_loss: 0.4113 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3581 - accuracy: 0.9161 - val_loss: 0.3899 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3646 - accuracy: 0.9160 - val_loss: 0.3633 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3658 - accuracy: 0.9144 - val_loss: 0.3650 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3668 - accuracy: 0.9144 - val_loss: 0.3850 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3745 - accuracy: 0.9126 - val_loss: 0.4026 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3775 - accuracy: 0.9120 - val_loss: 0.3854 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3606 - accuracy: 0.9155 - val_loss: 0.3875 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3603 - accuracy: 0.9149 - val_loss: 0.4291 - val_accuracy: 0.8925 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3291 - accuracy: 0.9255 - val_loss: 0.3607 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3106 - accuracy: 0.9291 - val_loss: 0.3418 - val_accuracy: 0.9114 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3007 - accuracy: 0.9297 - val_loss: 0.3465 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2910 - accuracy: 0.9301 - val_loss: 0.3360 - val_accuracy: 0.9101 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2857 - accuracy: 0.9314 - val_loss: 0.3471 - val_accuracy: 0.9047 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2716 - accuracy: 0.9332 - val_loss: 0.3446 - val_accuracy: 0.9037 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 0.2683 - accuracy: 0.9302 - val_loss: 0.3129 - val_accuracy: 0.9128 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2617 - accuracy: 0.9333 - val_loss: 0.3000 - val_accuracy: 0.9164 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2609 - accuracy: 0.9348 - val_loss: 0.2919 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.2574 - accuracy: 0.9332 - val_loss: 0.3122 - val_accuracy: 0.9081 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2515 - accuracy: 0.9353 - val_loss: 0.2855 - val_accuracy: 0.9188 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2487 - accuracy: 0.9380 - val_loss: 0.2820 - val_accuracy: 0.9204 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2462 - accuracy: 0.9374 - val_loss: 0.2829 - val_accuracy: 0.9204 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.2446 - accuracy: 0.9386 - val_loss: 0.2760 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2455 - accuracy: 0.9379 - val_loss: 0.2758 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.2437 - accuracy: 0.9396 - val_loss: 0.2781 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2407 - accuracy: 0.9383 - val_loss: 0.2750 - val_accuracy: 0.9250 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 60ms/step - loss: 0.2411 - accuracy: 0.9371 - val_loss: 0.2770 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2423 - accuracy: 0.9372 - val_loss: 0.2749 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 0.2390 - accuracy: 0.9396 - val_loss: 0.2751 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2438 - accuracy: 0.9378 - val_loss: 0.2743 - val_accuracy: 0.9238 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 0.2406 - accuracy: 0.9392 - val_loss: 0.2733 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 59ms/step - loss: 0.2395 - accuracy: 0.9392 - val_loss: 0.2735 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2379 - accuracy: 0.9377 - val_loss: 0.2735 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2387 - accuracy: 0.9387 - val_loss: 0.2733 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2421 - accuracy: 0.9366 - val_loss: 0.2735 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2340 - accuracy: 0.9385 - val_loss: 0.2730 - val_accuracy: 0.9248 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2415 - accuracy: 0.9392 - val_loss: 0.2728 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2405 - accuracy: 0.9392 - val_loss: 0.2724 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 7s 13ms/step - loss: 0.2820 - accuracy: 0.9241\n",
      "17/17 [==============================] - 2s 21ms/step\n",
      "TP:5645, TN:10015, FP:440, FN:846, loss0.2819516062736511, acc0.9241118848105747, sn0.8696656909567093, sp0.9579148732663797, f10.8977417302798982, auc0.9732672196451092\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 32s 74ms/step - loss: 4.7500 - accuracy: 0.6431 - val_loss: 4.6555 - val_accuracy: 0.4587 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 3.8183 - accuracy: 0.7709 - val_loss: 3.4455 - val_accuracy: 0.7736 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 3.1019 - accuracy: 0.8174 - val_loss: 2.7721 - val_accuracy: 0.8297 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 2.5097 - accuracy: 0.8399 - val_loss: 2.2530 - val_accuracy: 0.8426 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 2.0197 - accuracy: 0.8564 - val_loss: 1.8733 - val_accuracy: 0.8391 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 1.6309 - accuracy: 0.8686 - val_loss: 1.5020 - val_accuracy: 0.8534 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 1.3329 - accuracy: 0.8797 - val_loss: 1.2775 - val_accuracy: 0.8448 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 1.1072 - accuracy: 0.8834 - val_loss: 1.1105 - val_accuracy: 0.8532 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.9338 - accuracy: 0.8943 - val_loss: 1.0252 - val_accuracy: 0.8361 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.8039 - accuracy: 0.8961 - val_loss: 0.7997 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.7044 - accuracy: 0.8996 - val_loss: 0.7107 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.6343 - accuracy: 0.9007 - val_loss: 0.6524 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.5742 - accuracy: 0.9011 - val_loss: 0.8422 - val_accuracy: 0.8205 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.5413 - accuracy: 0.9011 - val_loss: 0.5189 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.5089 - accuracy: 0.9038 - val_loss: 0.4935 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4785 - accuracy: 0.9044 - val_loss: 0.5513 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4679 - accuracy: 0.9067 - val_loss: 0.4466 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4432 - accuracy: 0.9074 - val_loss: 0.4160 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4379 - accuracy: 0.9063 - val_loss: 0.5458 - val_accuracy: 0.8604 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4199 - accuracy: 0.9116 - val_loss: 0.4716 - val_accuracy: 0.8831 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.4103 - accuracy: 0.9120 - val_loss: 0.4080 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4102 - accuracy: 0.9095 - val_loss: 0.3978 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.4059 - accuracy: 0.9082 - val_loss: 0.3905 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3910 - accuracy: 0.9122 - val_loss: 0.4498 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3972 - accuracy: 0.9108 - val_loss: 0.4513 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3900 - accuracy: 0.9116 - val_loss: 0.4165 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3915 - accuracy: 0.9099 - val_loss: 0.3978 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3884 - accuracy: 0.9119 - val_loss: 0.3815 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3907 - accuracy: 0.9128 - val_loss: 0.3852 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3895 - accuracy: 0.9108 - val_loss: 0.5081 - val_accuracy: 0.8730 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3925 - accuracy: 0.9102 - val_loss: 0.3955 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3877 - accuracy: 0.9091 - val_loss: 0.3910 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3771 - accuracy: 0.9133 - val_loss: 0.3801 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3835 - accuracy: 0.9089 - val_loss: 0.3894 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3800 - accuracy: 0.9114 - val_loss: 0.3944 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3812 - accuracy: 0.9120 - val_loss: 0.6283 - val_accuracy: 0.8363 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3780 - accuracy: 0.9121 - val_loss: 0.4117 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3847 - accuracy: 0.9117 - val_loss: 0.3929 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3816 - accuracy: 0.9133 - val_loss: 0.4600 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3807 - accuracy: 0.9109 - val_loss: 0.3644 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3803 - accuracy: 0.9094 - val_loss: 0.4024 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3844 - accuracy: 0.9108 - val_loss: 0.5212 - val_accuracy: 0.8648 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3758 - accuracy: 0.9131 - val_loss: 0.3875 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3762 - accuracy: 0.9102 - val_loss: 0.3933 - val_accuracy: 0.8991 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3757 - accuracy: 0.9132 - val_loss: 0.3636 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3770 - accuracy: 0.9132 - val_loss: 0.3700 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3690 - accuracy: 0.9168 - val_loss: 0.4139 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3784 - accuracy: 0.9136 - val_loss: 0.4369 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3832 - accuracy: 0.9112 - val_loss: 0.3718 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3720 - accuracy: 0.9132 - val_loss: 0.4569 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3744 - accuracy: 0.9111 - val_loss: 0.3950 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3771 - accuracy: 0.9129 - val_loss: 0.3834 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3676 - accuracy: 0.9141 - val_loss: 0.4081 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3676 - accuracy: 0.9150 - val_loss: 0.4250 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3737 - accuracy: 0.9119 - val_loss: 0.3908 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3731 - accuracy: 0.9113 - val_loss: 0.3584 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3668 - accuracy: 0.9141 - val_loss: 0.4842 - val_accuracy: 0.8775 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3666 - accuracy: 0.9132 - val_loss: 0.3501 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3576 - accuracy: 0.9160 - val_loss: 0.3669 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3645 - accuracy: 0.9150 - val_loss: 0.3737 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3624 - accuracy: 0.9170 - val_loss: 0.4147 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3613 - accuracy: 0.9134 - val_loss: 0.3570 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3661 - accuracy: 0.9149 - val_loss: 0.3721 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3587 - accuracy: 0.9181 - val_loss: 0.3975 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3562 - accuracy: 0.9173 - val_loss: 0.4141 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3659 - accuracy: 0.9148 - val_loss: 0.3915 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3725 - accuracy: 0.9108 - val_loss: 0.3774 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3526 - accuracy: 0.9191 - val_loss: 0.4453 - val_accuracy: 0.8863 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3614 - accuracy: 0.9182 - val_loss: 0.5644 - val_accuracy: 0.8187 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3702 - accuracy: 0.9119 - val_loss: 0.4838 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3591 - accuracy: 0.9162 - val_loss: 0.3840 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3350 - accuracy: 0.9261 - val_loss: 0.3574 - val_accuracy: 0.9091 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3148 - accuracy: 0.9276 - val_loss: 0.3526 - val_accuracy: 0.9099 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3041 - accuracy: 0.9293 - val_loss: 0.3410 - val_accuracy: 0.9114 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2964 - accuracy: 0.9308 - val_loss: 0.3217 - val_accuracy: 0.9140 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2872 - accuracy: 0.9279 - val_loss: 0.3412 - val_accuracy: 0.9067 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.2783 - accuracy: 0.9309 - val_loss: 0.3568 - val_accuracy: 0.9033 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2715 - accuracy: 0.9320 - val_loss: 0.3064 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2674 - accuracy: 0.9327 - val_loss: 0.3072 - val_accuracy: 0.9146 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2633 - accuracy: 0.9328 - val_loss: 0.2781 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 59ms/step - loss: 0.2584 - accuracy: 0.9340 - val_loss: 0.3071 - val_accuracy: 0.9120 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2480 - accuracy: 0.9379 - val_loss: 0.2831 - val_accuracy: 0.9204 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2526 - accuracy: 0.9332 - val_loss: 0.2810 - val_accuracy: 0.9212 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2487 - accuracy: 0.9361 - val_loss: 0.2782 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2445 - accuracy: 0.9390 - val_loss: 0.2791 - val_accuracy: 0.9208 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2488 - accuracy: 0.9363 - val_loss: 0.2760 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2446 - accuracy: 0.9338 - val_loss: 0.2779 - val_accuracy: 0.9218 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2411 - accuracy: 0.9378 - val_loss: 0.2756 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.2418 - accuracy: 0.9390 - val_loss: 0.2738 - val_accuracy: 0.9238 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2405 - accuracy: 0.9409 - val_loss: 0.2767 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2420 - accuracy: 0.9380 - val_loss: 0.2758 - val_accuracy: 0.9212 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2409 - accuracy: 0.9379 - val_loss: 0.2745 - val_accuracy: 0.9226 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2368 - accuracy: 0.9408 - val_loss: 0.2741 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2421 - accuracy: 0.9385 - val_loss: 0.2740 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2372 - accuracy: 0.9406 - val_loss: 0.2742 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2391 - accuracy: 0.9395 - val_loss: 0.2743 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2455 - accuracy: 0.9382 - val_loss: 0.2743 - val_accuracy: 0.9226 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2430 - accuracy: 0.9376 - val_loss: 0.2745 - val_accuracy: 0.9230 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2400 - accuracy: 0.9360 - val_loss: 0.2744 - val_accuracy: 0.9224 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 0.2425 - accuracy: 0.9361 - val_loss: 0.2739 - val_accuracy: 0.9228 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 8s 14ms/step - loss: 0.2823 - accuracy: 0.9246\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5640, TN:10029, FP:426, FN:851, loss0.28234443068504333, acc0.9246429835949487, sn0.868895393621938, sp0.9592539454806313, f10.8983037349685434, auc0.9733390993275387\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 32s 62ms/step - loss: 4.7828 - accuracy: 0.6231 - val_loss: 4.6321 - val_accuracy: 0.4693 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 3.7733 - accuracy: 0.7756 - val_loss: 3.7437 - val_accuracy: 0.5070 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 3.0489 - accuracy: 0.8231 - val_loss: 2.8306 - val_accuracy: 0.7611 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 2.4527 - accuracy: 0.8444 - val_loss: 2.1798 - val_accuracy: 0.8674 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 1.9874 - accuracy: 0.8561 - val_loss: 1.7538 - val_accuracy: 0.8686 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.6120 - accuracy: 0.8643 - val_loss: 1.5108 - val_accuracy: 0.8414 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.3254 - accuracy: 0.8742 - val_loss: 1.2233 - val_accuracy: 0.8642 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 1.0964 - accuracy: 0.8876 - val_loss: 1.0845 - val_accuracy: 0.8502 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.9316 - accuracy: 0.8885 - val_loss: 0.8949 - val_accuracy: 0.8732 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.8114 - accuracy: 0.8922 - val_loss: 0.8173 - val_accuracy: 0.8740 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.7205 - accuracy: 0.8953 - val_loss: 0.6713 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.6369 - accuracy: 0.8987 - val_loss: 0.6605 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.5940 - accuracy: 0.8970 - val_loss: 0.6290 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.5430 - accuracy: 0.9033 - val_loss: 0.5170 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.5224 - accuracy: 0.9020 - val_loss: 0.5703 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4975 - accuracy: 0.9010 - val_loss: 0.4897 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4614 - accuracy: 0.9064 - val_loss: 0.5690 - val_accuracy: 0.8600 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.4506 - accuracy: 0.9026 - val_loss: 0.4289 - val_accuracy: 0.9081 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.4325 - accuracy: 0.9048 - val_loss: 0.4320 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4151 - accuracy: 0.9063 - val_loss: 0.4783 - val_accuracy: 0.8865 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.4200 - accuracy: 0.9052 - val_loss: 0.4239 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4013 - accuracy: 0.9079 - val_loss: 0.4818 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4025 - accuracy: 0.9069 - val_loss: 0.4096 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3990 - accuracy: 0.9070 - val_loss: 0.4032 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3913 - accuracy: 0.9118 - val_loss: 0.4803 - val_accuracy: 0.8789 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3943 - accuracy: 0.9067 - val_loss: 0.4064 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3900 - accuracy: 0.9107 - val_loss: 0.3861 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3762 - accuracy: 0.9118 - val_loss: 0.4590 - val_accuracy: 0.8901 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3823 - accuracy: 0.9110 - val_loss: 0.3786 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3831 - accuracy: 0.9122 - val_loss: 0.3894 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3853 - accuracy: 0.9079 - val_loss: 0.5270 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3843 - accuracy: 0.9123 - val_loss: 0.3699 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3817 - accuracy: 0.9122 - val_loss: 0.4079 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3857 - accuracy: 0.9106 - val_loss: 0.3915 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3854 - accuracy: 0.9087 - val_loss: 0.3850 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3834 - accuracy: 0.9102 - val_loss: 0.4187 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3677 - accuracy: 0.9153 - val_loss: 0.3754 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3777 - accuracy: 0.9085 - val_loss: 0.3951 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3815 - accuracy: 0.9102 - val_loss: 0.3888 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3820 - accuracy: 0.9094 - val_loss: 0.3674 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3864 - accuracy: 0.9067 - val_loss: 0.3917 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3667 - accuracy: 0.9110 - val_loss: 0.4264 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3646 - accuracy: 0.9144 - val_loss: 0.4385 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3630 - accuracy: 0.9133 - val_loss: 0.4246 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3832 - accuracy: 0.9092 - val_loss: 0.3844 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3725 - accuracy: 0.9105 - val_loss: 0.4149 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3687 - accuracy: 0.9130 - val_loss: 0.5571 - val_accuracy: 0.8293 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3695 - accuracy: 0.9091 - val_loss: 0.4392 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3707 - accuracy: 0.9124 - val_loss: 0.3642 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3836 - accuracy: 0.9087 - val_loss: 0.4714 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3653 - accuracy: 0.9150 - val_loss: 0.3633 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3648 - accuracy: 0.9129 - val_loss: 0.5490 - val_accuracy: 0.8446 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3717 - accuracy: 0.9093 - val_loss: 0.4230 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3629 - accuracy: 0.9150 - val_loss: 0.3715 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3630 - accuracy: 0.9140 - val_loss: 0.5598 - val_accuracy: 0.8381 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3653 - accuracy: 0.9146 - val_loss: 0.3593 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3703 - accuracy: 0.9131 - val_loss: 0.3568 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3660 - accuracy: 0.9134 - val_loss: 0.5232 - val_accuracy: 0.8510 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3760 - accuracy: 0.9127 - val_loss: 0.3740 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3664 - accuracy: 0.9136 - val_loss: 0.3835 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3714 - accuracy: 0.9149 - val_loss: 0.4488 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3764 - accuracy: 0.9105 - val_loss: 0.3594 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3592 - accuracy: 0.9177 - val_loss: 0.3885 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3675 - accuracy: 0.9147 - val_loss: 0.3672 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3742 - accuracy: 0.9112 - val_loss: 0.3931 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3729 - accuracy: 0.9120 - val_loss: 0.4221 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3647 - accuracy: 0.9146 - val_loss: 0.4200 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3681 - accuracy: 0.9122 - val_loss: 0.4345 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3692 - accuracy: 0.9128 - val_loss: 0.4472 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3732 - accuracy: 0.9132 - val_loss: 0.6476 - val_accuracy: 0.8181 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3659 - accuracy: 0.9138 - val_loss: 0.5313 - val_accuracy: 0.8690 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3328 - accuracy: 0.9244 - val_loss: 0.3895 - val_accuracy: 0.8985 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.3130 - accuracy: 0.9285 - val_loss: 0.3842 - val_accuracy: 0.8993 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3026 - accuracy: 0.9279 - val_loss: 0.3699 - val_accuracy: 0.9009 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2915 - accuracy: 0.9299 - val_loss: 0.3170 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2897 - accuracy: 0.9285 - val_loss: 0.3105 - val_accuracy: 0.9174 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2797 - accuracy: 0.9288 - val_loss: 0.3304 - val_accuracy: 0.9105 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2708 - accuracy: 0.9326 - val_loss: 0.3466 - val_accuracy: 0.9033 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2651 - accuracy: 0.9320 - val_loss: 0.3102 - val_accuracy: 0.9132 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2610 - accuracy: 0.9351 - val_loss: 0.3233 - val_accuracy: 0.9124 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2543 - accuracy: 0.9341 - val_loss: 0.2892 - val_accuracy: 0.9206 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2513 - accuracy: 0.9356 - val_loss: 0.2876 - val_accuracy: 0.9216 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2506 - accuracy: 0.9358 - val_loss: 0.2836 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.2533 - accuracy: 0.9346 - val_loss: 0.2820 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2478 - accuracy: 0.9363 - val_loss: 0.2794 - val_accuracy: 0.9228 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2456 - accuracy: 0.9376 - val_loss: 0.2821 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.2454 - accuracy: 0.9373 - val_loss: 0.2826 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2449 - accuracy: 0.9375 - val_loss: 0.2789 - val_accuracy: 0.9254 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2495 - accuracy: 0.9352 - val_loss: 0.2774 - val_accuracy: 0.9246 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2500 - accuracy: 0.9358 - val_loss: 0.2753 - val_accuracy: 0.9232 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2462 - accuracy: 0.9360 - val_loss: 0.2746 - val_accuracy: 0.9252 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2442 - accuracy: 0.9339 - val_loss: 0.2750 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2418 - accuracy: 0.9383 - val_loss: 0.2750 - val_accuracy: 0.9250 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.2422 - accuracy: 0.9382 - val_loss: 0.2748 - val_accuracy: 0.9252 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2444 - accuracy: 0.9381 - val_loss: 0.2752 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.2427 - accuracy: 0.9390 - val_loss: 0.2747 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.2435 - accuracy: 0.9374 - val_loss: 0.2746 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2459 - accuracy: 0.9327 - val_loss: 0.2744 - val_accuracy: 0.9258 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2447 - accuracy: 0.9367 - val_loss: 0.2748 - val_accuracy: 0.9254 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.2406 - accuracy: 0.9372 - val_loss: 0.2747 - val_accuracy: 0.9256 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 7s 13ms/step - loss: 0.2824 - accuracy: 0.9248\n",
      "17/17 [==============================] - 1s 21ms/step\n",
      "TP:5620, TN:10051, FP:404, FN:871, loss0.28242045640945435, acc0.9247610055470318, sn0.8658142042828532, sp0.9613582018173122, f10.8981222532960447, auc0.9733695280983913\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 33s 67ms/step - loss: 4.7953 - accuracy: 0.6244 - val_loss: 4.2647 - val_accuracy: 0.5407 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 3.8696 - accuracy: 0.7441 - val_loss: 3.4938 - val_accuracy: 0.7160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 3.1260 - accuracy: 0.8154 - val_loss: 2.8009 - val_accuracy: 0.8361 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 2.5524 - accuracy: 0.8352 - val_loss: 2.3704 - val_accuracy: 0.7886 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 2.0665 - accuracy: 0.8520 - val_loss: 1.8374 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.6808 - accuracy: 0.8609 - val_loss: 1.4945 - val_accuracy: 0.8716 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 1.3693 - accuracy: 0.8714 - val_loss: 1.2352 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 1.1377 - accuracy: 0.8790 - val_loss: 1.0494 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.9650 - accuracy: 0.8812 - val_loss: 1.0455 - val_accuracy: 0.8343 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.8294 - accuracy: 0.8898 - val_loss: 0.9402 - val_accuracy: 0.8373 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 5s 57ms/step - loss: 0.7335 - accuracy: 0.8918 - val_loss: 0.7923 - val_accuracy: 0.8504 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.6463 - accuracy: 0.8991 - val_loss: 0.6146 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.5950 - accuracy: 0.8969 - val_loss: 0.6174 - val_accuracy: 0.8785 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.5503 - accuracy: 0.8987 - val_loss: 0.5649 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.5123 - accuracy: 0.9023 - val_loss: 0.4772 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4874 - accuracy: 0.9033 - val_loss: 0.4530 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4568 - accuracy: 0.9092 - val_loss: 0.4544 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4385 - accuracy: 0.9116 - val_loss: 0.5524 - val_accuracy: 0.8644 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.4397 - accuracy: 0.9059 - val_loss: 0.4119 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4357 - accuracy: 0.9047 - val_loss: 0.4209 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.4159 - accuracy: 0.9100 - val_loss: 0.5375 - val_accuracy: 0.8656 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.4244 - accuracy: 0.9051 - val_loss: 0.4165 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.4095 - accuracy: 0.9078 - val_loss: 0.4859 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3955 - accuracy: 0.9115 - val_loss: 0.4017 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3968 - accuracy: 0.9085 - val_loss: 0.4211 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 5s 52ms/step - loss: 0.3967 - accuracy: 0.9091 - val_loss: 0.4074 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3920 - accuracy: 0.9132 - val_loss: 0.4075 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 0.3947 - accuracy: 0.9095 - val_loss: 0.3873 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3892 - accuracy: 0.9117 - val_loss: 0.3964 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3828 - accuracy: 0.9131 - val_loss: 0.4777 - val_accuracy: 0.8763 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3851 - accuracy: 0.9120 - val_loss: 0.4098 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3839 - accuracy: 0.9132 - val_loss: 0.3822 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 5s 55ms/step - loss: 0.3782 - accuracy: 0.9114 - val_loss: 0.3927 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 5s 60ms/step - loss: 0.3830 - accuracy: 0.9124 - val_loss: 0.3887 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 5s 54ms/step - loss: 0.3861 - accuracy: 0.9101 - val_loss: 0.3709 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3781 - accuracy: 0.9139 - val_loss: 0.4059 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3745 - accuracy: 0.9126 - val_loss: 0.3727 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3768 - accuracy: 0.9129 - val_loss: 0.3727 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3707 - accuracy: 0.9126 - val_loss: 0.4303 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3759 - accuracy: 0.9126 - val_loss: 0.4983 - val_accuracy: 0.8734 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3705 - accuracy: 0.9133 - val_loss: 0.5291 - val_accuracy: 0.8674 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3726 - accuracy: 0.9124 - val_loss: 0.3997 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3820 - accuracy: 0.9122 - val_loss: 0.3736 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3737 - accuracy: 0.9144 - val_loss: 0.4206 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3666 - accuracy: 0.9178 - val_loss: 0.3720 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3733 - accuracy: 0.9145 - val_loss: 0.3899 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3752 - accuracy: 0.9113 - val_loss: 0.3949 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3682 - accuracy: 0.9161 - val_loss: 0.3583 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3785 - accuracy: 0.9102 - val_loss: 0.4097 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3759 - accuracy: 0.9124 - val_loss: 0.3617 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3663 - accuracy: 0.9151 - val_loss: 0.3682 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3671 - accuracy: 0.9125 - val_loss: 0.3809 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3683 - accuracy: 0.9162 - val_loss: 0.3763 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3745 - accuracy: 0.9126 - val_loss: 0.4002 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3687 - accuracy: 0.9139 - val_loss: 0.3813 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3644 - accuracy: 0.9143 - val_loss: 0.3585 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3703 - accuracy: 0.9143 - val_loss: 0.4012 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3623 - accuracy: 0.9148 - val_loss: 0.3554 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3714 - accuracy: 0.9126 - val_loss: 0.4639 - val_accuracy: 0.8724 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3744 - accuracy: 0.9102 - val_loss: 0.3702 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3671 - accuracy: 0.9129 - val_loss: 0.4321 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3622 - accuracy: 0.9166 - val_loss: 0.4074 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3680 - accuracy: 0.9147 - val_loss: 0.4437 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3782 - accuracy: 0.9103 - val_loss: 0.4108 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3579 - accuracy: 0.9161 - val_loss: 0.3772 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3663 - accuracy: 0.9152 - val_loss: 0.3568 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3613 - accuracy: 0.9165 - val_loss: 0.3710 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3660 - accuracy: 0.9177 - val_loss: 0.3766 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3711 - accuracy: 0.9143 - val_loss: 0.4910 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3657 - accuracy: 0.9151 - val_loss: 0.3896 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3623 - accuracy: 0.9143 - val_loss: 0.4334 - val_accuracy: 0.8953 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3459 - accuracy: 0.9231 - val_loss: 0.3785 - val_accuracy: 0.9087 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3212 - accuracy: 0.9267 - val_loss: 0.3622 - val_accuracy: 0.9099 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.3108 - accuracy: 0.9262 - val_loss: 0.3334 - val_accuracy: 0.9142 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2985 - accuracy: 0.9280 - val_loss: 0.3459 - val_accuracy: 0.9083 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2932 - accuracy: 0.9307 - val_loss: 0.3246 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2816 - accuracy: 0.9313 - val_loss: 0.3123 - val_accuracy: 0.9166 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2744 - accuracy: 0.9326 - val_loss: 0.3307 - val_accuracy: 0.9112 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2736 - accuracy: 0.9306 - val_loss: 0.2969 - val_accuracy: 0.9170 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2619 - accuracy: 0.9345 - val_loss: 0.2929 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2570 - accuracy: 0.9346 - val_loss: 0.2847 - val_accuracy: 0.9208 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2482 - accuracy: 0.9389 - val_loss: 0.2876 - val_accuracy: 0.9204 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2505 - accuracy: 0.9367 - val_loss: 0.2834 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2479 - accuracy: 0.9365 - val_loss: 0.2840 - val_accuracy: 0.9222 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2461 - accuracy: 0.9367 - val_loss: 0.2804 - val_accuracy: 0.9230 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2454 - accuracy: 0.9392 - val_loss: 0.2788 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2495 - accuracy: 0.9358 - val_loss: 0.2797 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2470 - accuracy: 0.9359 - val_loss: 0.2783 - val_accuracy: 0.9236 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2427 - accuracy: 0.9373 - val_loss: 0.2796 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2457 - accuracy: 0.9358 - val_loss: 0.2787 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2420 - accuracy: 0.9378 - val_loss: 0.2756 - val_accuracy: 0.9242 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2434 - accuracy: 0.9380 - val_loss: 0.2762 - val_accuracy: 0.9236 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2432 - accuracy: 0.9396 - val_loss: 0.2768 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2375 - accuracy: 0.9388 - val_loss: 0.2772 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2426 - accuracy: 0.9384 - val_loss: 0.2775 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2433 - accuracy: 0.9361 - val_loss: 0.2770 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2412 - accuracy: 0.9380 - val_loss: 0.2765 - val_accuracy: 0.9242 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2406 - accuracy: 0.9397 - val_loss: 0.2764 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.2414 - accuracy: 0.9409 - val_loss: 0.2764 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2396 - accuracy: 0.9377 - val_loss: 0.2764 - val_accuracy: 0.9244 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2854 - accuracy: 0.9249\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5653, TN:10021, FP:434, FN:838, loss0.28535521030426025, acc0.9249380384751564, sn0.8708981666923432, sp0.9584887613582018, f10.8988710446811894, auc0.9734088202618186\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 52ms/step - loss: 4.7783 - accuracy: 0.6201 - val_loss: 5.9056 - val_accuracy: 0.3911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 3.8175 - accuracy: 0.7713 - val_loss: 3.5440 - val_accuracy: 0.6837 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 3.1038 - accuracy: 0.8249 - val_loss: 2.7818 - val_accuracy: 0.8365 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 2.5104 - accuracy: 0.8490 - val_loss: 2.2414 - val_accuracy: 0.8608 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 2.0289 - accuracy: 0.8601 - val_loss: 1.8097 - val_accuracy: 0.8618 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 1.6471 - accuracy: 0.8674 - val_loss: 1.5300 - val_accuracy: 0.8490 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 1.3349 - accuracy: 0.8758 - val_loss: 1.2217 - val_accuracy: 0.8712 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 1.1146 - accuracy: 0.8813 - val_loss: 1.0661 - val_accuracy: 0.8568 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.9306 - accuracy: 0.8916 - val_loss: 0.8682 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.8036 - accuracy: 0.8914 - val_loss: 0.9096 - val_accuracy: 0.8414 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.7043 - accuracy: 0.8997 - val_loss: 0.6985 - val_accuracy: 0.8895 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.6301 - accuracy: 0.9007 - val_loss: 0.6180 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.5772 - accuracy: 0.9026 - val_loss: 0.5724 - val_accuracy: 0.8929 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.5362 - accuracy: 0.9002 - val_loss: 0.5248 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4998 - accuracy: 0.9041 - val_loss: 0.5104 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4686 - accuracy: 0.9093 - val_loss: 0.4943 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4624 - accuracy: 0.9027 - val_loss: 0.5383 - val_accuracy: 0.8753 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.4389 - accuracy: 0.9086 - val_loss: 0.4213 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4409 - accuracy: 0.9040 - val_loss: 0.4192 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4220 - accuracy: 0.9070 - val_loss: 0.5753 - val_accuracy: 0.8476 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.4117 - accuracy: 0.9075 - val_loss: 0.4123 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4050 - accuracy: 0.9100 - val_loss: 0.3953 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3949 - accuracy: 0.9092 - val_loss: 0.3855 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.4038 - accuracy: 0.9108 - val_loss: 0.4017 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3923 - accuracy: 0.9076 - val_loss: 0.3977 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3904 - accuracy: 0.9106 - val_loss: 0.3826 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.3980 - accuracy: 0.9056 - val_loss: 0.4083 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3819 - accuracy: 0.9127 - val_loss: 0.4313 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3845 - accuracy: 0.9075 - val_loss: 0.4184 - val_accuracy: 0.8989 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3758 - accuracy: 0.9125 - val_loss: 0.4850 - val_accuracy: 0.8755 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3792 - accuracy: 0.9124 - val_loss: 0.4460 - val_accuracy: 0.8787 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3801 - accuracy: 0.9089 - val_loss: 0.3955 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3851 - accuracy: 0.9088 - val_loss: 0.4416 - val_accuracy: 0.8909 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3787 - accuracy: 0.9100 - val_loss: 0.3600 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3781 - accuracy: 0.9102 - val_loss: 0.3666 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3830 - accuracy: 0.9104 - val_loss: 0.3954 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3717 - accuracy: 0.9131 - val_loss: 0.5625 - val_accuracy: 0.8393 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3732 - accuracy: 0.9112 - val_loss: 0.3913 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3773 - accuracy: 0.9148 - val_loss: 0.3908 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3757 - accuracy: 0.9124 - val_loss: 0.4210 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3711 - accuracy: 0.9138 - val_loss: 0.3565 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3820 - accuracy: 0.9092 - val_loss: 0.3802 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3715 - accuracy: 0.9118 - val_loss: 0.3532 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3725 - accuracy: 0.9083 - val_loss: 0.3934 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3702 - accuracy: 0.9130 - val_loss: 0.4298 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3759 - accuracy: 0.9083 - val_loss: 0.4686 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3572 - accuracy: 0.9167 - val_loss: 0.3860 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3709 - accuracy: 0.9114 - val_loss: 0.4578 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3714 - accuracy: 0.9137 - val_loss: 0.3646 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3724 - accuracy: 0.9115 - val_loss: 0.3630 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3753 - accuracy: 0.9131 - val_loss: 0.3756 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.3764 - accuracy: 0.9139 - val_loss: 0.4056 - val_accuracy: 0.8957 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 4s 46ms/step - loss: 0.3644 - accuracy: 0.9147 - val_loss: 0.3747 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3718 - accuracy: 0.9108 - val_loss: 0.3701 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3698 - accuracy: 0.9155 - val_loss: 0.4546 - val_accuracy: 0.8821 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 4s 45ms/step - loss: 0.3651 - accuracy: 0.9139 - val_loss: 0.4186 - val_accuracy: 0.8995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 5s 53ms/step - loss: 0.3789 - accuracy: 0.9110 - val_loss: 0.3605 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3650 - accuracy: 0.9165 - val_loss: 0.4000 - val_accuracy: 0.9069 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3774 - accuracy: 0.9129 - val_loss: 0.3806 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.3689 - accuracy: 0.9163 - val_loss: 0.3707 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 5s 51ms/step - loss: 0.3780 - accuracy: 0.9161 - val_loss: 0.3558 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.3730 - accuracy: 0.9149 - val_loss: 0.4012 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3668 - accuracy: 0.9155 - val_loss: 0.4007 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3711 - accuracy: 0.9162 - val_loss: 0.3619 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3714 - accuracy: 0.9121 - val_loss: 0.3562 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.3726 - accuracy: 0.9158 - val_loss: 0.3512 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.3681 - accuracy: 0.9120 - val_loss: 0.3565 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3678 - accuracy: 0.9124 - val_loss: 0.4172 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3699 - accuracy: 0.9112 - val_loss: 0.3880 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3626 - accuracy: 0.9162 - val_loss: 0.3601 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3755 - accuracy: 0.9107 - val_loss: 0.3879 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3372 - accuracy: 0.9226 - val_loss: 0.3570 - val_accuracy: 0.9134 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3219 - accuracy: 0.9261 - val_loss: 0.3503 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3078 - accuracy: 0.9301 - val_loss: 0.3491 - val_accuracy: 0.9095 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3010 - accuracy: 0.9264 - val_loss: 0.3487 - val_accuracy: 0.9073 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2898 - accuracy: 0.9269 - val_loss: 0.3164 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2785 - accuracy: 0.9311 - val_loss: 0.3327 - val_accuracy: 0.9105 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2732 - accuracy: 0.9300 - val_loss: 0.3197 - val_accuracy: 0.9152 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2650 - accuracy: 0.9343 - val_loss: 0.3289 - val_accuracy: 0.9085 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2658 - accuracy: 0.9332 - val_loss: 0.2882 - val_accuracy: 0.9212 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2620 - accuracy: 0.9309 - val_loss: 0.3027 - val_accuracy: 0.9118 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2530 - accuracy: 0.9335 - val_loss: 0.2871 - val_accuracy: 0.9200 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2533 - accuracy: 0.9340 - val_loss: 0.2800 - val_accuracy: 0.9226 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2530 - accuracy: 0.9346 - val_loss: 0.2798 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2512 - accuracy: 0.9359 - val_loss: 0.2796 - val_accuracy: 0.9244 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2489 - accuracy: 0.9350 - val_loss: 0.2783 - val_accuracy: 0.9256 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2507 - accuracy: 0.9367 - val_loss: 0.2760 - val_accuracy: 0.9268 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2502 - accuracy: 0.9373 - val_loss: 0.2783 - val_accuracy: 0.9246 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 4s 40ms/step - loss: 0.2472 - accuracy: 0.9356 - val_loss: 0.2767 - val_accuracy: 0.9256 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 5s 50ms/step - loss: 0.2439 - accuracy: 0.9370 - val_loss: 0.2760 - val_accuracy: 0.9266 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 5s 49ms/step - loss: 0.2474 - accuracy: 0.9340 - val_loss: 0.2757 - val_accuracy: 0.9262 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2488 - accuracy: 0.9357 - val_loss: 0.2763 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 4s 48ms/step - loss: 0.2456 - accuracy: 0.9356 - val_loss: 0.2760 - val_accuracy: 0.9264 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 4s 47ms/step - loss: 0.2481 - accuracy: 0.9363 - val_loss: 0.2756 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.2443 - accuracy: 0.9361 - val_loss: 0.2758 - val_accuracy: 0.9260 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.2474 - accuracy: 0.9368 - val_loss: 0.2755 - val_accuracy: 0.9268 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 42ms/step - loss: 0.2486 - accuracy: 0.9355 - val_loss: 0.2756 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.2450 - accuracy: 0.9354 - val_loss: 0.2756 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 4s 41ms/step - loss: 0.2425 - accuracy: 0.9367 - val_loss: 0.2754 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.2458 - accuracy: 0.9378 - val_loss: 0.2757 - val_accuracy: 0.9262 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 10ms/step - loss: 0.2816 - accuracy: 0.9246\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5660, TN:10008, FP:447, FN:831, loss0.28159841895103455, acc0.9245839726189071, sn0.871976582961023, sp0.957245337159254, f10.8985553262422608, auc0.9731902488535611\n",
      "Average Test loss:  0.2838464260101318\n",
      "Average Accuracy:  0.9246252803021362\n",
      "Average Sensitivity:  0.8698967801571407\n",
      "Average Specificity:  0.9586035389765662\n",
      "Average F1 Score:  0.8983872069200253\n",
      "Average AUC Score:  0.9733659363245923\n",
      "AUC for ROC curve 1: 0.9732\n",
      "AUC for ROC curve 2: 0.9732\n",
      "AUC for ROC curve 3: 0.9730\n",
      "AUC for ROC curve 4: 0.9730\n",
      "AUC for ROC curve 5: 0.9732\n",
      "AUC for ROC curve 6: 0.9732\n",
      "AUC for ROC curve 7: 0.9738\n",
      "AUC for ROC curve 8: 0.9738\n",
      "AUC for ROC curve 9: 0.9739\n",
      "AUC for ROC curve 10: 0.9739\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af832c-9632-49ab-a779-795f12b22027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
