{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-028b2NqE02R",
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1693232083105,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "-028b2NqE02R"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-02 16:43:08.652764: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "tf.compat.v1.Session(config=config).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a034672d",
   "metadata": {
    "executionInfo": {
     "elapsed": 1150,
     "status": "ok",
     "timestamp": 1693232086282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "a034672d"
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import BatchNormalization, LeakyReLU, MaxPooling1D, Dropout, Flatten, Dense, Conv1D,Reshape,multiply,GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from scipy.interpolate import splev, splrep\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591596f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1693232087740,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "591596f3"
   },
   "outputs": [],
   "source": [
    "base_dir = \"./dataset\"\n",
    "ir = 3 \n",
    "before = 2\n",
    "after = 2\n",
    "\n",
    "scaler = lambda arr: (arr - np.min(arr)) / (np.max(arr) - np.min(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cef782d",
   "metadata": {
    "executionInfo": {
     "elapsed": 303,
     "status": "ok",
     "timestamp": 1693232089719,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "0cef782d"
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "def interpolate_numpy_array(arr, desired_length):\n",
    "    cs = CubicSpline(np.linspace(0, 1, len(arr)), arr)\n",
    "    x_new = np.linspace(0, 1, desired_length)\n",
    "    interpolated_arr = cs(x_new)\n",
    "    return interpolated_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "KXj-pyPoXKm3",
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1693232167282,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "KXj-pyPoXKm3"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def load_data():\n",
    "    tm = np.arange(0, (before + 1 + after) * 60, step=1 / float(ir))\n",
    "    with open(os.path.join(base_dir, \"apnea.pkl\"), 'rb') as f:\n",
    "        apnea_ecg = pickle.load(f)\n",
    "    x_train1,x_train2,x_train3 = [],[],[]\n",
    "    o_train, y_train = apnea_ecg[\"o_train\"], apnea_ecg[\"y_train\"]\n",
    "    groups_train = apnea_ecg[\"groups_train\"]\n",
    "    for i in range(len(o_train)):\n",
    "        min_distance_list, max_distance_list= o_train[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_train1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_train2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_train3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_training1,x_training2,x_training3,y_training,groups_training = [],[],[],[],[]\n",
    "    x_val1,x_val2,x_val3,y_val,groups_val = [],[],[],[],[]\n",
    "\n",
    "    trainlist = random.sample(range(len(o_train)),int(len(o_train)*0.7))\n",
    "    num=[i for i in range(16713)]\n",
    "    vallist = set(num) - set(trainlist)\n",
    "    vallist = list(vallist)\n",
    "    for i in trainlist:\n",
    "        x_training1.append(x_train1[i])\n",
    "        x_training2.append(x_train2[i])\n",
    "        x_training3.append(x_train3[i])\n",
    "        y_training.append(y_train[i])\n",
    "        groups_training.append(groups_train[i])\n",
    "    for i in vallist:\n",
    "        x_val1.append(x_train1[i])\n",
    "        x_val2.append(x_train2[i])\n",
    "        x_val3.append(x_train3[i])\n",
    "        y_val.append(y_train[i])\n",
    "        groups_val.append(groups_train[i])\n",
    "\n",
    "    x_training1 = np.array(x_training1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training2 = np.array(x_training2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_training3 = np.array(x_training3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_training = np.array(y_training, dtype=\"float32\")\n",
    "    x_val1 = np.array(x_val1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val2 = np.array(x_val2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_val3 = np.array(x_val3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_val = np.array(y_val, dtype=\"float32\")\n",
    "\n",
    "    x_test1,x_test2,x_test3 = [],[],[]\n",
    "    o_test, y_test = apnea_ecg[\"o_test\"], apnea_ecg[\"y_test\"]\n",
    "    groups_test = apnea_ecg[\"groups_test\"]\n",
    "    for i in range(len(o_test)):\n",
    "        min_distance_list, max_distance_list = o_test[i]\n",
    "        min_distance_list_inter = interpolate_numpy_array(min_distance_list,900)\n",
    "        max_distance_list_inter = interpolate_numpy_array(max_distance_list,900)\n",
    "        x_test1.append([min_distance_list_inter, max_distance_list_inter])\n",
    "        x_test2.append([min_distance_list_inter[180:720], max_distance_list_inter[180:720]])\n",
    "        x_test3.append([min_distance_list_inter[360:540], max_distance_list_inter[360:540]])\n",
    "    x_test1 = np.array(x_test1, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test2 = np.array(x_test2, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    x_test3 = np.array(x_test3, dtype=\"float32\").transpose((0, 2, 1))\n",
    "    y_test = np.array(y_test, dtype=\"float32\")\n",
    "\n",
    "    return x_training1, x_training2, x_training3, y_training, groups_training, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ef5b79-3b27-4888-9bb6-ad2415d780d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, ratio=4,**kwargs):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.channels = input_shape[-1]\n",
    "        self.fc1 = tf.keras.layers.Dense(self.channels // self.ratio, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(self.channels, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        max_pool = tf.reduce_max(inputs, axis=[1, 2], keepdims=True)\n",
    "        avg_pool = tf.reduce_mean(inputs, axis=[1, 2], keepdims=True)\n",
    "        max_pool = self.fc1(max_pool)\n",
    "        avg_pool = self.fc1(avg_pool)\n",
    "        attention = self.fc2(tf.keras.layers.add([max_pool, avg_pool]))\n",
    "        return inputs * attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c50e53-c397-4432-bb00-4ec3a202a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71576d5f-5bfa-4321-bd05-f1a9b3c2d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv1D(num_output_features, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv3 = tf.keras.layers.Conv1D(num_output_features, kernel_size=1, strides=1, padding='valid', use_bias=False)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu = tf.keras.layers.ReLU()\n",
    "        self.sigmoid = tf.keras.layers.Activation('sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x       \n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out = self.sigmoid(out)      \n",
    "\n",
    "        out = out * residual + residual\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "GnDFx0fGCy1m",
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1693232096445,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "GnDFx0fGCy1m"
   },
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling1D\n",
    "from tensorflow.keras.layers import Input, BatchNormalization, GRU, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "def create_model(input_a_shape, input_b_shape, input_c_shape, weight=1e-3,rate=1):\n",
    "    leaky_relu = LeakyReLU()\n",
    "\n",
    "    # CNN-1\n",
    "    input1 = Input(shape=input_a_shape)\n",
    "\n",
    "    x1 = BatchNormalization()(input1)    \n",
    "    x1 = Conv1D(96, kernel_size=11, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)   \n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=3,strides=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(384, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)\n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "\n",
    "    x1 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x1 = leaky_relu(x1)   \n",
    "    x1 = MaxPooling1D(pool_size=2, padding=\"same\")(x1)\n",
    "    x1 = Dropout(0.5)(x1)\n",
    "    \n",
    "    # CNN-2\n",
    "    input2 = Input(shape=input_b_shape)\n",
    "    x2 = BatchNormalization()(input2)\n",
    "\n",
    "    x2 = Conv1D(16, kernel_size=9, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    \n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(32, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(128, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)\n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "    x2 = Conv1D(256, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "    x2 = leaky_relu(x2)   \n",
    "    x2 = MaxPooling1D(pool_size=3)(x2)    \n",
    "    x2 = Dropout(0.5)(x2)\n",
    "\n",
    "\n",
    "    # CNN-3\n",
    "    input3 = Input(shape=input_c_shape)\n",
    "   \n",
    "    x3 = Conv1D(16, kernel_size=7, strides=4, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = MaxPooling1D(pool_size=3)(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    x3 = Conv1D(64, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",\n",
    "               kernel_regularizer=l2(weight), bias_regularizer=l2(weight))(x3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = leaky_relu(x3)\n",
    "    x3 = Dropout(0.5)(x3)\n",
    "\n",
    "    attention1 = ResidualAttentionBlock(256, 256)\n",
    "    attention2 = ResidualAttentionBlock(64, 64)\n",
    "    x1 = attention1(x1)\n",
    "    x2 = attention1(x2)\n",
    "    x3 = attention2(x3)\n",
    "  \n",
    "    concat = keras.layers.concatenate([x1, x2, x3], name=\"Concat_Layer\", axis=-1)  \n",
    "    concat=ChannelAttention()(concat)\n",
    "    x = GlobalAveragePooling1D()(concat)\n",
    "    dp = Dropout(0.5)(x)\n",
    "    outputs = Dense(2, activation='softmax', name=\"Output_Layer\")(dp)\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c830eb3",
   "metadata": {
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1693232100397,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "6c830eb3"
   },
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 70 and \\\n",
    "            (epoch - 1) % 10 == 0:\n",
    "        lr *= 0.1\n",
    "    print(\"Learning rate: \", lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ded3f606",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693232101699,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "ded3f606"
   },
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    \"\"\"Plot performance curve\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axes[0].plot(history[\"loss\"], \"r-\", history[\"val_loss\"], \"b-\", linewidth=0.5)\n",
    "    axes[0].set_title(\"Loss\")\n",
    "    axes[1].plot(history[\"accuracy\"], \"r-\", history[\"val_accuracy\"], \"b-\", linewidth=0.5)\n",
    "    axes[1].set_title(\"Accuracy\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "v6XhEYeu7cgM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45661,
     "status": "ok",
     "timestamp": 1693233004292,
     "user": {
      "displayName": "Hiếu Nguyễn Xuân",
      "userId": "09184859202144170734"
     },
     "user_tz": -420
    },
    "id": "v6XhEYeu7cgM",
    "outputId": "4f452c32-a1f2-4c9e-b7bc-05ca348f3613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape (11699, 900, 2) (11699, 540, 2) (11699, 180, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    x_train1, x_train2, x_train3, y_train, groups_train, x_val1, x_val2, x_val3, y_val, groups_val, x_test1, x_test2, x_test3, y_test, groups_test= load_data()\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=2)  # Convert to two categories\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=2)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "    print('input_shape', x_train1.shape, x_train2.shape, x_train3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae98c6d-d470-4371-b7bb-6707c20168f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 25s 51ms/step - loss: 4.2079 - accuracy: 0.6489 - val_loss: 4.3095 - val_accuracy: 0.4059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 2.9567 - accuracy: 0.8127 - val_loss: 2.7727 - val_accuracy: 0.5560 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 2.1068 - accuracy: 0.8358 - val_loss: 1.7945 - val_accuracy: 0.8444 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.5057 - accuracy: 0.8601 - val_loss: 1.2734 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.1357 - accuracy: 0.8708 - val_loss: 0.9876 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.8937 - accuracy: 0.8767 - val_loss: 0.7847 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.7328 - accuracy: 0.8896 - val_loss: 0.7908 - val_accuracy: 0.8504 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6389 - accuracy: 0.8903 - val_loss: 0.6429 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5769 - accuracy: 0.8952 - val_loss: 0.5585 - val_accuracy: 0.8947 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5213 - accuracy: 0.9020 - val_loss: 0.6584 - val_accuracy: 0.8590 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4979 - accuracy: 0.9017 - val_loss: 0.4491 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4842 - accuracy: 0.8988 - val_loss: 0.5711 - val_accuracy: 0.8588 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4707 - accuracy: 0.8981 - val_loss: 0.4435 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4399 - accuracy: 0.9049 - val_loss: 0.4288 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4344 - accuracy: 0.9051 - val_loss: 0.4210 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4314 - accuracy: 0.9053 - val_loss: 0.4122 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4263 - accuracy: 0.9075 - val_loss: 0.4193 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4222 - accuracy: 0.9062 - val_loss: 0.4144 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.4193 - accuracy: 0.9050 - val_loss: 0.5151 - val_accuracy: 0.8682 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4126 - accuracy: 0.9051 - val_loss: 0.4215 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4062 - accuracy: 0.9074 - val_loss: 0.3760 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4049 - accuracy: 0.9071 - val_loss: 0.4061 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4050 - accuracy: 0.9038 - val_loss: 0.3817 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4041 - accuracy: 0.9070 - val_loss: 0.4508 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3962 - accuracy: 0.9082 - val_loss: 0.3746 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3876 - accuracy: 0.9103 - val_loss: 0.4524 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3935 - accuracy: 0.9073 - val_loss: 0.3631 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3931 - accuracy: 0.9120 - val_loss: 0.4308 - val_accuracy: 0.8869 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3949 - accuracy: 0.9091 - val_loss: 0.3729 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3906 - accuracy: 0.9092 - val_loss: 0.4682 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3983 - accuracy: 0.9083 - val_loss: 0.3612 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3864 - accuracy: 0.9108 - val_loss: 0.3604 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3845 - accuracy: 0.9125 - val_loss: 0.3931 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3915 - accuracy: 0.9120 - val_loss: 0.4017 - val_accuracy: 0.9067 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3957 - accuracy: 0.9097 - val_loss: 0.5907 - val_accuracy: 0.8325 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3905 - accuracy: 0.9120 - val_loss: 0.3798 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3844 - accuracy: 0.9117 - val_loss: 0.3650 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3922 - accuracy: 0.9088 - val_loss: 0.3778 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3899 - accuracy: 0.9087 - val_loss: 0.3667 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3846 - accuracy: 0.9074 - val_loss: 0.3617 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3824 - accuracy: 0.9102 - val_loss: 0.3642 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3816 - accuracy: 0.9110 - val_loss: 0.3841 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3859 - accuracy: 0.9099 - val_loss: 0.3806 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3755 - accuracy: 0.9120 - val_loss: 0.3714 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3819 - accuracy: 0.9094 - val_loss: 0.3789 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3817 - accuracy: 0.9108 - val_loss: 0.4174 - val_accuracy: 0.8993 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3787 - accuracy: 0.9123 - val_loss: 0.3633 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3809 - accuracy: 0.9123 - val_loss: 0.3706 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3800 - accuracy: 0.9131 - val_loss: 0.3599 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3701 - accuracy: 0.9144 - val_loss: 0.3836 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3842 - accuracy: 0.9116 - val_loss: 0.3814 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3845 - accuracy: 0.9110 - val_loss: 0.3577 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3827 - accuracy: 0.9070 - val_loss: 0.7872 - val_accuracy: 0.7603 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3761 - accuracy: 0.9113 - val_loss: 0.3923 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3684 - accuracy: 0.9124 - val_loss: 0.3938 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3742 - accuracy: 0.9134 - val_loss: 0.4511 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3794 - accuracy: 0.9121 - val_loss: 0.3858 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3780 - accuracy: 0.9150 - val_loss: 0.4094 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3722 - accuracy: 0.9132 - val_loss: 0.3470 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3796 - accuracy: 0.9089 - val_loss: 0.3514 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3681 - accuracy: 0.9155 - val_loss: 0.3797 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3717 - accuracy: 0.9121 - val_loss: 0.4017 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3902 - accuracy: 0.9091 - val_loss: 0.4557 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3713 - accuracy: 0.9103 - val_loss: 0.3441 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3641 - accuracy: 0.9152 - val_loss: 0.3601 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3754 - accuracy: 0.9123 - val_loss: 0.3604 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3730 - accuracy: 0.9125 - val_loss: 0.4436 - val_accuracy: 0.8871 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3753 - accuracy: 0.9139 - val_loss: 0.4325 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3765 - accuracy: 0.9145 - val_loss: 0.3515 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3847 - accuracy: 0.9098 - val_loss: 0.3625 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3707 - accuracy: 0.9115 - val_loss: 0.3479 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3433 - accuracy: 0.9217 - val_loss: 0.3288 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3273 - accuracy: 0.9255 - val_loss: 0.3210 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3151 - accuracy: 0.9263 - val_loss: 0.3109 - val_accuracy: 0.9230 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3042 - accuracy: 0.9288 - val_loss: 0.3054 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2988 - accuracy: 0.9269 - val_loss: 0.2893 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2880 - accuracy: 0.9285 - val_loss: 0.2863 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2846 - accuracy: 0.9291 - val_loss: 0.2823 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2827 - accuracy: 0.9279 - val_loss: 0.2800 - val_accuracy: 0.9246 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2798 - accuracy: 0.9291 - val_loss: 0.2735 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2718 - accuracy: 0.9286 - val_loss: 0.2693 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2629 - accuracy: 0.9332 - val_loss: 0.2675 - val_accuracy: 0.9284 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2600 - accuracy: 0.9350 - val_loss: 0.2662 - val_accuracy: 0.9280 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2648 - accuracy: 0.9308 - val_loss: 0.2682 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2594 - accuracy: 0.9344 - val_loss: 0.2670 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2588 - accuracy: 0.9338 - val_loss: 0.2645 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2578 - accuracy: 0.9343 - val_loss: 0.2667 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2564 - accuracy: 0.9330 - val_loss: 0.2653 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2562 - accuracy: 0.9345 - val_loss: 0.2668 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2539 - accuracy: 0.9346 - val_loss: 0.2647 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2539 - accuracy: 0.9356 - val_loss: 0.2668 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2563 - accuracy: 0.9331 - val_loss: 0.2656 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2516 - accuracy: 0.9347 - val_loss: 0.2661 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2590 - accuracy: 0.9305 - val_loss: 0.2658 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2530 - accuracy: 0.9344 - val_loss: 0.2661 - val_accuracy: 0.9308 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2522 - accuracy: 0.9340 - val_loss: 0.2663 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2569 - accuracy: 0.9332 - val_loss: 0.2658 - val_accuracy: 0.9314 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2549 - accuracy: 0.9341 - val_loss: 0.2662 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2574 - accuracy: 0.9322 - val_loss: 0.2655 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2564 - accuracy: 0.9331 - val_loss: 0.2657 - val_accuracy: 0.9310 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 9ms/step - loss: 0.2850 - accuracy: 0.9267\n",
      "17/17 [==============================] - 2s 36ms/step\n",
      "TP:5868, TN:9836, FP:619, FN:623, loss0.2849666178226471, acc0.9267083677564026, sn0.9040209520875058, sp0.9407938785270206, f10.9042995839112344, auc0.9743358957600198\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 45ms/step - loss: 4.1954 - accuracy: 0.6572 - val_loss: 5.0778 - val_accuracy: 0.3885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 2.9418 - accuracy: 0.8155 - val_loss: 2.8974 - val_accuracy: 0.5552 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 2.0870 - accuracy: 0.8506 - val_loss: 1.8531 - val_accuracy: 0.7844 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.5170 - accuracy: 0.8592 - val_loss: 1.2791 - val_accuracy: 0.8751 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.1431 - accuracy: 0.8677 - val_loss: 1.0054 - val_accuracy: 0.8746 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.9105 - accuracy: 0.8768 - val_loss: 0.8579 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.7574 - accuracy: 0.8796 - val_loss: 0.8043 - val_accuracy: 0.8470 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6610 - accuracy: 0.8866 - val_loss: 0.7967 - val_accuracy: 0.8221 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5804 - accuracy: 0.8947 - val_loss: 0.7690 - val_accuracy: 0.8309 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5454 - accuracy: 0.8925 - val_loss: 0.5236 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5045 - accuracy: 0.9000 - val_loss: 0.5453 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4762 - accuracy: 0.9032 - val_loss: 0.5040 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4638 - accuracy: 0.9020 - val_loss: 0.4379 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4493 - accuracy: 0.9014 - val_loss: 0.4288 - val_accuracy: 0.9093 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4408 - accuracy: 0.9017 - val_loss: 0.4479 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4286 - accuracy: 0.9062 - val_loss: 0.4223 - val_accuracy: 0.9051 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4304 - accuracy: 0.9061 - val_loss: 0.4501 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4186 - accuracy: 0.9053 - val_loss: 0.3989 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4132 - accuracy: 0.9055 - val_loss: 0.3835 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4186 - accuracy: 0.9002 - val_loss: 0.4020 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4116 - accuracy: 0.9048 - val_loss: 0.4278 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4103 - accuracy: 0.9022 - val_loss: 0.4429 - val_accuracy: 0.8893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3967 - accuracy: 0.9095 - val_loss: 0.4540 - val_accuracy: 0.8857 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4024 - accuracy: 0.9064 - val_loss: 0.3745 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3851 - accuracy: 0.9095 - val_loss: 0.4110 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3999 - accuracy: 0.9062 - val_loss: 0.4205 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4018 - accuracy: 0.9068 - val_loss: 0.3627 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3891 - accuracy: 0.9091 - val_loss: 0.3552 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3947 - accuracy: 0.9086 - val_loss: 0.5774 - val_accuracy: 0.8319 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3989 - accuracy: 0.9067 - val_loss: 0.3745 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3890 - accuracy: 0.9067 - val_loss: 0.3613 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3955 - accuracy: 0.9042 - val_loss: 0.3629 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3815 - accuracy: 0.9078 - val_loss: 0.3938 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3858 - accuracy: 0.9079 - val_loss: 0.3573 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3926 - accuracy: 0.9095 - val_loss: 0.4586 - val_accuracy: 0.8867 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3938 - accuracy: 0.9089 - val_loss: 0.3633 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3808 - accuracy: 0.9121 - val_loss: 0.3654 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3817 - accuracy: 0.9097 - val_loss: 0.3800 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3935 - accuracy: 0.9071 - val_loss: 0.3703 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3873 - accuracy: 0.9118 - val_loss: 0.3837 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3893 - accuracy: 0.9123 - val_loss: 0.3815 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3899 - accuracy: 0.9124 - val_loss: 0.4020 - val_accuracy: 0.9021 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3898 - accuracy: 0.9065 - val_loss: 0.3618 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3781 - accuracy: 0.9100 - val_loss: 0.4262 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3790 - accuracy: 0.9115 - val_loss: 0.3561 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3851 - accuracy: 0.9106 - val_loss: 0.3696 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3972 - accuracy: 0.9092 - val_loss: 0.3791 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3810 - accuracy: 0.9115 - val_loss: 0.4859 - val_accuracy: 0.8722 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3827 - accuracy: 0.9104 - val_loss: 0.3924 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3749 - accuracy: 0.9115 - val_loss: 0.3686 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3774 - accuracy: 0.9087 - val_loss: 0.3632 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3706 - accuracy: 0.9144 - val_loss: 0.3669 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3817 - accuracy: 0.9105 - val_loss: 0.4050 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3787 - accuracy: 0.9132 - val_loss: 0.5512 - val_accuracy: 0.8357 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3837 - accuracy: 0.9086 - val_loss: 0.3557 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3814 - accuracy: 0.9068 - val_loss: 0.3962 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3794 - accuracy: 0.9099 - val_loss: 0.3562 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3754 - accuracy: 0.9120 - val_loss: 0.4356 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3696 - accuracy: 0.9132 - val_loss: 0.3704 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3732 - accuracy: 0.9126 - val_loss: 0.3820 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3808 - accuracy: 0.9141 - val_loss: 0.4608 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3672 - accuracy: 0.9153 - val_loss: 0.3572 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3673 - accuracy: 0.9138 - val_loss: 0.3399 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3715 - accuracy: 0.9110 - val_loss: 0.3711 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3691 - accuracy: 0.9142 - val_loss: 0.4004 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3769 - accuracy: 0.9147 - val_loss: 0.3736 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3781 - accuracy: 0.9132 - val_loss: 0.4176 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3800 - accuracy: 0.9116 - val_loss: 0.4239 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3794 - accuracy: 0.9097 - val_loss: 0.3526 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3721 - accuracy: 0.9108 - val_loss: 0.3467 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3670 - accuracy: 0.9132 - val_loss: 0.3476 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3438 - accuracy: 0.9216 - val_loss: 0.3227 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3207 - accuracy: 0.9279 - val_loss: 0.3161 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3134 - accuracy: 0.9271 - val_loss: 0.3032 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3058 - accuracy: 0.9279 - val_loss: 0.2939 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2971 - accuracy: 0.9265 - val_loss: 0.2887 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2884 - accuracy: 0.9304 - val_loss: 0.2831 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2844 - accuracy: 0.9278 - val_loss: 0.2755 - val_accuracy: 0.9258 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2798 - accuracy: 0.9312 - val_loss: 0.2819 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2706 - accuracy: 0.9302 - val_loss: 0.2772 - val_accuracy: 0.9288 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2679 - accuracy: 0.9320 - val_loss: 0.2695 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2649 - accuracy: 0.9306 - val_loss: 0.2617 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2650 - accuracy: 0.9316 - val_loss: 0.2597 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2602 - accuracy: 0.9308 - val_loss: 0.2605 - val_accuracy: 0.9316 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2621 - accuracy: 0.9311 - val_loss: 0.2597 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2570 - accuracy: 0.9328 - val_loss: 0.2594 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2558 - accuracy: 0.9343 - val_loss: 0.2589 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2558 - accuracy: 0.9339 - val_loss: 0.2589 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2567 - accuracy: 0.9331 - val_loss: 0.2585 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2539 - accuracy: 0.9321 - val_loss: 0.2602 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2527 - accuracy: 0.9344 - val_loss: 0.2565 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2548 - accuracy: 0.9355 - val_loss: 0.2575 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2557 - accuracy: 0.9333 - val_loss: 0.2578 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2574 - accuracy: 0.9306 - val_loss: 0.2580 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2553 - accuracy: 0.9350 - val_loss: 0.2581 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2542 - accuracy: 0.9326 - val_loss: 0.2583 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 4s 38ms/step - loss: 0.2535 - accuracy: 0.9326 - val_loss: 0.2580 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2485 - accuracy: 0.9369 - val_loss: 0.2579 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2535 - accuracy: 0.9347 - val_loss: 0.2580 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2573 - accuracy: 0.9324 - val_loss: 0.2578 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2781 - accuracy: 0.9286\n",
      "17/17 [==============================] - 1s 16ms/step\n",
      "TP:5864, TN:9872, FP:583, FN:627, loss0.27814266085624695, acc0.9285967189897321, sn0.9034047142196888, sp0.9442372070779531, f10.9064770443654352, auc0.9747489755340157\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 45ms/step - loss: 4.1509 - accuracy: 0.6505 - val_loss: 3.8350 - val_accuracy: 0.4101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 2.8912 - accuracy: 0.7879 - val_loss: 2.8358 - val_accuracy: 0.5014 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 2.0223 - accuracy: 0.8320 - val_loss: 1.8456 - val_accuracy: 0.7327 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.4471 - accuracy: 0.8528 - val_loss: 1.2430 - val_accuracy: 0.8622 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.0908 - accuracy: 0.8646 - val_loss: 1.0001 - val_accuracy: 0.8526 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.8742 - accuracy: 0.8750 - val_loss: 0.7550 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.7227 - accuracy: 0.8827 - val_loss: 0.6703 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6306 - accuracy: 0.8889 - val_loss: 0.6367 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5693 - accuracy: 0.8967 - val_loss: 0.5196 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5338 - accuracy: 0.8973 - val_loss: 0.5081 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5072 - accuracy: 0.8968 - val_loss: 0.4659 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4913 - accuracy: 0.8959 - val_loss: 0.4968 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4589 - accuracy: 0.9024 - val_loss: 0.6494 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4575 - accuracy: 0.9013 - val_loss: 0.4386 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4457 - accuracy: 0.8994 - val_loss: 0.4209 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4364 - accuracy: 0.9034 - val_loss: 0.4390 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4221 - accuracy: 0.9044 - val_loss: 0.3984 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4133 - accuracy: 0.9075 - val_loss: 0.4468 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4164 - accuracy: 0.9041 - val_loss: 0.4075 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4183 - accuracy: 0.9049 - val_loss: 0.3742 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4074 - accuracy: 0.9062 - val_loss: 0.3806 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4014 - accuracy: 0.9073 - val_loss: 0.3953 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3975 - accuracy: 0.9072 - val_loss: 0.3960 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4051 - accuracy: 0.9060 - val_loss: 0.3939 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3997 - accuracy: 0.9084 - val_loss: 0.3631 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3907 - accuracy: 0.9079 - val_loss: 0.3625 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3988 - accuracy: 0.9065 - val_loss: 0.3690 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3906 - accuracy: 0.9109 - val_loss: 0.3813 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3894 - accuracy: 0.9105 - val_loss: 0.4128 - val_accuracy: 0.8959 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3912 - accuracy: 0.9088 - val_loss: 0.3674 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3937 - accuracy: 0.9061 - val_loss: 0.3736 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3914 - accuracy: 0.9102 - val_loss: 0.3784 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3915 - accuracy: 0.9106 - val_loss: 0.4130 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3901 - accuracy: 0.9110 - val_loss: 0.5406 - val_accuracy: 0.8502 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3868 - accuracy: 0.9129 - val_loss: 0.4474 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3852 - accuracy: 0.9116 - val_loss: 0.3575 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3840 - accuracy: 0.9101 - val_loss: 0.5196 - val_accuracy: 0.8612 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3939 - accuracy: 0.9091 - val_loss: 0.3963 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3842 - accuracy: 0.9089 - val_loss: 0.3555 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3800 - accuracy: 0.9110 - val_loss: 0.3735 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3775 - accuracy: 0.9140 - val_loss: 0.3991 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3874 - accuracy: 0.9089 - val_loss: 0.4279 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3808 - accuracy: 0.9135 - val_loss: 0.3842 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3731 - accuracy: 0.9113 - val_loss: 0.4033 - val_accuracy: 0.8987 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3817 - accuracy: 0.9097 - val_loss: 0.4423 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3763 - accuracy: 0.9094 - val_loss: 0.3813 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3852 - accuracy: 0.9063 - val_loss: 0.3513 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3713 - accuracy: 0.9129 - val_loss: 0.4037 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3851 - accuracy: 0.9080 - val_loss: 0.3767 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3712 - accuracy: 0.9140 - val_loss: 0.3616 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3790 - accuracy: 0.9095 - val_loss: 0.3560 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3787 - accuracy: 0.9108 - val_loss: 0.3993 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3703 - accuracy: 0.9144 - val_loss: 0.3644 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3807 - accuracy: 0.9125 - val_loss: 0.3692 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3799 - accuracy: 0.9142 - val_loss: 0.3576 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3678 - accuracy: 0.9138 - val_loss: 0.3467 - val_accuracy: 0.9226 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3753 - accuracy: 0.9142 - val_loss: 0.3728 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3733 - accuracy: 0.9138 - val_loss: 0.3565 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3702 - accuracy: 0.9120 - val_loss: 0.3487 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3650 - accuracy: 0.9156 - val_loss: 0.3511 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3746 - accuracy: 0.9102 - val_loss: 0.3627 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3782 - accuracy: 0.9136 - val_loss: 0.3657 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3722 - accuracy: 0.9139 - val_loss: 0.4126 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3774 - accuracy: 0.9131 - val_loss: 0.3718 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3668 - accuracy: 0.9132 - val_loss: 0.3545 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3681 - accuracy: 0.9123 - val_loss: 0.3732 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3716 - accuracy: 0.9129 - val_loss: 0.3651 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3758 - accuracy: 0.9109 - val_loss: 0.3874 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3823 - accuracy: 0.9093 - val_loss: 0.3332 - val_accuracy: 0.9276 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3645 - accuracy: 0.9128 - val_loss: 0.3467 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3653 - accuracy: 0.9152 - val_loss: 0.3779 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3473 - accuracy: 0.9217 - val_loss: 0.3289 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3228 - accuracy: 0.9268 - val_loss: 0.3236 - val_accuracy: 0.9232 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3152 - accuracy: 0.9274 - val_loss: 0.3105 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3074 - accuracy: 0.9288 - val_loss: 0.3040 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2968 - accuracy: 0.9272 - val_loss: 0.2955 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2918 - accuracy: 0.9275 - val_loss: 0.2860 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2832 - accuracy: 0.9299 - val_loss: 0.2835 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2818 - accuracy: 0.9277 - val_loss: 0.2948 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2724 - accuracy: 0.9304 - val_loss: 0.2723 - val_accuracy: 0.9288 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2711 - accuracy: 0.9326 - val_loss: 0.2669 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2677 - accuracy: 0.9331 - val_loss: 0.2663 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2598 - accuracy: 0.9336 - val_loss: 0.2660 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2592 - accuracy: 0.9344 - val_loss: 0.2675 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2617 - accuracy: 0.9323 - val_loss: 0.2667 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2606 - accuracy: 0.9327 - val_loss: 0.2675 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2542 - accuracy: 0.9342 - val_loss: 0.2659 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2623 - accuracy: 0.9311 - val_loss: 0.2643 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2570 - accuracy: 0.9322 - val_loss: 0.2658 - val_accuracy: 0.9278 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2586 - accuracy: 0.9312 - val_loss: 0.2686 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2599 - accuracy: 0.9335 - val_loss: 0.2633 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2533 - accuracy: 0.9355 - val_loss: 0.2639 - val_accuracy: 0.9288 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2514 - accuracy: 0.9356 - val_loss: 0.2645 - val_accuracy: 0.9286 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2536 - accuracy: 0.9361 - val_loss: 0.2643 - val_accuracy: 0.9280 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2572 - accuracy: 0.9334 - val_loss: 0.2646 - val_accuracy: 0.9290 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2539 - accuracy: 0.9347 - val_loss: 0.2645 - val_accuracy: 0.9286 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2576 - accuracy: 0.9338 - val_loss: 0.2650 - val_accuracy: 0.9290 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2559 - accuracy: 0.9350 - val_loss: 0.2648 - val_accuracy: 0.9288 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2533 - accuracy: 0.9341 - val_loss: 0.2650 - val_accuracy: 0.9286 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2533 - accuracy: 0.9344 - val_loss: 0.2654 - val_accuracy: 0.9286 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2814 - accuracy: 0.9286\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5899, TN:9837, FP:618, FN:592, loss0.28135478496551514, acc0.9285967189897321, sn0.9087967955630873, sp0.9408895265423243, f10.906980319803198, auc0.9752449276602021\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 46ms/step - loss: 4.1925 - accuracy: 0.6709 - val_loss: 4.9569 - val_accuracy: 0.3905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 3.0041 - accuracy: 0.8108 - val_loss: 2.7644 - val_accuracy: 0.6604 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 2.1484 - accuracy: 0.8422 - val_loss: 1.7888 - val_accuracy: 0.8570 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.5664 - accuracy: 0.8543 - val_loss: 1.3101 - val_accuracy: 0.8726 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.1711 - accuracy: 0.8660 - val_loss: 1.0077 - val_accuracy: 0.8750 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.9149 - accuracy: 0.8779 - val_loss: 0.8593 - val_accuracy: 0.8670 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.7519 - accuracy: 0.8847 - val_loss: 0.6678 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6526 - accuracy: 0.8860 - val_loss: 0.6146 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5782 - accuracy: 0.8932 - val_loss: 0.6525 - val_accuracy: 0.8656 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5289 - accuracy: 0.8991 - val_loss: 0.4875 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4989 - accuracy: 0.8986 - val_loss: 0.4645 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4736 - accuracy: 0.8980 - val_loss: 0.4379 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4526 - accuracy: 0.9036 - val_loss: 0.4089 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4462 - accuracy: 0.9024 - val_loss: 0.4181 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4344 - accuracy: 0.9044 - val_loss: 0.5013 - val_accuracy: 0.8849 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4411 - accuracy: 0.8989 - val_loss: 0.4632 - val_accuracy: 0.8945 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4240 - accuracy: 0.9031 - val_loss: 0.4003 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4129 - accuracy: 0.9043 - val_loss: 0.4782 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4025 - accuracy: 0.9062 - val_loss: 0.4029 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4147 - accuracy: 0.9051 - val_loss: 0.3756 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3996 - accuracy: 0.9096 - val_loss: 0.3925 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4149 - accuracy: 0.9049 - val_loss: 0.3839 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3987 - accuracy: 0.9090 - val_loss: 0.3812 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4016 - accuracy: 0.9079 - val_loss: 0.4101 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4076 - accuracy: 0.9055 - val_loss: 0.3885 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4001 - accuracy: 0.9079 - val_loss: 0.3707 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4023 - accuracy: 0.9059 - val_loss: 0.4416 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4056 - accuracy: 0.9092 - val_loss: 0.3823 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3988 - accuracy: 0.9086 - val_loss: 0.3603 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3928 - accuracy: 0.9082 - val_loss: 0.3752 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3853 - accuracy: 0.9105 - val_loss: 0.4040 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3973 - accuracy: 0.9067 - val_loss: 0.3849 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3830 - accuracy: 0.9102 - val_loss: 0.4278 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3863 - accuracy: 0.9081 - val_loss: 0.3612 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3848 - accuracy: 0.9126 - val_loss: 0.3675 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4006 - accuracy: 0.9070 - val_loss: 0.4063 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3999 - accuracy: 0.9055 - val_loss: 0.3791 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3933 - accuracy: 0.9073 - val_loss: 0.3870 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3856 - accuracy: 0.9102 - val_loss: 0.4210 - val_accuracy: 0.9039 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3900 - accuracy: 0.9095 - val_loss: 0.3840 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3817 - accuracy: 0.9124 - val_loss: 0.4486 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3858 - accuracy: 0.9120 - val_loss: 0.3521 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3858 - accuracy: 0.9104 - val_loss: 0.3694 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3954 - accuracy: 0.9073 - val_loss: 0.5292 - val_accuracy: 0.8468 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3837 - accuracy: 0.9114 - val_loss: 0.3530 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3800 - accuracy: 0.9129 - val_loss: 0.3663 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3751 - accuracy: 0.9128 - val_loss: 0.3620 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3783 - accuracy: 0.9134 - val_loss: 0.3894 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3754 - accuracy: 0.9113 - val_loss: 0.3520 - val_accuracy: 0.9230 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3787 - accuracy: 0.9102 - val_loss: 0.3794 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3796 - accuracy: 0.9093 - val_loss: 0.4765 - val_accuracy: 0.8730 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3876 - accuracy: 0.9129 - val_loss: 0.4049 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3848 - accuracy: 0.9105 - val_loss: 0.3661 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3791 - accuracy: 0.9087 - val_loss: 0.3836 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3729 - accuracy: 0.9130 - val_loss: 0.3689 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3784 - accuracy: 0.9119 - val_loss: 0.3734 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3733 - accuracy: 0.9103 - val_loss: 0.3515 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3722 - accuracy: 0.9117 - val_loss: 0.4026 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3690 - accuracy: 0.9126 - val_loss: 0.3551 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3862 - accuracy: 0.9097 - val_loss: 0.3769 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3761 - accuracy: 0.9117 - val_loss: 0.3394 - val_accuracy: 0.9258 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3762 - accuracy: 0.9128 - val_loss: 0.3901 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3762 - accuracy: 0.9126 - val_loss: 0.3515 - val_accuracy: 0.9228 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3717 - accuracy: 0.9153 - val_loss: 0.4506 - val_accuracy: 0.8771 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3748 - accuracy: 0.9132 - val_loss: 0.3723 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3758 - accuracy: 0.9138 - val_loss: 0.3455 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3805 - accuracy: 0.9105 - val_loss: 0.3717 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.3684 - accuracy: 0.9150 - val_loss: 0.3661 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3634 - accuracy: 0.9126 - val_loss: 0.4166 - val_accuracy: 0.8951 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3653 - accuracy: 0.9142 - val_loss: 0.3976 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3739 - accuracy: 0.9152 - val_loss: 0.3624 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3387 - accuracy: 0.9246 - val_loss: 0.3272 - val_accuracy: 0.9244 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3281 - accuracy: 0.9261 - val_loss: 0.3248 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3185 - accuracy: 0.9241 - val_loss: 0.3081 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3074 - accuracy: 0.9285 - val_loss: 0.3014 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3014 - accuracy: 0.9266 - val_loss: 0.2905 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2913 - accuracy: 0.9284 - val_loss: 0.2937 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2887 - accuracy: 0.9286 - val_loss: 0.2792 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2816 - accuracy: 0.9286 - val_loss: 0.2756 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2754 - accuracy: 0.9324 - val_loss: 0.2701 - val_accuracy: 0.9290 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2690 - accuracy: 0.9315 - val_loss: 0.2793 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2657 - accuracy: 0.9329 - val_loss: 0.2633 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2628 - accuracy: 0.9329 - val_loss: 0.2638 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2619 - accuracy: 0.9334 - val_loss: 0.2657 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2625 - accuracy: 0.9327 - val_loss: 0.2648 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2618 - accuracy: 0.9321 - val_loss: 0.2623 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2578 - accuracy: 0.9344 - val_loss: 0.2628 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2598 - accuracy: 0.9308 - val_loss: 0.2613 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2556 - accuracy: 0.9351 - val_loss: 0.2610 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2552 - accuracy: 0.9345 - val_loss: 0.2618 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2546 - accuracy: 0.9332 - val_loss: 0.2599 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2564 - accuracy: 0.9317 - val_loss: 0.2613 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2594 - accuracy: 0.9319 - val_loss: 0.2615 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2578 - accuracy: 0.9331 - val_loss: 0.2616 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2574 - accuracy: 0.9327 - val_loss: 0.2620 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2618 - accuracy: 0.9314 - val_loss: 0.2611 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2562 - accuracy: 0.9327 - val_loss: 0.2613 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2591 - accuracy: 0.9325 - val_loss: 0.2612 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2563 - accuracy: 0.9349 - val_loss: 0.2616 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2554 - accuracy: 0.9330 - val_loss: 0.2616 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2821 - accuracy: 0.9273\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5891, TN:9822, FP:633, FN:600, loss0.28207075595855713, acc0.9272394665407766, sn0.9075643198274534, sp0.939454806312769, f10.9052631578947369, auc0.9750562840164003\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 43ms/step - loss: 4.1901 - accuracy: 0.6430 - val_loss: 4.2081 - val_accuracy: 0.4322 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 2.9322 - accuracy: 0.7964 - val_loss: 3.2579 - val_accuracy: 0.3995 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 2.0523 - accuracy: 0.8351 - val_loss: 1.9276 - val_accuracy: 0.6978 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.4921 - accuracy: 0.8539 - val_loss: 1.2773 - val_accuracy: 0.8598 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.1309 - accuracy: 0.8602 - val_loss: 0.9679 - val_accuracy: 0.8829 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.8878 - accuracy: 0.8779 - val_loss: 0.7752 - val_accuracy: 0.8899 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.7445 - accuracy: 0.8776 - val_loss: 0.6896 - val_accuracy: 0.8815 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6427 - accuracy: 0.8849 - val_loss: 0.5829 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5786 - accuracy: 0.8933 - val_loss: 0.5496 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5334 - accuracy: 0.8996 - val_loss: 0.5489 - val_accuracy: 0.8927 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5058 - accuracy: 0.8985 - val_loss: 0.4595 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4847 - accuracy: 0.8980 - val_loss: 0.5346 - val_accuracy: 0.8779 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4625 - accuracy: 0.9008 - val_loss: 0.4505 - val_accuracy: 0.9047 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4479 - accuracy: 0.9023 - val_loss: 0.4710 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4394 - accuracy: 0.9054 - val_loss: 0.3991 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4280 - accuracy: 0.9044 - val_loss: 0.4581 - val_accuracy: 0.8939 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4261 - accuracy: 0.9038 - val_loss: 0.4982 - val_accuracy: 0.8714 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4214 - accuracy: 0.9064 - val_loss: 0.3814 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4189 - accuracy: 0.9030 - val_loss: 0.6069 - val_accuracy: 0.8091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4102 - accuracy: 0.9068 - val_loss: 0.3977 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4051 - accuracy: 0.9071 - val_loss: 0.3862 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3904 - accuracy: 0.9103 - val_loss: 0.5625 - val_accuracy: 0.8398 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4070 - accuracy: 0.9073 - val_loss: 0.3688 - val_accuracy: 0.9208 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4037 - accuracy: 0.9048 - val_loss: 0.3708 - val_accuracy: 0.9228 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3937 - accuracy: 0.9100 - val_loss: 0.3995 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3983 - accuracy: 0.9077 - val_loss: 0.3894 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3928 - accuracy: 0.9063 - val_loss: 0.4097 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3831 - accuracy: 0.9097 - val_loss: 0.3747 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4033 - accuracy: 0.9054 - val_loss: 0.3593 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3960 - accuracy: 0.9060 - val_loss: 0.4743 - val_accuracy: 0.8670 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3826 - accuracy: 0.9079 - val_loss: 0.4946 - val_accuracy: 0.8544 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3794 - accuracy: 0.9094 - val_loss: 0.3607 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3796 - accuracy: 0.9119 - val_loss: 0.3923 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3871 - accuracy: 0.9091 - val_loss: 0.4140 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3814 - accuracy: 0.9089 - val_loss: 0.5250 - val_accuracy: 0.8335 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3777 - accuracy: 0.9102 - val_loss: 0.3721 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3781 - accuracy: 0.9102 - val_loss: 0.3505 - val_accuracy: 0.9234 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3898 - accuracy: 0.9070 - val_loss: 0.4395 - val_accuracy: 0.9009 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3823 - accuracy: 0.9108 - val_loss: 0.4251 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3769 - accuracy: 0.9108 - val_loss: 0.3750 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3824 - accuracy: 0.9102 - val_loss: 0.5099 - val_accuracy: 0.8592 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3824 - accuracy: 0.9078 - val_loss: 0.3664 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3769 - accuracy: 0.9136 - val_loss: 0.3701 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3907 - accuracy: 0.9076 - val_loss: 0.3594 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3800 - accuracy: 0.9094 - val_loss: 0.3683 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3790 - accuracy: 0.9130 - val_loss: 0.4017 - val_accuracy: 0.9071 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3898 - accuracy: 0.9090 - val_loss: 0.4717 - val_accuracy: 0.8839 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3735 - accuracy: 0.9121 - val_loss: 0.3974 - val_accuracy: 0.9015 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3792 - accuracy: 0.9127 - val_loss: 0.3592 - val_accuracy: 0.9238 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3876 - accuracy: 0.9090 - val_loss: 0.3569 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3743 - accuracy: 0.9148 - val_loss: 0.3619 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3712 - accuracy: 0.9141 - val_loss: 0.3510 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3788 - accuracy: 0.9111 - val_loss: 0.3613 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3756 - accuracy: 0.9138 - val_loss: 0.4050 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3763 - accuracy: 0.9100 - val_loss: 0.4217 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3691 - accuracy: 0.9121 - val_loss: 0.3497 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3593 - accuracy: 0.9173 - val_loss: 0.3507 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3698 - accuracy: 0.9126 - val_loss: 0.3776 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3717 - accuracy: 0.9147 - val_loss: 0.3700 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3706 - accuracy: 0.9126 - val_loss: 0.4112 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3736 - accuracy: 0.9119 - val_loss: 0.4197 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3763 - accuracy: 0.9110 - val_loss: 0.3599 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3705 - accuracy: 0.9153 - val_loss: 0.3731 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3720 - accuracy: 0.9104 - val_loss: 0.3622 - val_accuracy: 0.9158 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3688 - accuracy: 0.9114 - val_loss: 0.3535 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3615 - accuracy: 0.9156 - val_loss: 0.3835 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3758 - accuracy: 0.9136 - val_loss: 0.3615 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3665 - accuracy: 0.9150 - val_loss: 0.3487 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3720 - accuracy: 0.9140 - val_loss: 0.4010 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3713 - accuracy: 0.9122 - val_loss: 0.3757 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3756 - accuracy: 0.9108 - val_loss: 0.3423 - val_accuracy: 0.9246 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3451 - accuracy: 0.9226 - val_loss: 0.3271 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3263 - accuracy: 0.9257 - val_loss: 0.3168 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3161 - accuracy: 0.9256 - val_loss: 0.3071 - val_accuracy: 0.9272 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3070 - accuracy: 0.9285 - val_loss: 0.3102 - val_accuracy: 0.9194 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2973 - accuracy: 0.9283 - val_loss: 0.2899 - val_accuracy: 0.9288 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2908 - accuracy: 0.9286 - val_loss: 0.2822 - val_accuracy: 0.9302 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2856 - accuracy: 0.9295 - val_loss: 0.2795 - val_accuracy: 0.9282 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.2777 - accuracy: 0.9301 - val_loss: 0.2749 - val_accuracy: 0.9298 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2734 - accuracy: 0.9310 - val_loss: 0.2700 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2693 - accuracy: 0.9328 - val_loss: 0.2666 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2615 - accuracy: 0.9342 - val_loss: 0.2629 - val_accuracy: 0.9288 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2644 - accuracy: 0.9344 - val_loss: 0.2633 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2606 - accuracy: 0.9320 - val_loss: 0.2644 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2585 - accuracy: 0.9338 - val_loss: 0.2647 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2579 - accuracy: 0.9313 - val_loss: 0.2635 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2571 - accuracy: 0.9333 - val_loss: 0.2652 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2599 - accuracy: 0.9329 - val_loss: 0.2644 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2613 - accuracy: 0.9315 - val_loss: 0.2622 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2548 - accuracy: 0.9356 - val_loss: 0.2625 - val_accuracy: 0.9320 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2542 - accuracy: 0.9347 - val_loss: 0.2621 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2573 - accuracy: 0.9337 - val_loss: 0.2617 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2523 - accuracy: 0.9330 - val_loss: 0.2616 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2563 - accuracy: 0.9365 - val_loss: 0.2617 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2545 - accuracy: 0.9340 - val_loss: 0.2618 - val_accuracy: 0.9318 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2527 - accuracy: 0.9349 - val_loss: 0.2615 - val_accuracy: 0.9322 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2528 - accuracy: 0.9350 - val_loss: 0.2616 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2546 - accuracy: 0.9338 - val_loss: 0.2617 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2509 - accuracy: 0.9341 - val_loss: 0.2619 - val_accuracy: 0.9320 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2502 - accuracy: 0.9347 - val_loss: 0.2621 - val_accuracy: 0.9316 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2791 - accuracy: 0.9291\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5884, TN:9860, FP:595, FN:607, loss0.2790820002555847, acc0.9290688067980645, sn0.9064859035587737, sp0.943089430894309, f10.9073245952197379, auc0.9751197423707224\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 41ms/step - loss: 4.2731 - accuracy: 0.6409 - val_loss: 4.2084 - val_accuracy: 0.4180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 3.0635 - accuracy: 0.7980 - val_loss: 2.8148 - val_accuracy: 0.5832 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 2.2059 - accuracy: 0.8400 - val_loss: 1.9334 - val_accuracy: 0.8113 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 1.6171 - accuracy: 0.8542 - val_loss: 1.3873 - val_accuracy: 0.8560 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.2312 - accuracy: 0.8633 - val_loss: 1.0356 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.9553 - accuracy: 0.8775 - val_loss: 0.8628 - val_accuracy: 0.8748 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.7853 - accuracy: 0.8858 - val_loss: 0.7948 - val_accuracy: 0.8620 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.6832 - accuracy: 0.8848 - val_loss: 0.6332 - val_accuracy: 0.8935 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.6069 - accuracy: 0.8878 - val_loss: 0.5735 - val_accuracy: 0.8943 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5488 - accuracy: 0.8943 - val_loss: 0.5225 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5046 - accuracy: 0.9014 - val_loss: 0.4926 - val_accuracy: 0.8985 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4836 - accuracy: 0.9012 - val_loss: 0.4571 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4672 - accuracy: 0.9008 - val_loss: 0.4488 - val_accuracy: 0.9075 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4594 - accuracy: 0.9011 - val_loss: 0.5144 - val_accuracy: 0.8759 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4410 - accuracy: 0.9017 - val_loss: 0.4058 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.4270 - accuracy: 0.9039 - val_loss: 0.4025 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4173 - accuracy: 0.9061 - val_loss: 0.3982 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4186 - accuracy: 0.9044 - val_loss: 0.4069 - val_accuracy: 0.9150 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4112 - accuracy: 0.9066 - val_loss: 0.4756 - val_accuracy: 0.8809 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4078 - accuracy: 0.9066 - val_loss: 0.4167 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4092 - accuracy: 0.9074 - val_loss: 0.3677 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4084 - accuracy: 0.9020 - val_loss: 0.3991 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3955 - accuracy: 0.9077 - val_loss: 0.4005 - val_accuracy: 0.9057 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4021 - accuracy: 0.9059 - val_loss: 0.4248 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3949 - accuracy: 0.9079 - val_loss: 0.4539 - val_accuracy: 0.8873 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3962 - accuracy: 0.9072 - val_loss: 0.4301 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3958 - accuracy: 0.9060 - val_loss: 0.5242 - val_accuracy: 0.8508 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3959 - accuracy: 0.9079 - val_loss: 0.4261 - val_accuracy: 0.9029 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3994 - accuracy: 0.9061 - val_loss: 0.3739 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3871 - accuracy: 0.9088 - val_loss: 0.3918 - val_accuracy: 0.9063 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3855 - accuracy: 0.9072 - val_loss: 0.4268 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3863 - accuracy: 0.9113 - val_loss: 0.3660 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3885 - accuracy: 0.9090 - val_loss: 0.3593 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3833 - accuracy: 0.9114 - val_loss: 0.3964 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3805 - accuracy: 0.9108 - val_loss: 0.4353 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3955 - accuracy: 0.9073 - val_loss: 0.3936 - val_accuracy: 0.9087 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3953 - accuracy: 0.9095 - val_loss: 0.4174 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3920 - accuracy: 0.9127 - val_loss: 0.3766 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3871 - accuracy: 0.9111 - val_loss: 0.3889 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3902 - accuracy: 0.9124 - val_loss: 0.4266 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3800 - accuracy: 0.9126 - val_loss: 0.3659 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3825 - accuracy: 0.9121 - val_loss: 0.4471 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3798 - accuracy: 0.9146 - val_loss: 0.4102 - val_accuracy: 0.9025 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3835 - accuracy: 0.9085 - val_loss: 0.3694 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3760 - accuracy: 0.9134 - val_loss: 0.4076 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3834 - accuracy: 0.9095 - val_loss: 0.3689 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3801 - accuracy: 0.9127 - val_loss: 0.4037 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3765 - accuracy: 0.9106 - val_loss: 0.3828 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3727 - accuracy: 0.9105 - val_loss: 0.3696 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3798 - accuracy: 0.9138 - val_loss: 0.3589 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3878 - accuracy: 0.9089 - val_loss: 0.3691 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3814 - accuracy: 0.9129 - val_loss: 0.3736 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3786 - accuracy: 0.9133 - val_loss: 0.3523 - val_accuracy: 0.9240 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3770 - accuracy: 0.9137 - val_loss: 0.3611 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3801 - accuracy: 0.9127 - val_loss: 0.3818 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3821 - accuracy: 0.9126 - val_loss: 0.4118 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3749 - accuracy: 0.9130 - val_loss: 0.4264 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 0.3743 - accuracy: 0.9145 - val_loss: 0.4811 - val_accuracy: 0.8710 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3746 - accuracy: 0.9123 - val_loss: 0.4304 - val_accuracy: 0.8919 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3840 - accuracy: 0.9101 - val_loss: 0.4827 - val_accuracy: 0.8825 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3759 - accuracy: 0.9120 - val_loss: 0.4269 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3695 - accuracy: 0.9100 - val_loss: 0.3581 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3723 - accuracy: 0.9107 - val_loss: 0.3718 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3682 - accuracy: 0.9138 - val_loss: 0.3864 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3685 - accuracy: 0.9138 - val_loss: 0.3576 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3711 - accuracy: 0.9149 - val_loss: 0.3779 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3748 - accuracy: 0.9110 - val_loss: 0.4079 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3811 - accuracy: 0.9097 - val_loss: 0.3578 - val_accuracy: 0.9130 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3629 - accuracy: 0.9142 - val_loss: 0.3497 - val_accuracy: 0.9228 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3786 - accuracy: 0.9132 - val_loss: 0.3568 - val_accuracy: 0.9228 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3735 - accuracy: 0.9152 - val_loss: 0.5279 - val_accuracy: 0.8325 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3509 - accuracy: 0.9210 - val_loss: 0.3309 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3264 - accuracy: 0.9246 - val_loss: 0.3290 - val_accuracy: 0.9222 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3122 - accuracy: 0.9299 - val_loss: 0.3118 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3081 - accuracy: 0.9277 - val_loss: 0.3165 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3018 - accuracy: 0.9273 - val_loss: 0.2952 - val_accuracy: 0.9242 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2952 - accuracy: 0.9260 - val_loss: 0.2950 - val_accuracy: 0.9250 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2823 - accuracy: 0.9297 - val_loss: 0.2901 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2792 - accuracy: 0.9270 - val_loss: 0.2866 - val_accuracy: 0.9264 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2794 - accuracy: 0.9281 - val_loss: 0.2728 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2720 - accuracy: 0.9308 - val_loss: 0.2656 - val_accuracy: 0.9308 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2670 - accuracy: 0.9314 - val_loss: 0.2654 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2664 - accuracy: 0.9296 - val_loss: 0.2663 - val_accuracy: 0.9286 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2597 - accuracy: 0.9353 - val_loss: 0.2668 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2577 - accuracy: 0.9345 - val_loss: 0.2639 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2553 - accuracy: 0.9343 - val_loss: 0.2649 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2570 - accuracy: 0.9322 - val_loss: 0.2628 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2596 - accuracy: 0.9346 - val_loss: 0.2651 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2563 - accuracy: 0.9342 - val_loss: 0.2644 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2530 - accuracy: 0.9357 - val_loss: 0.2645 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2558 - accuracy: 0.9341 - val_loss: 0.2642 - val_accuracy: 0.9304 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2580 - accuracy: 0.9347 - val_loss: 0.2645 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2560 - accuracy: 0.9334 - val_loss: 0.2644 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2536 - accuracy: 0.9354 - val_loss: 0.2642 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2539 - accuracy: 0.9352 - val_loss: 0.2640 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2543 - accuracy: 0.9326 - val_loss: 0.2644 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2562 - accuracy: 0.9331 - val_loss: 0.2642 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2552 - accuracy: 0.9355 - val_loss: 0.2640 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2537 - accuracy: 0.9367 - val_loss: 0.2639 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2529 - accuracy: 0.9336 - val_loss: 0.2638 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2778 - accuracy: 0.9300\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5847, TN:9913, FP:542, FN:644, loss0.27776777744293213, acc0.9300129824147292, sn0.9007857032814667, sp0.9481587757054041, f10.9079192546583852, auc0.975320365961596\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 24s 43ms/step - loss: 4.2444 - accuracy: 0.6436 - val_loss: 4.7471 - val_accuracy: 0.3893 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 3.0279 - accuracy: 0.8078 - val_loss: 3.0959 - val_accuracy: 0.4392 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 2.1789 - accuracy: 0.8445 - val_loss: 1.8747 - val_accuracy: 0.8410 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.5991 - accuracy: 0.8608 - val_loss: 1.3397 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 1.2025 - accuracy: 0.8725 - val_loss: 1.0383 - val_accuracy: 0.8781 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.9453 - accuracy: 0.8839 - val_loss: 0.9987 - val_accuracy: 0.8371 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.7778 - accuracy: 0.8916 - val_loss: 0.7794 - val_accuracy: 0.8728 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6728 - accuracy: 0.8924 - val_loss: 0.8012 - val_accuracy: 0.8357 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6022 - accuracy: 0.8943 - val_loss: 0.6885 - val_accuracy: 0.8650 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5440 - accuracy: 0.9005 - val_loss: 0.5159 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5046 - accuracy: 0.9039 - val_loss: 0.4810 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4902 - accuracy: 0.9023 - val_loss: 0.5413 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4769 - accuracy: 0.9038 - val_loss: 0.4597 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4653 - accuracy: 0.9003 - val_loss: 0.4492 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4447 - accuracy: 0.9066 - val_loss: 0.4228 - val_accuracy: 0.9136 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4324 - accuracy: 0.9041 - val_loss: 0.4013 - val_accuracy: 0.9162 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4346 - accuracy: 0.9042 - val_loss: 0.4406 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4276 - accuracy: 0.9035 - val_loss: 0.4175 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4145 - accuracy: 0.9084 - val_loss: 0.3958 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4117 - accuracy: 0.9038 - val_loss: 0.3851 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4085 - accuracy: 0.9069 - val_loss: 0.4367 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4038 - accuracy: 0.9101 - val_loss: 0.3999 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4078 - accuracy: 0.9054 - val_loss: 0.3872 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3985 - accuracy: 0.9084 - val_loss: 0.4107 - val_accuracy: 0.9073 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4066 - accuracy: 0.9047 - val_loss: 0.3932 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3963 - accuracy: 0.9091 - val_loss: 0.3779 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3950 - accuracy: 0.9066 - val_loss: 0.3724 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3958 - accuracy: 0.9098 - val_loss: 0.3747 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3967 - accuracy: 0.9069 - val_loss: 0.3594 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3984 - accuracy: 0.9085 - val_loss: 0.3726 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3969 - accuracy: 0.9074 - val_loss: 0.3885 - val_accuracy: 0.9110 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3909 - accuracy: 0.9116 - val_loss: 0.3954 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3916 - accuracy: 0.9084 - val_loss: 0.3846 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3920 - accuracy: 0.9065 - val_loss: 0.3866 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3778 - accuracy: 0.9115 - val_loss: 0.3624 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3959 - accuracy: 0.9061 - val_loss: 0.4073 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3848 - accuracy: 0.9116 - val_loss: 0.3689 - val_accuracy: 0.9124 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3881 - accuracy: 0.9111 - val_loss: 0.3753 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3880 - accuracy: 0.9097 - val_loss: 0.4131 - val_accuracy: 0.9017 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3898 - accuracy: 0.9111 - val_loss: 0.3663 - val_accuracy: 0.9210 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3978 - accuracy: 0.9069 - val_loss: 0.3893 - val_accuracy: 0.9103 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3905 - accuracy: 0.9085 - val_loss: 0.3523 - val_accuracy: 0.9242 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3814 - accuracy: 0.9114 - val_loss: 0.3724 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3975 - accuracy: 0.9085 - val_loss: 0.3664 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3769 - accuracy: 0.9138 - val_loss: 0.3656 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3925 - accuracy: 0.9094 - val_loss: 0.3946 - val_accuracy: 0.9134 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3836 - accuracy: 0.9138 - val_loss: 0.3618 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3835 - accuracy: 0.9114 - val_loss: 0.3637 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3755 - accuracy: 0.9115 - val_loss: 0.3543 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3872 - accuracy: 0.9084 - val_loss: 0.5048 - val_accuracy: 0.8636 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3846 - accuracy: 0.9067 - val_loss: 0.3688 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3737 - accuracy: 0.9154 - val_loss: 0.3867 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3776 - accuracy: 0.9111 - val_loss: 0.3820 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3738 - accuracy: 0.9120 - val_loss: 0.3895 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3822 - accuracy: 0.9094 - val_loss: 0.3680 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3716 - accuracy: 0.9136 - val_loss: 0.5001 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3753 - accuracy: 0.9157 - val_loss: 0.3771 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3762 - accuracy: 0.9125 - val_loss: 0.4121 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3815 - accuracy: 0.9087 - val_loss: 0.3607 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3610 - accuracy: 0.9169 - val_loss: 0.3437 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3734 - accuracy: 0.9124 - val_loss: 0.5319 - val_accuracy: 0.8540 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3776 - accuracy: 0.9118 - val_loss: 0.4589 - val_accuracy: 0.8797 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3698 - accuracy: 0.9167 - val_loss: 0.3616 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3695 - accuracy: 0.9152 - val_loss: 0.3732 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3753 - accuracy: 0.9107 - val_loss: 0.3462 - val_accuracy: 0.9250 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3708 - accuracy: 0.9149 - val_loss: 0.4552 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3775 - accuracy: 0.9104 - val_loss: 0.3426 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3720 - accuracy: 0.9108 - val_loss: 0.4036 - val_accuracy: 0.8905 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3713 - accuracy: 0.9112 - val_loss: 0.3539 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3630 - accuracy: 0.9122 - val_loss: 0.4106 - val_accuracy: 0.8921 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3712 - accuracy: 0.9126 - val_loss: 0.3518 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3384 - accuracy: 0.9208 - val_loss: 0.3179 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3212 - accuracy: 0.9270 - val_loss: 0.3115 - val_accuracy: 0.9256 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3106 - accuracy: 0.9230 - val_loss: 0.3048 - val_accuracy: 0.9254 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3037 - accuracy: 0.9267 - val_loss: 0.2964 - val_accuracy: 0.9240 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2933 - accuracy: 0.9269 - val_loss: 0.3009 - val_accuracy: 0.9202 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2878 - accuracy: 0.9270 - val_loss: 0.2868 - val_accuracy: 0.9238 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2820 - accuracy: 0.9282 - val_loss: 0.2832 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2762 - accuracy: 0.9293 - val_loss: 0.2707 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2750 - accuracy: 0.9292 - val_loss: 0.2694 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2670 - accuracy: 0.9297 - val_loss: 0.2663 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2594 - accuracy: 0.9337 - val_loss: 0.2628 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2594 - accuracy: 0.9330 - val_loss: 0.2622 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2595 - accuracy: 0.9320 - val_loss: 0.2604 - val_accuracy: 0.9294 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2576 - accuracy: 0.9350 - val_loss: 0.2620 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2536 - accuracy: 0.9318 - val_loss: 0.2629 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2535 - accuracy: 0.9358 - val_loss: 0.2617 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2550 - accuracy: 0.9325 - val_loss: 0.2606 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2565 - accuracy: 0.9314 - val_loss: 0.2615 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2511 - accuracy: 0.9359 - val_loss: 0.2623 - val_accuracy: 0.9292 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2486 - accuracy: 0.9360 - val_loss: 0.2607 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2517 - accuracy: 0.9338 - val_loss: 0.2617 - val_accuracy: 0.9294 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2481 - accuracy: 0.9362 - val_loss: 0.2621 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2503 - accuracy: 0.9338 - val_loss: 0.2624 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2471 - accuracy: 0.9357 - val_loss: 0.2622 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2515 - accuracy: 0.9321 - val_loss: 0.2615 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2483 - accuracy: 0.9338 - val_loss: 0.2616 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2512 - accuracy: 0.9327 - val_loss: 0.2613 - val_accuracy: 0.9294 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2483 - accuracy: 0.9358 - val_loss: 0.2614 - val_accuracy: 0.9294 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2538 - accuracy: 0.9323 - val_loss: 0.2617 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 5s 8ms/step - loss: 0.2807 - accuracy: 0.9269\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5898, TN:9810, FP:645, FN:593, loss0.2806643545627594, acc0.9269444116605688, sn0.9086427360961331, sp0.9383070301291249, f10.9050176461562068, auc0.975013094317917\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 43ms/step - loss: 4.3287 - accuracy: 0.6294 - val_loss: 4.2481 - val_accuracy: 0.4049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 3.1357 - accuracy: 0.7982 - val_loss: 2.7471 - val_accuracy: 0.7629 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 2.3205 - accuracy: 0.8255 - val_loss: 2.0215 - val_accuracy: 0.8113 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.6868 - accuracy: 0.8588 - val_loss: 1.6116 - val_accuracy: 0.7988 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.2791 - accuracy: 0.8669 - val_loss: 1.1594 - val_accuracy: 0.8642 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.9970 - accuracy: 0.8790 - val_loss: 1.1062 - val_accuracy: 0.8263 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.8248 - accuracy: 0.8814 - val_loss: 0.8571 - val_accuracy: 0.8618 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.6993 - accuracy: 0.8920 - val_loss: 0.7498 - val_accuracy: 0.8712 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6278 - accuracy: 0.8936 - val_loss: 0.6366 - val_accuracy: 0.8923 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5646 - accuracy: 0.8967 - val_loss: 0.5122 - val_accuracy: 0.9128 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5214 - accuracy: 0.8997 - val_loss: 0.5191 - val_accuracy: 0.8997 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.5006 - accuracy: 0.9014 - val_loss: 0.4582 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4778 - accuracy: 0.9007 - val_loss: 0.4405 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4593 - accuracy: 0.9005 - val_loss: 0.4197 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4387 - accuracy: 0.9045 - val_loss: 0.4111 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4286 - accuracy: 0.9051 - val_loss: 0.4107 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4240 - accuracy: 0.9050 - val_loss: 0.4171 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4184 - accuracy: 0.9053 - val_loss: 0.4956 - val_accuracy: 0.8714 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4181 - accuracy: 0.9069 - val_loss: 0.3810 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4068 - accuracy: 0.9079 - val_loss: 0.3984 - val_accuracy: 0.9146 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4110 - accuracy: 0.9076 - val_loss: 0.4160 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4083 - accuracy: 0.9074 - val_loss: 0.3860 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4032 - accuracy: 0.9072 - val_loss: 0.3852 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4082 - accuracy: 0.9026 - val_loss: 0.4748 - val_accuracy: 0.8666 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3964 - accuracy: 0.9091 - val_loss: 0.3667 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3890 - accuracy: 0.9073 - val_loss: 0.3861 - val_accuracy: 0.9107 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3921 - accuracy: 0.9051 - val_loss: 0.3714 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3904 - accuracy: 0.9086 - val_loss: 0.3726 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3786 - accuracy: 0.9114 - val_loss: 0.3757 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3877 - accuracy: 0.9108 - val_loss: 0.3720 - val_accuracy: 0.9112 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3855 - accuracy: 0.9068 - val_loss: 0.4432 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3819 - accuracy: 0.9099 - val_loss: 0.3621 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3898 - accuracy: 0.9093 - val_loss: 0.3612 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3884 - accuracy: 0.9084 - val_loss: 0.4985 - val_accuracy: 0.8644 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3898 - accuracy: 0.9085 - val_loss: 0.3880 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3838 - accuracy: 0.9097 - val_loss: 0.3766 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3828 - accuracy: 0.9124 - val_loss: 0.3665 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3844 - accuracy: 0.9118 - val_loss: 0.6426 - val_accuracy: 0.7964 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3909 - accuracy: 0.9099 - val_loss: 0.4064 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3787 - accuracy: 0.9138 - val_loss: 0.3890 - val_accuracy: 0.9099 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3879 - accuracy: 0.9105 - val_loss: 0.3558 - val_accuracy: 0.9260 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3818 - accuracy: 0.9095 - val_loss: 0.3492 - val_accuracy: 0.9242 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3801 - accuracy: 0.9105 - val_loss: 0.3523 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3786 - accuracy: 0.9102 - val_loss: 0.4049 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3851 - accuracy: 0.9079 - val_loss: 0.3920 - val_accuracy: 0.9013 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3738 - accuracy: 0.9115 - val_loss: 0.3600 - val_accuracy: 0.9182 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3813 - accuracy: 0.9126 - val_loss: 0.3978 - val_accuracy: 0.9003 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3812 - accuracy: 0.9093 - val_loss: 0.3542 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3823 - accuracy: 0.9114 - val_loss: 0.4148 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3806 - accuracy: 0.9106 - val_loss: 0.3436 - val_accuracy: 0.9244 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3692 - accuracy: 0.9155 - val_loss: 0.4319 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3826 - accuracy: 0.9079 - val_loss: 0.3526 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3717 - accuracy: 0.9123 - val_loss: 0.3588 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3806 - accuracy: 0.9112 - val_loss: 0.3450 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3684 - accuracy: 0.9112 - val_loss: 0.3441 - val_accuracy: 0.9266 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3753 - accuracy: 0.9110 - val_loss: 0.3694 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3798 - accuracy: 0.9108 - val_loss: 0.3880 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3697 - accuracy: 0.9142 - val_loss: 0.3836 - val_accuracy: 0.9120 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3826 - accuracy: 0.9119 - val_loss: 0.3579 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3784 - accuracy: 0.9133 - val_loss: 0.3978 - val_accuracy: 0.9061 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3773 - accuracy: 0.9131 - val_loss: 0.3685 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3800 - accuracy: 0.9110 - val_loss: 0.3818 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3780 - accuracy: 0.9145 - val_loss: 0.3651 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3721 - accuracy: 0.9119 - val_loss: 0.3632 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3705 - accuracy: 0.9148 - val_loss: 0.3498 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3719 - accuracy: 0.9130 - val_loss: 0.3959 - val_accuracy: 0.9085 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3793 - accuracy: 0.9126 - val_loss: 0.4243 - val_accuracy: 0.8933 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 4s 39ms/step - loss: 0.3758 - accuracy: 0.9116 - val_loss: 0.3541 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 4s 43ms/step - loss: 0.3615 - accuracy: 0.9144 - val_loss: 0.3538 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3629 - accuracy: 0.9165 - val_loss: 0.3643 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3738 - accuracy: 0.9132 - val_loss: 0.3556 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3419 - accuracy: 0.9210 - val_loss: 0.3291 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3256 - accuracy: 0.9267 - val_loss: 0.3146 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3146 - accuracy: 0.9259 - val_loss: 0.3245 - val_accuracy: 0.9184 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3082 - accuracy: 0.9291 - val_loss: 0.2956 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2930 - accuracy: 0.9306 - val_loss: 0.2928 - val_accuracy: 0.9304 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2867 - accuracy: 0.9290 - val_loss: 0.2981 - val_accuracy: 0.9218 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2865 - accuracy: 0.9279 - val_loss: 0.2781 - val_accuracy: 0.9292 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2788 - accuracy: 0.9291 - val_loss: 0.2766 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2687 - accuracy: 0.9326 - val_loss: 0.2719 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2684 - accuracy: 0.9318 - val_loss: 0.2652 - val_accuracy: 0.9308 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2632 - accuracy: 0.9320 - val_loss: 0.2627 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2679 - accuracy: 0.9300 - val_loss: 0.2624 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2612 - accuracy: 0.9342 - val_loss: 0.2622 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2620 - accuracy: 0.9333 - val_loss: 0.2635 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2599 - accuracy: 0.9351 - val_loss: 0.2640 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2521 - accuracy: 0.9373 - val_loss: 0.2637 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2595 - accuracy: 0.9309 - val_loss: 0.2646 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2550 - accuracy: 0.9353 - val_loss: 0.2621 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2557 - accuracy: 0.9330 - val_loss: 0.2644 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2541 - accuracy: 0.9340 - val_loss: 0.2611 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2542 - accuracy: 0.9347 - val_loss: 0.2620 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2528 - accuracy: 0.9363 - val_loss: 0.2625 - val_accuracy: 0.9288 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2550 - accuracy: 0.9317 - val_loss: 0.2622 - val_accuracy: 0.9290 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2563 - accuracy: 0.9354 - val_loss: 0.2623 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2527 - accuracy: 0.9326 - val_loss: 0.2624 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2545 - accuracy: 0.9360 - val_loss: 0.2619 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2536 - accuracy: 0.9344 - val_loss: 0.2623 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2523 - accuracy: 0.9347 - val_loss: 0.2619 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2495 - accuracy: 0.9371 - val_loss: 0.2616 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2797 - accuracy: 0.9281\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5885, TN:9842, FP:613, FN:606, loss0.27970370650291443, acc0.9280656202053582, sn0.9066399630257279, sp0.9413677666188427, f10.9061513588420972, auc0.9754990115806893\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 22s 44ms/step - loss: 4.2042 - accuracy: 0.6548 - val_loss: 4.1235 - val_accuracy: 0.4294 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 2.9880 - accuracy: 0.7987 - val_loss: 3.0221 - val_accuracy: 0.4615 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 2.1274 - accuracy: 0.8396 - val_loss: 1.8998 - val_accuracy: 0.7738 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.5311 - accuracy: 0.8590 - val_loss: 1.2848 - val_accuracy: 0.8732 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 1.1627 - accuracy: 0.8646 - val_loss: 1.0194 - val_accuracy: 0.8692 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.9217 - accuracy: 0.8741 - val_loss: 0.9774 - val_accuracy: 0.8243 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.7539 - accuracy: 0.8832 - val_loss: 0.7481 - val_accuracy: 0.8702 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.6535 - accuracy: 0.8865 - val_loss: 0.5815 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5828 - accuracy: 0.8920 - val_loss: 0.5436 - val_accuracy: 0.9007 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5436 - accuracy: 0.8899 - val_loss: 0.5147 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.5111 - accuracy: 0.8960 - val_loss: 0.4433 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4755 - accuracy: 0.9013 - val_loss: 0.5613 - val_accuracy: 0.8628 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4617 - accuracy: 0.9049 - val_loss: 0.4317 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4490 - accuracy: 0.9031 - val_loss: 0.4776 - val_accuracy: 0.8937 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4402 - accuracy: 0.9026 - val_loss: 0.4675 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4312 - accuracy: 0.9038 - val_loss: 0.4751 - val_accuracy: 0.8845 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4178 - accuracy: 0.9067 - val_loss: 0.3867 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.4222 - accuracy: 0.9035 - val_loss: 0.4020 - val_accuracy: 0.9166 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4178 - accuracy: 0.9055 - val_loss: 0.4770 - val_accuracy: 0.8751 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4114 - accuracy: 0.9061 - val_loss: 0.3974 - val_accuracy: 0.9108 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4101 - accuracy: 0.9042 - val_loss: 0.4005 - val_accuracy: 0.9132 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4041 - accuracy: 0.9058 - val_loss: 0.3802 - val_accuracy: 0.9140 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4044 - accuracy: 0.9035 - val_loss: 0.4250 - val_accuracy: 0.9037 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3970 - accuracy: 0.9064 - val_loss: 0.3824 - val_accuracy: 0.9116 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3913 - accuracy: 0.9101 - val_loss: 0.5013 - val_accuracy: 0.8632 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3982 - accuracy: 0.9089 - val_loss: 0.3821 - val_accuracy: 0.9144 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3937 - accuracy: 0.9083 - val_loss: 0.4847 - val_accuracy: 0.8696 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3976 - accuracy: 0.9061 - val_loss: 0.4437 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3968 - accuracy: 0.9061 - val_loss: 0.4313 - val_accuracy: 0.8967 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3929 - accuracy: 0.9073 - val_loss: 0.3967 - val_accuracy: 0.9089 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3902 - accuracy: 0.9083 - val_loss: 0.3668 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3935 - accuracy: 0.9086 - val_loss: 0.3679 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3932 - accuracy: 0.9079 - val_loss: 0.3722 - val_accuracy: 0.9148 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3901 - accuracy: 0.9064 - val_loss: 0.4731 - val_accuracy: 0.8793 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3836 - accuracy: 0.9119 - val_loss: 0.3820 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3806 - accuracy: 0.9103 - val_loss: 0.4003 - val_accuracy: 0.9045 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3931 - accuracy: 0.9072 - val_loss: 0.4217 - val_accuracy: 0.9053 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3855 - accuracy: 0.9119 - val_loss: 0.5361 - val_accuracy: 0.8492 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3832 - accuracy: 0.9120 - val_loss: 0.3615 - val_accuracy: 0.9186 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3880 - accuracy: 0.9103 - val_loss: 0.3698 - val_accuracy: 0.9184 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3811 - accuracy: 0.9102 - val_loss: 0.3655 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3753 - accuracy: 0.9130 - val_loss: 0.4682 - val_accuracy: 0.8676 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3855 - accuracy: 0.9118 - val_loss: 0.3670 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3820 - accuracy: 0.9138 - val_loss: 0.3862 - val_accuracy: 0.9095 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3824 - accuracy: 0.9100 - val_loss: 0.3613 - val_accuracy: 0.9236 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3889 - accuracy: 0.9118 - val_loss: 0.3831 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3863 - accuracy: 0.9125 - val_loss: 0.3668 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3838 - accuracy: 0.9126 - val_loss: 0.3965 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3816 - accuracy: 0.9127 - val_loss: 0.3874 - val_accuracy: 0.9077 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3751 - accuracy: 0.9105 - val_loss: 0.4300 - val_accuracy: 0.8915 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3759 - accuracy: 0.9159 - val_loss: 0.4218 - val_accuracy: 0.8973 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3774 - accuracy: 0.9114 - val_loss: 0.3858 - val_accuracy: 0.9079 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3833 - accuracy: 0.9117 - val_loss: 0.4642 - val_accuracy: 0.8708 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3869 - accuracy: 0.9083 - val_loss: 0.4356 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3663 - accuracy: 0.9126 - val_loss: 0.4103 - val_accuracy: 0.8969 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3713 - accuracy: 0.9110 - val_loss: 0.3600 - val_accuracy: 0.9214 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3821 - accuracy: 0.9061 - val_loss: 0.3709 - val_accuracy: 0.9168 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3676 - accuracy: 0.9165 - val_loss: 0.3454 - val_accuracy: 0.9196 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3683 - accuracy: 0.9120 - val_loss: 0.3711 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3739 - accuracy: 0.9113 - val_loss: 0.3754 - val_accuracy: 0.9152 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 37ms/step - loss: 0.3756 - accuracy: 0.9142 - val_loss: 0.4063 - val_accuracy: 0.8979 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3703 - accuracy: 0.9166 - val_loss: 0.3979 - val_accuracy: 0.9055 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3701 - accuracy: 0.9135 - val_loss: 0.4120 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3699 - accuracy: 0.9159 - val_loss: 0.4000 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3750 - accuracy: 0.9141 - val_loss: 0.3671 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3710 - accuracy: 0.9120 - val_loss: 0.3707 - val_accuracy: 0.9122 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3680 - accuracy: 0.9142 - val_loss: 0.3537 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3700 - accuracy: 0.9136 - val_loss: 0.4380 - val_accuracy: 0.8887 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3787 - accuracy: 0.9113 - val_loss: 0.3588 - val_accuracy: 0.9212 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3662 - accuracy: 0.9145 - val_loss: 0.3970 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3674 - accuracy: 0.9145 - val_loss: 0.3593 - val_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3392 - accuracy: 0.9214 - val_loss: 0.3177 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3194 - accuracy: 0.9255 - val_loss: 0.3177 - val_accuracy: 0.9192 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3109 - accuracy: 0.9277 - val_loss: 0.3037 - val_accuracy: 0.9234 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3024 - accuracy: 0.9242 - val_loss: 0.3016 - val_accuracy: 0.9210 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2945 - accuracy: 0.9268 - val_loss: 0.2854 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2927 - accuracy: 0.9271 - val_loss: 0.2808 - val_accuracy: 0.9270 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2863 - accuracy: 0.9278 - val_loss: 0.2746 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2752 - accuracy: 0.9324 - val_loss: 0.2738 - val_accuracy: 0.9266 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2722 - accuracy: 0.9303 - val_loss: 0.2713 - val_accuracy: 0.9286 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2670 - accuracy: 0.9298 - val_loss: 0.2769 - val_accuracy: 0.9248 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2647 - accuracy: 0.9304 - val_loss: 0.2626 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2598 - accuracy: 0.9330 - val_loss: 0.2618 - val_accuracy: 0.9314 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.2576 - accuracy: 0.9334 - val_loss: 0.2609 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2607 - accuracy: 0.9326 - val_loss: 0.2599 - val_accuracy: 0.9308 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2543 - accuracy: 0.9338 - val_loss: 0.2602 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2582 - accuracy: 0.9307 - val_loss: 0.2590 - val_accuracy: 0.9318 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2548 - accuracy: 0.9350 - val_loss: 0.2585 - val_accuracy: 0.9306 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2537 - accuracy: 0.9349 - val_loss: 0.2590 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2497 - accuracy: 0.9377 - val_loss: 0.2589 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2522 - accuracy: 0.9343 - val_loss: 0.2587 - val_accuracy: 0.9310 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 38ms/step - loss: 0.2542 - accuracy: 0.9327 - val_loss: 0.2585 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2509 - accuracy: 0.9344 - val_loss: 0.2585 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2504 - accuracy: 0.9335 - val_loss: 0.2584 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2551 - accuracy: 0.9311 - val_loss: 0.2581 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2495 - accuracy: 0.9343 - val_loss: 0.2582 - val_accuracy: 0.9302 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2537 - accuracy: 0.9332 - val_loss: 0.2581 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2517 - accuracy: 0.9363 - val_loss: 0.2582 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2550 - accuracy: 0.9321 - val_loss: 0.2579 - val_accuracy: 0.9306 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2501 - accuracy: 0.9368 - val_loss: 0.2580 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2763 - accuracy: 0.9286\n",
      "17/17 [==============================] - 1s 15ms/step\n",
      "TP:5834, TN:9902, FP:553, FN:657, loss0.2763185501098633, acc0.9285967189897321, sn0.8987829302110615, sp0.9471066475370636, f10.9060413107625407, auc0.9752651889482999\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 1/100\n",
      "92/92 [==============================] - 23s 43ms/step - loss: 4.1660 - accuracy: 0.6721 - val_loss: 4.9307 - val_accuracy: 0.3891 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 2/100\n",
      "92/92 [==============================] - 3s 31ms/step - loss: 2.9341 - accuracy: 0.8184 - val_loss: 2.5221 - val_accuracy: 0.7808 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 3/100\n",
      "92/92 [==============================] - 3s 32ms/step - loss: 2.1023 - accuracy: 0.8431 - val_loss: 1.7473 - val_accuracy: 0.8528 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 4/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.5140 - accuracy: 0.8600 - val_loss: 1.3350 - val_accuracy: 0.8450 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 5/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 1.1538 - accuracy: 0.8630 - val_loss: 0.9777 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 6/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.8948 - accuracy: 0.8797 - val_loss: 0.9165 - val_accuracy: 0.8422 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 7/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.7491 - accuracy: 0.8849 - val_loss: 0.7066 - val_accuracy: 0.8861 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 8/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.6411 - accuracy: 0.8918 - val_loss: 0.5810 - val_accuracy: 0.8975 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 9/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.5763 - accuracy: 0.8956 - val_loss: 0.6480 - val_accuracy: 0.8660 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 10/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5280 - accuracy: 0.8982 - val_loss: 0.5320 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 11/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.5002 - accuracy: 0.9011 - val_loss: 0.5102 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 12/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4736 - accuracy: 0.9032 - val_loss: 0.5085 - val_accuracy: 0.8917 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 13/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4656 - accuracy: 0.9007 - val_loss: 0.4766 - val_accuracy: 0.8931 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 14/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4449 - accuracy: 0.9018 - val_loss: 0.4061 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 15/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4465 - accuracy: 0.9018 - val_loss: 0.3929 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 16/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4204 - accuracy: 0.9096 - val_loss: 0.4375 - val_accuracy: 0.9023 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 17/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4260 - accuracy: 0.9079 - val_loss: 0.4157 - val_accuracy: 0.9105 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 18/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4196 - accuracy: 0.9039 - val_loss: 0.3833 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 19/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4115 - accuracy: 0.9081 - val_loss: 0.3839 - val_accuracy: 0.9206 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 20/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4231 - accuracy: 0.9055 - val_loss: 0.3863 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 21/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.4126 - accuracy: 0.9057 - val_loss: 0.3865 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 22/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4133 - accuracy: 0.9093 - val_loss: 0.4244 - val_accuracy: 0.9019 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 23/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4068 - accuracy: 0.9071 - val_loss: 0.5075 - val_accuracy: 0.8566 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 24/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4101 - accuracy: 0.9046 - val_loss: 0.3724 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 25/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3991 - accuracy: 0.9067 - val_loss: 0.4093 - val_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 26/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3995 - accuracy: 0.9070 - val_loss: 0.4037 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 27/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3989 - accuracy: 0.9062 - val_loss: 0.3747 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 28/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3945 - accuracy: 0.9085 - val_loss: 0.3718 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 29/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3950 - accuracy: 0.9099 - val_loss: 0.3763 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 30/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4014 - accuracy: 0.9087 - val_loss: 0.3844 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 31/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.4012 - accuracy: 0.9097 - val_loss: 0.4189 - val_accuracy: 0.9001 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 32/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3923 - accuracy: 0.9079 - val_loss: 0.3632 - val_accuracy: 0.9176 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 33/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3948 - accuracy: 0.9077 - val_loss: 0.4224 - val_accuracy: 0.8971 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 34/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3938 - accuracy: 0.9097 - val_loss: 0.3573 - val_accuracy: 0.9224 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 35/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3836 - accuracy: 0.9115 - val_loss: 0.3948 - val_accuracy: 0.9154 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 36/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3966 - accuracy: 0.9089 - val_loss: 0.3556 - val_accuracy: 0.9220 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 37/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3911 - accuracy: 0.9083 - val_loss: 0.3634 - val_accuracy: 0.9198 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 38/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3902 - accuracy: 0.9082 - val_loss: 0.3663 - val_accuracy: 0.9126 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 39/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3880 - accuracy: 0.9086 - val_loss: 0.4557 - val_accuracy: 0.8851 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 40/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3936 - accuracy: 0.9109 - val_loss: 0.3610 - val_accuracy: 0.9216 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 41/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3970 - accuracy: 0.9084 - val_loss: 0.3750 - val_accuracy: 0.9164 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 42/100\n",
      "92/92 [==============================] - 3s 33ms/step - loss: 0.3805 - accuracy: 0.9123 - val_loss: 0.3610 - val_accuracy: 0.9170 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 43/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3771 - accuracy: 0.9119 - val_loss: 0.3704 - val_accuracy: 0.9192 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 44/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.4012 - accuracy: 0.9085 - val_loss: 0.3774 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 45/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3803 - accuracy: 0.9111 - val_loss: 0.3553 - val_accuracy: 0.9172 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 46/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3793 - accuracy: 0.9138 - val_loss: 0.3514 - val_accuracy: 0.9242 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 47/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3874 - accuracy: 0.9113 - val_loss: 0.3864 - val_accuracy: 0.9142 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 48/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3893 - accuracy: 0.9108 - val_loss: 0.3764 - val_accuracy: 0.9160 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 49/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3836 - accuracy: 0.9136 - val_loss: 0.3575 - val_accuracy: 0.9202 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 50/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3854 - accuracy: 0.9112 - val_loss: 0.3923 - val_accuracy: 0.9043 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 51/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3775 - accuracy: 0.9125 - val_loss: 0.3855 - val_accuracy: 0.9097 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 52/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3831 - accuracy: 0.9133 - val_loss: 0.3573 - val_accuracy: 0.9180 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 53/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3686 - accuracy: 0.9141 - val_loss: 0.3756 - val_accuracy: 0.9114 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 54/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3762 - accuracy: 0.9140 - val_loss: 0.5054 - val_accuracy: 0.8592 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 55/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3733 - accuracy: 0.9128 - val_loss: 0.3509 - val_accuracy: 0.9222 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 56/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3747 - accuracy: 0.9136 - val_loss: 0.4468 - val_accuracy: 0.8827 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 57/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3755 - accuracy: 0.9134 - val_loss: 0.4197 - val_accuracy: 0.8913 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 58/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3730 - accuracy: 0.9138 - val_loss: 0.3627 - val_accuracy: 0.9228 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 59/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3828 - accuracy: 0.9153 - val_loss: 0.3898 - val_accuracy: 0.9035 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 60/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.3780 - accuracy: 0.9102 - val_loss: 0.3729 - val_accuracy: 0.9101 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 61/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3721 - accuracy: 0.9132 - val_loss: 0.4362 - val_accuracy: 0.8807 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 62/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3754 - accuracy: 0.9080 - val_loss: 0.3552 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 63/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3679 - accuracy: 0.9144 - val_loss: 0.4174 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 64/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3668 - accuracy: 0.9091 - val_loss: 0.4343 - val_accuracy: 0.8835 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 65/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3627 - accuracy: 0.9136 - val_loss: 0.3528 - val_accuracy: 0.9200 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 66/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3614 - accuracy: 0.9165 - val_loss: 0.3498 - val_accuracy: 0.9194 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 67/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3666 - accuracy: 0.9148 - val_loss: 0.3622 - val_accuracy: 0.9174 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 68/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3899 - accuracy: 0.9092 - val_loss: 0.3680 - val_accuracy: 0.9190 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 69/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3757 - accuracy: 0.9153 - val_loss: 0.3828 - val_accuracy: 0.9156 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 70/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3843 - accuracy: 0.9101 - val_loss: 0.3510 - val_accuracy: 0.9204 - lr: 0.0010\n",
      "Learning rate:  0.0010000000474974513\n",
      "Epoch 71/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3734 - accuracy: 0.9138 - val_loss: 0.3552 - val_accuracy: 0.9218 - lr: 0.0010\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 72/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3492 - accuracy: 0.9211 - val_loss: 0.3286 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 73/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.3338 - accuracy: 0.9255 - val_loss: 0.3163 - val_accuracy: 0.9278 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 74/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3241 - accuracy: 0.9244 - val_loss: 0.3146 - val_accuracy: 0.9274 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 75/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.3076 - accuracy: 0.9294 - val_loss: 0.3003 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 76/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2991 - accuracy: 0.9280 - val_loss: 0.2951 - val_accuracy: 0.9252 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 77/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2940 - accuracy: 0.9289 - val_loss: 0.2891 - val_accuracy: 0.9276 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 78/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2864 - accuracy: 0.9295 - val_loss: 0.2804 - val_accuracy: 0.9280 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 79/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2814 - accuracy: 0.9268 - val_loss: 0.2868 - val_accuracy: 0.9260 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 80/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2773 - accuracy: 0.9293 - val_loss: 0.2705 - val_accuracy: 0.9296 - lr: 1.0000e-04\n",
      "Learning rate:  0.00010000000474974513\n",
      "Epoch 81/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2749 - accuracy: 0.9308 - val_loss: 0.2745 - val_accuracy: 0.9284 - lr: 1.0000e-04\n",
      "Learning rate:  1.0000000474974514e-05\n",
      "Epoch 82/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2664 - accuracy: 0.9309 - val_loss: 0.2673 - val_accuracy: 0.9302 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 83/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2656 - accuracy: 0.9322 - val_loss: 0.2649 - val_accuracy: 0.9286 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 84/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2625 - accuracy: 0.9330 - val_loss: 0.2636 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 85/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2637 - accuracy: 0.9324 - val_loss: 0.2643 - val_accuracy: 0.9298 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 86/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2615 - accuracy: 0.9320 - val_loss: 0.2639 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 87/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2612 - accuracy: 0.9330 - val_loss: 0.2639 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 88/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2651 - accuracy: 0.9315 - val_loss: 0.2628 - val_accuracy: 0.9296 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 89/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2643 - accuracy: 0.9314 - val_loss: 0.2623 - val_accuracy: 0.9300 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 90/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2606 - accuracy: 0.9323 - val_loss: 0.2628 - val_accuracy: 0.9290 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-05\n",
      "Epoch 91/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2630 - accuracy: 0.9326 - val_loss: 0.2621 - val_accuracy: 0.9312 - lr: 1.0000e-05\n",
      "Learning rate:  1.0000000656873453e-06\n",
      "Epoch 92/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2574 - accuracy: 0.9332 - val_loss: 0.2620 - val_accuracy: 0.9312 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 93/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2593 - accuracy: 0.9318 - val_loss: 0.2622 - val_accuracy: 0.9300 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 94/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2589 - accuracy: 0.9359 - val_loss: 0.2622 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 95/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2601 - accuracy: 0.9338 - val_loss: 0.2623 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 96/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2576 - accuracy: 0.9350 - val_loss: 0.2623 - val_accuracy: 0.9296 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 97/100\n",
      "92/92 [==============================] - 3s 35ms/step - loss: 0.2587 - accuracy: 0.9344 - val_loss: 0.2623 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 98/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2549 - accuracy: 0.9344 - val_loss: 0.2624 - val_accuracy: 0.9298 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 99/100\n",
      "92/92 [==============================] - 3s 34ms/step - loss: 0.2617 - accuracy: 0.9334 - val_loss: 0.2619 - val_accuracy: 0.9304 - lr: 1.0000e-06\n",
      "Learning rate:  1.0000001111620804e-06\n",
      "Epoch 100/100\n",
      "92/92 [==============================] - 3s 36ms/step - loss: 0.2554 - accuracy: 0.9367 - val_loss: 0.2617 - val_accuracy: 0.9292 - lr: 1.0000e-06\n",
      "530/530 [==============================] - 4s 8ms/step - loss: 0.2814 - accuracy: 0.9285\n",
      "17/17 [==============================] - 1s 16ms/step\n",
      "TP:5849, TN:9886, FP:569, FN:642, loss0.2813703119754791, acc0.9285377080136905, sn0.9010938222153752, sp0.9455762792922047, f10.9061894802076071, auc0.9745924405060431\n",
      "Average Test loss:  0.28014415204524995\n",
      "Average Accuracy:  0.9282367520358786\n",
      "Average Sensitivity:  0.9046217840086272\n",
      "Average Specificity:  0.9428981348637017\n",
      "Average F1 Score:  0.9061663751821178\n",
      "Average AUC Score:  0.9750195926655906\n",
      "AUC for ROC curve 1: 0.9743\n",
      "AUC for ROC curve 2: 0.9743\n",
      "AUC for ROC curve 3: 0.9747\n",
      "AUC for ROC curve 4: 0.9747\n",
      "AUC for ROC curve 5: 0.9752\n",
      "AUC for ROC curve 6: 0.9752\n",
      "AUC for ROC curve 7: 0.9751\n",
      "AUC for ROC curve 8: 0.9751\n",
      "AUC for ROC curve 9: 0.9751\n",
      "AUC for ROC curve 10: 0.9751\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, f1_score,roc_auc_score\n",
    "losses = []\n",
    "accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "fprs = []\n",
    "tprs = []\n",
    "\n",
    "num_trainings = 10\n",
    "for i in range(num_trainings):\n",
    "    model = create_model(x_train1.shape[1:], x_train2.shape[1:], x_train3.shape[1:])\n",
    "    model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "    history = model.fit([x_train1, x_train2, x_train3], y_train, batch_size=128, epochs=100,\n",
    "                        validation_data=([x_val1, x_val2, x_val3], y_val), callbacks=[lr_scheduler])\n",
    "    loss, accuracy = model.evaluate([x_test1, x_test2, x_test3], y_test)\n",
    "    y_prob = model.predict([x_test1, x_test2, x_test3], batch_size=1024, verbose=1)\n",
    "\n",
    "    y_true = y_test.argmax(axis=-1)\n",
    "    y_pred = np.argmax(y_prob, axis=-1)\n",
    "\n",
    "    C = confusion_matrix(y_true, y_pred, labels=(1, 0))\n",
    "    TP, TN, FP, FN = C[0, 0], C[1, 1], C[1, 0], C[0, 1]\n",
    "    acc, sn, sp = 1. * (TP + TN) / (TP + TN + FP + FN), 1. * TP / (TP + FN), 1. * TN / (TN + FP)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sn = TP / (TP + FN)  \n",
    "    sp = TN / (TN + FP) \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = sn\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    auc_scores.append(roc_auc)\n",
    "    \n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}, loss{}, acc{}, sn{}, sp{}, f1{}, auc{}\".format(TP, TN, FP, FN, loss, acc, sn, sp, f1, roc_auc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)\n",
    "    sensitivities.append(sn)\n",
    "    specificities.append(sp)\n",
    "    f1_scores.append(f1)\n",
    "    auc_scores.append(roc_auc)\n",
    "\n",
    "avg_loss = np.mean(losses)\n",
    "avg_accuracy = np.mean(accuracies)\n",
    "avg_sensitivity = np.mean(sensitivities)\n",
    "avg_specificity = np.mean(specificities)\n",
    "avg_f1_score = np.mean(f1_scores)\n",
    "avg_auc_score = np.mean(auc_scores)\n",
    "\n",
    "print(\"Average Test loss: \", avg_loss)\n",
    "print(\"Average Accuracy: \", avg_accuracy)\n",
    "print(\"Average Sensitivity: \", avg_sensitivity)\n",
    "print(\"Average Specificity: \", avg_specificity)\n",
    "print(\"Average F1 Score: \", avg_f1_score)\n",
    "print(\"Average AUC Score: \", avg_auc_score)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "mean_tpr = np.zeros_like(mean_fpr)\n",
    "for i in range(num_trainings):\n",
    "    mean_tpr += np.interp(mean_fpr, fprs[i], tprs[i])\n",
    "    plt.plot(fprs[i], tprs[i], alpha=0.3, label='ROC curve {} (AUC = {:.4f})'.format(i+1, auc_scores[i]))\n",
    "    print('AUC for ROC curve {}: {:.4f}'.format(i+1, auc_scores[i]))\n",
    "mean_tpr /= num_trainings\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "plt.plot(mean_fpr, mean_tpr, 'k--', label='Mean ROC (AUC = {:.4f})'.format(mean_auc), lw=2)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f9963-cfa7-47f5-8701-2a24e58e20bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
